// Code generated by command: go run asm.go -out asm_x86.s -stubs asm_x86.go. DO NOT EDIT.

#include "textflag.h"

DATA dct256<>+0(SB)/4, $(1.9999623)
DATA dct256<>+4(SB)/4, $(1.9996612)
DATA dct256<>+8(SB)/4, $(1.9990588)
DATA dct256<>+12(SB)/4, $(1.9981555)
DATA dct256<>+16(SB)/4, $(1.9969511)
DATA dct256<>+20(SB)/4, $(1.9954461)
DATA dct256<>+24(SB)/4, $(1.9936405)
DATA dct256<>+28(SB)/4, $(1.9915348)
DATA dct256<>+32(SB)/4, $(1.9891292)
DATA dct256<>+36(SB)/4, $(1.9864239)
DATA dct256<>+40(SB)/4, $(1.9834195)
DATA dct256<>+44(SB)/4, $(1.9801164)
DATA dct256<>+48(SB)/4, $(1.9765152)
DATA dct256<>+52(SB)/4, $(1.9726162)
DATA dct256<>+56(SB)/4, $(1.9684201)
DATA dct256<>+60(SB)/4, $(1.9639277)
DATA dct256<>+64(SB)/4, $(1.9591396)
DATA dct256<>+68(SB)/4, $(1.9540563)
DATA dct256<>+72(SB)/4, $(1.9486787)
DATA dct256<>+76(SB)/4, $(1.9430078)
DATA dct256<>+80(SB)/4, $(1.9370441)
DATA dct256<>+84(SB)/4, $(1.9307889)
DATA dct256<>+88(SB)/4, $(1.9242429)
DATA dct256<>+92(SB)/4, $(1.9174069)
DATA dct256<>+96(SB)/4, $(1.9102824)
DATA dct256<>+100(SB)/4, $(1.90287)
DATA dct256<>+104(SB)/4, $(1.8951712)
DATA dct256<>+108(SB)/4, $(1.8871869)
DATA dct256<>+112(SB)/4, $(1.8789184)
DATA dct256<>+116(SB)/4, $(1.870367)
DATA dct256<>+120(SB)/4, $(1.8615339)
DATA dct256<>+124(SB)/4, $(1.8524204)
DATA dct256<>+128(SB)/4, $(1.8430281)
DATA dct256<>+132(SB)/4, $(1.8333582)
DATA dct256<>+136(SB)/4, $(1.8234121)
DATA dct256<>+140(SB)/4, $(1.8131914)
DATA dct256<>+144(SB)/4, $(1.8026977)
DATA dct256<>+148(SB)/4, $(1.7919325)
DATA dct256<>+152(SB)/4, $(1.7808975)
DATA dct256<>+156(SB)/4, $(1.7695942)
DATA dct256<>+160(SB)/4, $(1.7580245)
DATA dct256<>+164(SB)/4, $(1.74619)
DATA dct256<>+168(SB)/4, $(1.7340925)
DATA dct256<>+172(SB)/4, $(1.7217339)
DATA dct256<>+176(SB)/4, $(1.709116)
DATA dct256<>+180(SB)/4, $(1.6962407)
DATA dct256<>+184(SB)/4, $(1.68311)
DATA dct256<>+188(SB)/4, $(1.6697258)
DATA dct256<>+192(SB)/4, $(1.6560901)
DATA dct256<>+196(SB)/4, $(1.642205)
DATA dct256<>+200(SB)/4, $(1.6280726)
DATA dct256<>+204(SB)/4, $(1.6136951)
DATA dct256<>+208(SB)/4, $(1.5990745)
DATA dct256<>+212(SB)/4, $(1.5842131)
DATA dct256<>+216(SB)/4, $(1.5691131)
DATA dct256<>+220(SB)/4, $(1.553777)
DATA dct256<>+224(SB)/4, $(1.5382067)
DATA dct256<>+228(SB)/4, $(1.5224048)
DATA dct256<>+232(SB)/4, $(1.5063736)
DATA dct256<>+236(SB)/4, $(1.4901155)
DATA dct256<>+240(SB)/4, $(1.4736332)
DATA dct256<>+244(SB)/4, $(1.4569287)
DATA dct256<>+248(SB)/4, $(1.4400051)
DATA dct256<>+252(SB)/4, $(1.4228644)
DATA dct256<>+256(SB)/4, $(1.4055095)
DATA dct256<>+260(SB)/4, $(1.3879429)
DATA dct256<>+264(SB)/4, $(1.3701674)
DATA dct256<>+268(SB)/4, $(1.3521854)
DATA dct256<>+272(SB)/4, $(1.3339999)
DATA dct256<>+276(SB)/4, $(1.3156134)
DATA dct256<>+280(SB)/4, $(1.2970288)
DATA dct256<>+284(SB)/4, $(1.2782489)
DATA dct256<>+288(SB)/4, $(1.2592765)
DATA dct256<>+292(SB)/4, $(1.2401145)
DATA dct256<>+296(SB)/4, $(1.2207656)
DATA dct256<>+300(SB)/4, $(1.2012329)
DATA dct256<>+304(SB)/4, $(1.1815194)
DATA dct256<>+308(SB)/4, $(1.1616279)
DATA dct256<>+312(SB)/4, $(1.1415615)
DATA dct256<>+316(SB)/4, $(1.1213231)
DATA dct256<>+320(SB)/4, $(1.1009159)
DATA dct256<>+324(SB)/4, $(1.0803429)
DATA dct256<>+328(SB)/4, $(1.0596073)
DATA dct256<>+332(SB)/4, $(1.038712)
DATA dct256<>+336(SB)/4, $(1.0176603)
DATA dct256<>+340(SB)/4, $(0.9964553)
DATA dct256<>+344(SB)/4, $(0.97510034)
DATA dct256<>+348(SB)/4, $(0.95359844)
DATA dct256<>+352(SB)/4, $(0.931953)
DATA dct256<>+356(SB)/4, $(0.91016716)
DATA dct256<>+360(SB)/4, $(0.8882443)
DATA dct256<>+364(SB)/4, $(0.86618763)
DATA dct256<>+368(SB)/4, $(0.8440005)
DATA dct256<>+372(SB)/4, $(0.8216863)
DATA dct256<>+376(SB)/4, $(0.7992484)
DATA dct256<>+380(SB)/4, $(0.77669007)
DATA dct256<>+384(SB)/4, $(0.75401485)
DATA dct256<>+388(SB)/4, $(0.73122597)
DATA dct256<>+392(SB)/4, $(0.70832705)
DATA dct256<>+396(SB)/4, $(0.68532145)
DATA dct256<>+400(SB)/4, $(0.6622126)
DATA dct256<>+404(SB)/4, $(0.63900405)
DATA dct256<>+408(SB)/4, $(0.6156993)
DATA dct256<>+412(SB)/4, $(0.5923018)
DATA dct256<>+416(SB)/4, $(0.56881505)
DATA dct256<>+420(SB)/4, $(0.5452427)
DATA dct256<>+424(SB)/4, $(0.5215882)
DATA dct256<>+428(SB)/4, $(0.49785522)
DATA dct256<>+432(SB)/4, $(0.4740472)
DATA dct256<>+436(SB)/4, $(0.45016783)
DATA dct256<>+440(SB)/4, $(0.42622063)
DATA dct256<>+444(SB)/4, $(0.40220928)
DATA dct256<>+448(SB)/4, $(0.37813732)
DATA dct256<>+452(SB)/4, $(0.35400844)
DATA dct256<>+456(SB)/4, $(0.32982624)
DATA dct256<>+460(SB)/4, $(0.30559438)
DATA dct256<>+464(SB)/4, $(0.2813165)
DATA dct256<>+468(SB)/4, $(0.2569962)
DATA dct256<>+472(SB)/4, $(0.23263726)
DATA dct256<>+476(SB)/4, $(0.20824327)
DATA dct256<>+480(SB)/4, $(0.18381791)
DATA dct256<>+484(SB)/4, $(0.15936488)
DATA dct256<>+488(SB)/4, $(0.13488784)
DATA dct256<>+492(SB)/4, $(0.11039049)
DATA dct256<>+496(SB)/4, $(0.08587652)
DATA dct256<>+500(SB)/4, $(0.061349608)
DATA dct256<>+504(SB)/4, $(0.03681346)
DATA dct256<>+508(SB)/4, $(0.012271769)
GLOBL dct256<>(SB), RODATA|NOPTR, $512

DATA dct128<>+0(SB)/4, $(1.9998494)
DATA dct128<>+4(SB)/4, $(1.9986447)
DATA dct128<>+8(SB)/4, $(1.9962362)
DATA dct128<>+12(SB)/4, $(1.9926252)
DATA dct128<>+16(SB)/4, $(1.987814)
DATA dct128<>+20(SB)/4, $(1.9818053)
DATA dct128<>+24(SB)/4, $(1.9746028)
DATA dct128<>+28(SB)/4, $(1.966211)
DATA dct128<>+32(SB)/4, $(1.9566348)
DATA dct128<>+36(SB)/4, $(1.9458799)
DATA dct128<>+40(SB)/4, $(1.9339529)
DATA dct128<>+44(SB)/4, $(1.920861)
DATA dct128<>+48(SB)/4, $(1.906612)
DATA dct128<>+52(SB)/4, $(1.8912146)
DATA dct128<>+56(SB)/4, $(1.874678)
DATA dct128<>+60(SB)/4, $(1.8570122)
DATA dct128<>+64(SB)/4, $(1.8382277)
DATA dct128<>+68(SB)/4, $(1.818336)
DATA dct128<>+72(SB)/4, $(1.797349)
DATA dct128<>+76(SB)/4, $(1.7752793)
DATA dct128<>+80(SB)/4, $(1.7521402)
DATA dct128<>+84(SB)/4, $(1.7279457)
DATA dct128<>+88(SB)/4, $(1.7027104)
DATA dct128<>+92(SB)/4, $(1.6764494)
DATA dct128<>+96(SB)/4, $(1.6491786)
DATA dct128<>+100(SB)/4, $(1.6209143)
DATA dct128<>+104(SB)/4, $(1.5916739)
DATA dct128<>+108(SB)/4, $(1.5614744)
DATA dct128<>+112(SB)/4, $(1.5303345)
DATA dct128<>+116(SB)/4, $(1.4982728)
DATA dct128<>+120(SB)/4, $(1.4653085)
DATA dct128<>+124(SB)/4, $(1.4314617)
DATA dct128<>+128(SB)/4, $(1.3967525)
DATA dct128<>+132(SB)/4, $(1.361202)
DATA dct128<>+136(SB)/4, $(1.3248316)
DATA dct128<>+140(SB)/4, $(1.2876631)
DATA dct128<>+144(SB)/4, $(1.249719)
DATA dct128<>+148(SB)/4, $(1.2110221)
DATA dct128<>+152(SB)/4, $(1.1715957)
DATA dct128<>+156(SB)/4, $(1.1314636)
DATA dct128<>+160(SB)/4, $(1.09065)
DATA dct128<>+164(SB)/4, $(1.0491793)
DATA dct128<>+168(SB)/4, $(1.0070767)
DATA dct128<>+172(SB)/4, $(0.96436757)
DATA dct128<>+176(SB)/4, $(0.92107743)
DATA dct128<>+180(SB)/4, $(0.8772325)
DATA dct128<>+184(SB)/4, $(0.8328591)
DATA dct128<>+188(SB)/4, $(0.7879841)
DATA dct128<>+192(SB)/4, $(0.7426344)
DATA dct128<>+196(SB)/4, $(0.69683737)
DATA dct128<>+200(SB)/4, $(0.6506206)
DATA dct128<>+204(SB)/4, $(0.6040119)
DATA dct128<>+208(SB)/4, $(0.5570394)
DATA dct128<>+212(SB)/4, $(0.5097313)
DATA dct128<>+216(SB)/4, $(0.4621162)
DATA dct128<>+220(SB)/4, $(0.41422275)
DATA dct128<>+224(SB)/4, $(0.36607978)
DATA dct128<>+228(SB)/4, $(0.3177163)
DATA dct128<>+232(SB)/4, $(0.2691614)
DATA dct128<>+236(SB)/4, $(0.22044441)
DATA dct128<>+240(SB)/4, $(0.17159462)
DATA dct128<>+244(SB)/4, $(0.122641474)
DATA dct128<>+248(SB)/4, $(0.07361445)
DATA dct128<>+252(SB)/4, $(0.024543077)
GLOBL dct128<>(SB), RODATA|NOPTR, $256

DATA dct64<>+0(SB)/4, $(1.9993976)
DATA dct64<>+4(SB)/4, $(1.9945809)
DATA dct64<>+8(SB)/4, $(1.9849591)
DATA dct64<>+12(SB)/4, $(1.9705553)
DATA dct64<>+16(SB)/4, $(1.9514042)
DATA dct64<>+20(SB)/4, $(1.9275521)
DATA dct64<>+24(SB)/4, $(1.8990563)
DATA dct64<>+28(SB)/4, $(1.8659856)
DATA dct64<>+32(SB)/4, $(1.8284196)
DATA dct64<>+36(SB)/4, $(1.7864486)
DATA dct64<>+40(SB)/4, $(1.7401739)
DATA dct64<>+44(SB)/4, $(1.6897072)
DATA dct64<>+48(SB)/4, $(1.6351696)
DATA dct64<>+52(SB)/4, $(1.5766928)
DATA dct64<>+56(SB)/4, $(1.5144176)
DATA dct64<>+60(SB)/4, $(1.4484942)
DATA dct64<>+64(SB)/4, $(1.3790811)
DATA dct64<>+68(SB)/4, $(1.3063457)
DATA dct64<>+72(SB)/4, $(1.2304631)
DATA dct64<>+76(SB)/4, $(1.1516163)
DATA dct64<>+80(SB)/4, $(1.0699953)
DATA dct64<>+84(SB)/4, $(0.9857964)
DATA dct64<>+88(SB)/4, $(0.8992227)
DATA dct64<>+92(SB)/4, $(0.8104826)
DATA dct64<>+96(SB)/4, $(0.7197901)
DATA dct64<>+100(SB)/4, $(0.6273635)
DATA dct64<>+104(SB)/4, $(0.5334255)
DATA dct64<>+108(SB)/4, $(0.43820247)
DATA dct64<>+112(SB)/4, $(0.34192377)
DATA dct64<>+116(SB)/4, $(0.24482135)
DATA dct64<>+120(SB)/4, $(0.14712913)
DATA dct64<>+124(SB)/4, $(0.049082458)
GLOBL dct64<>(SB), RODATA|NOPTR, $128

DATA dct32<>+0(SB)/4, $(1.9975909)
DATA dct32<>+4(SB)/4, $(1.978353)
DATA dct32<>+8(SB)/4, $(1.9400625)
DATA dct32<>+12(SB)/4, $(1.8830881)
DATA dct32<>+16(SB)/4, $(1.8079786)
DATA dct32<>+20(SB)/4, $(1.7154572)
DATA dct32<>+24(SB)/4, $(1.606415)
DATA dct32<>+28(SB)/4, $(1.4819022)
DATA dct32<>+32(SB)/4, $(1.343118)
DATA dct32<>+36(SB)/4, $(1.1913986)
DATA dct32<>+40(SB)/4, $(1.0282055)
DATA dct32<>+44(SB)/4, $(0.85511017)
DATA dct32<>+48(SB)/4, $(0.6737797)
DATA dct32<>+52(SB)/4, $(0.48596036)
DATA dct32<>+56(SB)/4, $(0.29346094)
DATA dct32<>+60(SB)/4, $(0.09813535)
GLOBL dct32<>(SB), RODATA|NOPTR, $64

DATA dct16<>+0(SB)/4, $(1.9903694)
DATA dct16<>+4(SB)/4, $(1.9138807)
DATA dct16<>+8(SB)/4, $(1.7638426)
DATA dct16<>+12(SB)/4, $(1.5460209)
DATA dct16<>+16(SB)/4, $(1.2687865)
DATA dct16<>+20(SB)/4, $(0.9427935)
DATA dct16<>+24(SB)/4, $(0.5805693)
DATA dct16<>+28(SB)/4, $(0.19603428)
GLOBL dct16<>(SB), RODATA|NOPTR, $32

DATA dct8<>+0(SB)/4, $(1.9615705)
DATA dct8<>+4(SB)/4, $(1.6629392)
DATA dct8<>+8(SB)/4, $(1.1111405)
DATA dct8<>+12(SB)/4, $(0.39018065)
GLOBL dct8<>(SB), RODATA|NOPTR, $16

DATA dct4<>+0(SB)/4, $(1.847759)
DATA dct4<>+4(SB)/4, $(1.847759)
DATA dct4<>+8(SB)/4, $(0.76536685)
DATA dct4<>+12(SB)/4, $(0.76536685)
GLOBL dct4<>(SB), RODATA|NOPTR, $16

DATA dct2<>+0(SB)/4, $(1.4142135)
DATA dct2<>+4(SB)/4, $(1.4142135)
DATA dct2<>+8(SB)/4, $(1.4142135)
DATA dct2<>+12(SB)/4, $(1.4142135)
GLOBL dct2<>(SB), RODATA|NOPTR, $16

DATA gather<>+0(SB)/4, $0x00000000
DATA gather<>+4(SB)/4, $0x00000040
DATA gather<>+8(SB)/4, $0x00000080
DATA gather<>+12(SB)/4, $0x000000c0
DATA gather<>+16(SB)/4, $0x00000100
DATA gather<>+20(SB)/4, $0x00000140
DATA gather<>+24(SB)/4, $0x00000180
DATA gather<>+28(SB)/4, $0x000001c0
DATA gather<>+32(SB)/4, $0x00000001
GLOBL gather<>(SB), RODATA|NOPTR, $36

DATA perm<>+0(SB)/1, $0x07
DATA perm<>+1(SB)/1, $0x06
DATA perm<>+2(SB)/1, $0x05
DATA perm<>+3(SB)/1, $0x04
DATA perm<>+4(SB)/1, $0x03
DATA perm<>+5(SB)/1, $0x02
DATA perm<>+6(SB)/1, $0x01
DATA perm<>+7(SB)/1, $0x00
DATA perm<>+8(SB)/1, $0x01
DATA perm<>+9(SB)/1, $0x02
DATA perm<>+10(SB)/1, $0x03
DATA perm<>+11(SB)/1, $0x04
DATA perm<>+12(SB)/1, $0x05
DATA perm<>+13(SB)/1, $0x06
DATA perm<>+14(SB)/1, $0x07
DATA perm<>+15(SB)/1, $0x00
GLOBL perm<>(SB), RODATA|NOPTR, $16

DATA constyCbCrGray<>+0(SB)/4, $+128
DATA constyCbCrGray<>+4(SB)/4, $+65793
DATA constyCbCrGray<>+8(SB)/4, $+91881
DATA constyCbCrGray<>+12(SB)/4, $+46802
DATA constyCbCrGray<>+16(SB)/4, $+22554
DATA constyCbCrGray<>+20(SB)/4, $+116130
DATA constyCbCrGray<>+24(SB)/4, $(0.29783657)
DATA constyCbCrGray<>+28(SB)/4, $(0.58471596)
DATA constyCbCrGray<>+32(SB)/4, $(0.114)
GLOBL constyCbCrGray<>(SB), RODATA|NOPTR, $36

// func asmDCT2DHash64(input []float32) [64]float32
// Requires: AVX, AVX2, SSE, SSE2, SSE4.1
TEXT ·asmDCT2DHash64(SB), NOSPLIT|NOPTR, $272-280
	MOVQ input_base+0(FP), AX
	XORL CX, CX
	XORL DX, DX
	XORL BX, BX

j:
	CMPL DX, $0x40
	JE   i

	// Start innerloop instructions
	MOVL  $0x00000040, BX
	IMULL DX, BX
	VZEROUPPER

	// DCT64
	VPMOVZXBD perm<>+0(SB), Y9
	VMOVUPS   (AX)(BX*4), Y0
	VMOVUPS   32(AX)(BX*4), Y1
	VMOVUPS   64(AX)(BX*4), Y4
	VMOVUPS   96(AX)(BX*4), Y5
	VPERMD    128(AX)(BX*4), Y9, Y6
	VPERMD    160(AX)(BX*4), Y9, Y7
	VPERMD    192(AX)(BX*4), Y9, Y8
	VPERMD    224(AX)(BX*4), Y9, Y9
	VADDPS    Y9, Y0, Y10
	VMOVUPS   Y10, (AX)(BX*4)
	VSUBPS    Y9, Y0, Y10
	VDIVPS    dct64<>+0(SB), Y10, Y10
	VMOVUPS   Y10, 128(AX)(BX*4)
	VADDPS    Y8, Y1, Y10
	VMOVUPS   Y10, 32(AX)(BX*4)
	VSUBPS    Y8, Y1, Y10
	VDIVPS    dct64<>+32(SB), Y10, Y10
	VMOVUPS   Y10, 160(AX)(BX*4)
	VADDPS    Y7, Y4, Y10
	VMOVUPS   Y10, 64(AX)(BX*4)
	VSUBPS    Y7, Y4, Y10
	VDIVPS    dct64<>+64(SB), Y10, Y10
	VMOVUPS   Y10, 192(AX)(BX*4)
	VADDPS    Y6, Y5, Y10
	VMOVUPS   Y10, 96(AX)(BX*4)
	VSUBPS    Y6, Y5, Y10
	VDIVPS    dct64<>+96(SB), Y10, Y10
	VMOVUPS   Y10, 224(AX)(BX*4)
	VZEROALL

	// DCT32
	MOVUPS (AX)(BX*4), X0
	MOVUPS 112(AX)(BX*4), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X4
	VSUBPS X1, X0, X5
	DIVPS  dct32<>+0(SB), X5
	MOVUPS 16(AX)(BX*4), X0
	MOVUPS 96(AX)(BX*4), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X6
	VSUBPS X1, X0, X7
	DIVPS  dct32<>+16(SB), X7
	MOVUPS 32(AX)(BX*4), X0
	MOVUPS 80(AX)(BX*4), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X8
	VSUBPS X1, X0, X9
	DIVPS  dct32<>+32(SB), X9
	MOVUPS 48(AX)(BX*4), X0
	MOVUPS 64(AX)(BX*4), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X10
	VSUBPS X1, X0, X0
	DIVPS  dct32<>+48(SB), X0

	// DCT16
	PSHUFD $0x1b, X8, X8
	VADDPS X8, X6, X1
	VSUBPS X8, X6, X11
	DIVPS  dct16<>+16(SB), X11
	PSHUFD $0x1b, X10, X8
	VADDPS X8, X4, X10
	VSUBPS X8, X4, X8
	DIVPS  dct16<>+0(SB), X8

	// DCT8
	PSHUFD $0x1b, X1, X4
	VADDPS X4, X10, X6
	VSUBPS X4, X10, X10
	DIVPS  dct8<>+0(SB), X10

	// DCT4
	PSHUFD     $0xb4, X6, X6
	PSHUFD     $0xb4, X10, X10
	VPUNPCKLDQ X10, X6, X1
	VPUNPCKHDQ X10, X6, X4
	VADDPS     X4, X1, X6
	VSUBPS     X4, X1, X10
	DIVPS      dct4<>+0(SB), X10
	VPUNPCKLDQ X10, X6, X1
	VPUNPCKHDQ X10, X6, X4

	// DCT2
	PSHUFD    $0xd8, X1, X6
	PSHUFD    $0xd8, X4, X10
	VADDPS    X10, X6, X1
	VSUBPS    X10, X6, X10
	DIVPS     dct2<>+0(SB), X10
	VADDPS    X1, X10, X6
	VBLENDPS  $0x03, X1, X6, X6
	VSHUFPS   $0x88, X10, X6, X1
	VSHUFPS   $0xdd, X10, X6, X4
	VPSRLDQ   $0x04, X4, X10
	ADDPS     X10, X4
	VUNPCKLPS X4, X1, X10
	VUNPCKHPS X4, X1, X1

	// end DCT8
	// DCT8
	PSHUFD $0x1b, X11, X6
	VADDPS X6, X8, X11
	VSUBPS X6, X8, X8
	DIVPS  dct8<>+0(SB), X8

	// DCT4
	PSHUFD     $0xb4, X11, X11
	PSHUFD     $0xb4, X8, X8
	VPUNPCKLDQ X8, X11, X4
	VPUNPCKHDQ X8, X11, X6
	VADDPS     X6, X4, X11
	VSUBPS     X6, X4, X8
	DIVPS      dct4<>+0(SB), X8
	VPUNPCKLDQ X8, X11, X4
	VPUNPCKHDQ X8, X11, X6

	// DCT2
	PSHUFD    $0xd8, X4, X11
	PSHUFD    $0xd8, X6, X8
	VADDPS    X8, X11, X4
	VSUBPS    X8, X11, X8
	DIVPS     dct2<>+0(SB), X8
	VADDPS    X4, X8, X11
	VBLENDPS  $0x03, X4, X11, X11
	VSHUFPS   $0x88, X8, X11, X4
	VSHUFPS   $0xdd, X8, X11, X6
	VPSRLDQ   $0x04, X6, X8
	ADDPS     X8, X6
	VUNPCKLPS X6, X4, X8
	VUNPCKHPS X6, X4, X11

	// end DCT8
	VPSRLDQ   $0x04, X8, X4
	VPSLLDQ   $0x0c, X11, X6
	VADDPS    X4, X6, X4
	VADDPS    X4, X8, X8
	VPSRLDQ   $0x04, X11, X6
	VADDPS    X6, X11, X11
	VUNPCKLPS X8, X10, X4
	VUNPCKHPS X8, X10, X6
	VUNPCKLPS X11, X1, X8
	VUNPCKHPS X11, X1, X10

	// end DCT16
	// DCT16
	PSHUFD $0x1b, X9, X9
	VADDPS X9, X7, X1
	VSUBPS X9, X7, X11
	DIVPS  dct16<>+16(SB), X11
	PSHUFD $0x1b, X0, X9
	VADDPS X9, X5, X0
	VSUBPS X9, X5, X9
	DIVPS  dct16<>+0(SB), X9

	// DCT8
	PSHUFD $0x1b, X1, X5
	VADDPS X5, X0, X7
	VSUBPS X5, X0, X0
	DIVPS  dct8<>+0(SB), X0

	// DCT4
	PSHUFD     $0xb4, X7, X7
	PSHUFD     $0xb4, X0, X0
	VPUNPCKLDQ X0, X7, X1
	VPUNPCKHDQ X0, X7, X5
	VADDPS     X5, X1, X7
	VSUBPS     X5, X1, X0
	DIVPS      dct4<>+0(SB), X0
	VPUNPCKLDQ X0, X7, X1
	VPUNPCKHDQ X0, X7, X5

	// DCT2
	PSHUFD    $0xd8, X1, X7
	PSHUFD    $0xd8, X5, X0
	VADDPS    X0, X7, X1
	VSUBPS    X0, X7, X0
	DIVPS     dct2<>+0(SB), X0
	VADDPS    X1, X0, X7
	VBLENDPS  $0x03, X1, X7, X7
	VSHUFPS   $0x88, X0, X7, X1
	VSHUFPS   $0xdd, X0, X7, X5
	VPSRLDQ   $0x04, X5, X0
	ADDPS     X0, X5
	VUNPCKLPS X5, X1, X0
	VUNPCKHPS X5, X1, X1

	// end DCT8
	// DCT8
	PSHUFD $0x1b, X11, X7
	VADDPS X7, X9, X11
	VSUBPS X7, X9, X9
	DIVPS  dct8<>+0(SB), X9

	// DCT4
	PSHUFD     $0xb4, X11, X11
	PSHUFD     $0xb4, X9, X9
	VPUNPCKLDQ X9, X11, X5
	VPUNPCKHDQ X9, X11, X7
	VADDPS     X7, X5, X11
	VSUBPS     X7, X5, X9
	DIVPS      dct4<>+0(SB), X9
	VPUNPCKLDQ X9, X11, X5
	VPUNPCKHDQ X9, X11, X7

	// DCT2
	PSHUFD    $0xd8, X5, X11
	PSHUFD    $0xd8, X7, X9
	VADDPS    X9, X11, X5
	VSUBPS    X9, X11, X9
	DIVPS     dct2<>+0(SB), X9
	VADDPS    X5, X9, X11
	VBLENDPS  $0x03, X5, X11, X11
	VSHUFPS   $0x88, X9, X11, X5
	VSHUFPS   $0xdd, X9, X11, X7
	VPSRLDQ   $0x04, X7, X9
	ADDPS     X9, X7
	VUNPCKLPS X7, X5, X9
	VUNPCKHPS X7, X5, X11

	// end DCT8
	VPSRLDQ   $0x04, X9, X5
	VPSLLDQ   $0x0c, X11, X7
	VADDPS    X5, X7, X5
	VADDPS    X5, X9, X9
	VPSRLDQ   $0x04, X11, X7
	VADDPS    X7, X11, X11
	VUNPCKLPS X9, X0, X5
	VUNPCKHPS X9, X0, X7
	VUNPCKLPS X11, X1, X9
	VUNPCKHPS X11, X1, X0

	// end DCT16
	VPSRLDQ   $0x04, X5, X1
	VPSLLDQ   $0x0c, X7, X11
	VADDPS    X1, X11, X1
	VADDPS    X1, X5, X5
	VUNPCKLPS X5, X4, X1
	MOVUPS    X1, (AX)(BX*4)
	VUNPCKHPS X5, X4, X11
	MOVUPS    X11, 16(AX)(BX*4)
	VPSRLDQ   $0x04, X7, X1
	VPSLLDQ   $0x0c, X9, X11
	VADDPS    X1, X11, X1
	VADDPS    X1, X7, X7
	VUNPCKLPS X7, X6, X1
	MOVUPS    X1, 32(AX)(BX*4)
	VUNPCKHPS X7, X6, X11
	MOVUPS    X11, 48(AX)(BX*4)
	VPSRLDQ   $0x04, X9, X1
	VPSLLDQ   $0x0c, X0, X11
	VADDPS    X1, X11, X1
	VADDPS    X1, X9, X9
	VUNPCKLPS X9, X8, X1
	MOVUPS    X1, 64(AX)(BX*4)
	VUNPCKHPS X9, X8, X11
	MOVUPS    X11, 80(AX)(BX*4)
	VPSRLDQ   $0x04, X0, X11
	VADDPS    X11, X0, X0
	VUNPCKLPS X0, X10, X1
	MOVUPS    X1, 96(AX)(BX*4)
	VUNPCKHPS X0, X10, X11
	MOVUPS    X11, 112(AX)(BX*4)

	// end DCT32
	// DCT32
	MOVUPS 128(AX)(BX*4), X0
	MOVUPS 240(AX)(BX*4), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X4
	VSUBPS X1, X0, X5
	DIVPS  dct32<>+0(SB), X5
	MOVUPS 144(AX)(BX*4), X0
	MOVUPS 224(AX)(BX*4), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X6
	VSUBPS X1, X0, X7
	DIVPS  dct32<>+16(SB), X7
	MOVUPS 160(AX)(BX*4), X0
	MOVUPS 208(AX)(BX*4), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X8
	VSUBPS X1, X0, X9
	DIVPS  dct32<>+32(SB), X9
	MOVUPS 176(AX)(BX*4), X0
	MOVUPS 192(AX)(BX*4), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X10
	VSUBPS X1, X0, X0
	DIVPS  dct32<>+48(SB), X0

	// DCT16
	PSHUFD $0x1b, X8, X8
	VADDPS X8, X6, X1
	VSUBPS X8, X6, X11
	DIVPS  dct16<>+16(SB), X11
	PSHUFD $0x1b, X10, X8
	VADDPS X8, X4, X10
	VSUBPS X8, X4, X8
	DIVPS  dct16<>+0(SB), X8

	// DCT8
	PSHUFD $0x1b, X1, X4
	VADDPS X4, X10, X6
	VSUBPS X4, X10, X10
	DIVPS  dct8<>+0(SB), X10

	// DCT4
	PSHUFD     $0xb4, X6, X6
	PSHUFD     $0xb4, X10, X10
	VPUNPCKLDQ X10, X6, X1
	VPUNPCKHDQ X10, X6, X4
	VADDPS     X4, X1, X6
	VSUBPS     X4, X1, X10
	DIVPS      dct4<>+0(SB), X10
	VPUNPCKLDQ X10, X6, X1
	VPUNPCKHDQ X10, X6, X4

	// DCT2
	PSHUFD    $0xd8, X1, X6
	PSHUFD    $0xd8, X4, X10
	VADDPS    X10, X6, X1
	VSUBPS    X10, X6, X10
	DIVPS     dct2<>+0(SB), X10
	VADDPS    X1, X10, X6
	VBLENDPS  $0x03, X1, X6, X6
	VSHUFPS   $0x88, X10, X6, X1
	VSHUFPS   $0xdd, X10, X6, X4
	VPSRLDQ   $0x04, X4, X10
	ADDPS     X10, X4
	VUNPCKLPS X4, X1, X10
	VUNPCKHPS X4, X1, X1

	// end DCT8
	// DCT8
	PSHUFD $0x1b, X11, X6
	VADDPS X6, X8, X11
	VSUBPS X6, X8, X8
	DIVPS  dct8<>+0(SB), X8

	// DCT4
	PSHUFD     $0xb4, X11, X11
	PSHUFD     $0xb4, X8, X8
	VPUNPCKLDQ X8, X11, X4
	VPUNPCKHDQ X8, X11, X6
	VADDPS     X6, X4, X11
	VSUBPS     X6, X4, X8
	DIVPS      dct4<>+0(SB), X8
	VPUNPCKLDQ X8, X11, X4
	VPUNPCKHDQ X8, X11, X6

	// DCT2
	PSHUFD    $0xd8, X4, X11
	PSHUFD    $0xd8, X6, X8
	VADDPS    X8, X11, X4
	VSUBPS    X8, X11, X8
	DIVPS     dct2<>+0(SB), X8
	VADDPS    X4, X8, X11
	VBLENDPS  $0x03, X4, X11, X11
	VSHUFPS   $0x88, X8, X11, X4
	VSHUFPS   $0xdd, X8, X11, X6
	VPSRLDQ   $0x04, X6, X8
	ADDPS     X8, X6
	VUNPCKLPS X6, X4, X8
	VUNPCKHPS X6, X4, X11

	// end DCT8
	VPSRLDQ   $0x04, X8, X4
	VPSLLDQ   $0x0c, X11, X6
	VADDPS    X4, X6, X4
	VADDPS    X4, X8, X8
	VPSRLDQ   $0x04, X11, X6
	VADDPS    X6, X11, X11
	VUNPCKLPS X8, X10, X4
	VUNPCKHPS X8, X10, X6
	VUNPCKLPS X11, X1, X8
	VUNPCKHPS X11, X1, X10

	// end DCT16
	// DCT16
	PSHUFD $0x1b, X9, X9
	VADDPS X9, X7, X1
	VSUBPS X9, X7, X11
	DIVPS  dct16<>+16(SB), X11
	PSHUFD $0x1b, X0, X9
	VADDPS X9, X5, X0
	VSUBPS X9, X5, X9
	DIVPS  dct16<>+0(SB), X9

	// DCT8
	PSHUFD $0x1b, X1, X5
	VADDPS X5, X0, X7
	VSUBPS X5, X0, X0
	DIVPS  dct8<>+0(SB), X0

	// DCT4
	PSHUFD     $0xb4, X7, X7
	PSHUFD     $0xb4, X0, X0
	VPUNPCKLDQ X0, X7, X1
	VPUNPCKHDQ X0, X7, X5
	VADDPS     X5, X1, X7
	VSUBPS     X5, X1, X0
	DIVPS      dct4<>+0(SB), X0
	VPUNPCKLDQ X0, X7, X1
	VPUNPCKHDQ X0, X7, X5

	// DCT2
	PSHUFD    $0xd8, X1, X7
	PSHUFD    $0xd8, X5, X0
	VADDPS    X0, X7, X1
	VSUBPS    X0, X7, X0
	DIVPS     dct2<>+0(SB), X0
	VADDPS    X1, X0, X7
	VBLENDPS  $0x03, X1, X7, X7
	VSHUFPS   $0x88, X0, X7, X1
	VSHUFPS   $0xdd, X0, X7, X5
	VPSRLDQ   $0x04, X5, X0
	ADDPS     X0, X5
	VUNPCKLPS X5, X1, X0
	VUNPCKHPS X5, X1, X1

	// end DCT8
	// DCT8
	PSHUFD $0x1b, X11, X7
	VADDPS X7, X9, X11
	VSUBPS X7, X9, X9
	DIVPS  dct8<>+0(SB), X9

	// DCT4
	PSHUFD     $0xb4, X11, X11
	PSHUFD     $0xb4, X9, X9
	VPUNPCKLDQ X9, X11, X5
	VPUNPCKHDQ X9, X11, X7
	VADDPS     X7, X5, X11
	VSUBPS     X7, X5, X9
	DIVPS      dct4<>+0(SB), X9
	VPUNPCKLDQ X9, X11, X5
	VPUNPCKHDQ X9, X11, X7

	// DCT2
	PSHUFD    $0xd8, X5, X11
	PSHUFD    $0xd8, X7, X9
	VADDPS    X9, X11, X5
	VSUBPS    X9, X11, X9
	DIVPS     dct2<>+0(SB), X9
	VADDPS    X5, X9, X11
	VBLENDPS  $0x03, X5, X11, X11
	VSHUFPS   $0x88, X9, X11, X5
	VSHUFPS   $0xdd, X9, X11, X7
	VPSRLDQ   $0x04, X7, X9
	ADDPS     X9, X7
	VUNPCKLPS X7, X5, X9
	VUNPCKHPS X7, X5, X11

	// end DCT8
	VPSRLDQ   $0x04, X9, X5
	VPSLLDQ   $0x0c, X11, X7
	VADDPS    X5, X7, X5
	VADDPS    X5, X9, X9
	VPSRLDQ   $0x04, X11, X7
	VADDPS    X7, X11, X11
	VUNPCKLPS X9, X0, X5
	VUNPCKHPS X9, X0, X7
	VUNPCKLPS X11, X1, X9
	VUNPCKHPS X11, X1, X0

	// end DCT16
	VPSRLDQ   $0x04, X5, X1
	VPSLLDQ   $0x0c, X7, X11
	VADDPS    X1, X11, X1
	VADDPS    X1, X5, X5
	VUNPCKLPS X5, X4, X1
	MOVUPS    X1, 128(AX)(BX*4)
	VUNPCKHPS X5, X4, X11
	MOVUPS    X11, 144(AX)(BX*4)
	VPSRLDQ   $0x04, X7, X1
	VPSLLDQ   $0x0c, X9, X11
	VADDPS    X1, X11, X1
	VADDPS    X1, X7, X7
	VUNPCKLPS X7, X6, X1
	MOVUPS    X1, 160(AX)(BX*4)
	VUNPCKHPS X7, X6, X11
	MOVUPS    X11, 176(AX)(BX*4)
	VPSRLDQ   $0x04, X9, X1
	VPSLLDQ   $0x0c, X0, X11
	VADDPS    X1, X11, X1
	VADDPS    X1, X9, X9
	VUNPCKLPS X9, X8, X1
	MOVUPS    X1, 192(AX)(BX*4)
	VUNPCKHPS X9, X8, X11
	MOVUPS    X11, 208(AX)(BX*4)
	VPSRLDQ   $0x04, X0, X11
	VADDPS    X11, X0, X0
	VUNPCKLPS X0, X10, X1
	MOVUPS    X1, 224(AX)(BX*4)
	VUNPCKHPS X0, X10, X11
	MOVUPS    X11, 240(AX)(BX*4)

	// end DCT32
	VZEROUPPER
	MOVL       252(AX)(BX*4), SI
	VMOVUPS    (AX)(BX*4), Y0
	VMOVUPS    32(AX)(BX*4), Y1
	VMOVUPS    64(AX)(BX*4), Y4
	VMOVUPS    96(AX)(BX*4), Y5
	VMOVUPS    128(AX)(BX*4), Y10
	VADDPS     132(AX)(BX*4), Y10, Y6
	VUNPCKLPS  Y6, Y0, Y10
	VUNPCKHPS  Y6, Y0, Y6
	VPERM2F128 $0x02, Y10, Y6, Y0
	VMOVUPS    Y0, (AX)(BX*4)
	VPERM2F128 $0x13, Y10, Y6, Y0
	VMOVUPS    Y0, 32(AX)(BX*4)
	VMOVUPS    160(AX)(BX*4), Y10
	VADDPS     164(AX)(BX*4), Y10, Y6
	VUNPCKLPS  Y6, Y1, Y10
	VUNPCKHPS  Y6, Y1, Y6
	VPERM2F128 $0x02, Y10, Y6, Y0
	VMOVUPS    Y0, 64(AX)(BX*4)
	VPERM2F128 $0x13, Y10, Y6, Y0
	VMOVUPS    Y0, 96(AX)(BX*4)
	VMOVUPS    192(AX)(BX*4), Y10
	VADDPS     196(AX)(BX*4), Y10, Y6
	VUNPCKLPS  Y6, Y4, Y10
	VUNPCKHPS  Y6, Y4, Y6
	VPERM2F128 $0x02, Y10, Y6, Y0
	VMOVUPS    Y0, 128(AX)(BX*4)
	VPERM2F128 $0x13, Y10, Y6, Y0
	VMOVUPS    Y0, 160(AX)(BX*4)
	VMOVUPS    224(AX)(BX*4), Y10
	VPMOVZXBD  perm<>+8(SB), Y9
	VPERMPS    224(AX)(BX*4), Y9, Y6
	VADDPS     Y6, Y10, Y6
	VUNPCKLPS  Y6, Y5, Y10
	VUNPCKHPS  Y6, Y5, Y6
	VPERM2F128 $0x02, Y10, Y6, Y0
	VMOVUPS    Y0, 192(AX)(BX*4)
	VPERM2F128 $0x13, Y10, Y6, Y0
	VMOVUPS    Y0, 224(AX)(BX*4)
	MOVL       SI, 252(AX)(BX*4)
	VZEROUPPER

	// end DCT64
	// End innerloop instructions
	INCL DX
	JMP  j

i:
	CMPL CX, $0x08
	JE   done

	// Start innerloop instructions
	// --Loop load DCT64 values
	MOVL         CX, 256(SP)
	VZEROUPPER
	VPMOVZXBD    perm<>+0(SB), Y0
	VPBROADCASTD 256(SP), Y1
	VPADDD       gather<>+0(SB), Y1, Y1
	VPCMPEQD     Y4, Y4, Y4
	VPGATHERDD   Y4, (AX)(Y1*4), Y2
	VPCMPEQD     Y4, Y4, Y4
	VPGATHERDD   Y4, 14336(AX)(Y1*4), Y3
	VPERMD       Y3, Y0, Y3
	VADDPS       Y3, Y2, Y4
	VMOVUPS      Y4, (SP)
	VSUBPS       Y3, Y2, Y4
	VDIVPS       dct64<>+0(SB), Y4, Y4
	VMOVUPS      Y4, 128(SP)
	VPCMPEQD     Y4, Y4, Y4
	VPGATHERDD   Y4, 2048(AX)(Y1*4), Y2
	VPCMPEQD     Y4, Y4, Y4
	VPGATHERDD   Y4, 12288(AX)(Y1*4), Y3
	VPERMD       Y3, Y0, Y3
	VADDPS       Y3, Y2, Y4
	VMOVUPS      Y4, 32(SP)
	VSUBPS       Y3, Y2, Y4
	VDIVPS       dct64<>+32(SB), Y4, Y4
	VMOVUPS      Y4, 160(SP)
	VPCMPEQD     Y4, Y4, Y4
	VPGATHERDD   Y4, 4096(AX)(Y1*4), Y2
	VPCMPEQD     Y4, Y4, Y4
	VPGATHERDD   Y4, 10240(AX)(Y1*4), Y3
	VPERMD       Y3, Y0, Y3
	VADDPS       Y3, Y2, Y4
	VMOVUPS      Y4, 64(SP)
	VSUBPS       Y3, Y2, Y4
	VDIVPS       dct64<>+64(SB), Y4, Y4
	VMOVUPS      Y4, 192(SP)
	VPCMPEQD     Y4, Y4, Y4
	VPGATHERDD   Y4, 6144(AX)(Y1*4), Y2
	VPCMPEQD     Y4, Y4, Y4
	VPGATHERDD   Y4, 8192(AX)(Y1*4), Y3
	VPERMD       Y3, Y0, Y3
	VADDPS       Y3, Y2, Y4
	VMOVUPS      Y4, 96(SP)
	VSUBPS       Y3, Y2, Y4
	VDIVPS       dct64<>+96(SB), Y4, Y4
	VMOVUPS      Y4, 224(SP)

	// --Loop load DCT64 values
	VZEROUPPER

	// DCT32 Unaligned
	MOVUPS (SP), X0
	MOVUPS 112(SP), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X4
	VSUBPS X1, X0, X5
	DIVPS  dct32<>+0(SB), X5
	MOVUPS 16(SP), X0
	MOVUPS 96(SP), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X6
	VSUBPS X1, X0, X7
	DIVPS  dct32<>+16(SB), X7
	MOVUPS 32(SP), X0
	MOVUPS 80(SP), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X8
	VSUBPS X1, X0, X9
	DIVPS  dct32<>+32(SB), X9
	MOVUPS 48(SP), X0
	MOVUPS 64(SP), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X10
	VSUBPS X1, X0, X0
	DIVPS  dct32<>+48(SB), X0

	// DCT16
	PSHUFD $0x1b, X8, X8
	VADDPS X8, X6, X1
	VSUBPS X8, X6, X11
	DIVPS  dct16<>+16(SB), X11
	PSHUFD $0x1b, X10, X8
	VADDPS X8, X4, X10
	VSUBPS X8, X4, X8
	DIVPS  dct16<>+0(SB), X8

	// DCT8
	PSHUFD $0x1b, X1, X4
	VADDPS X4, X10, X6
	VSUBPS X4, X10, X10
	DIVPS  dct8<>+0(SB), X10

	// DCT4
	PSHUFD     $0xb4, X6, X6
	PSHUFD     $0xb4, X10, X10
	VPUNPCKLDQ X10, X6, X1
	VPUNPCKHDQ X10, X6, X4
	VADDPS     X4, X1, X6
	VSUBPS     X4, X1, X10
	DIVPS      dct4<>+0(SB), X10
	VPUNPCKLDQ X10, X6, X1
	VPUNPCKHDQ X10, X6, X4

	// DCT2
	PSHUFD    $0xd8, X1, X6
	PSHUFD    $0xd8, X4, X10
	VADDPS    X10, X6, X1
	VSUBPS    X10, X6, X10
	DIVPS     dct2<>+0(SB), X10
	VADDPS    X1, X10, X6
	VBLENDPS  $0x03, X1, X6, X6
	VSHUFPS   $0x88, X10, X6, X1
	VSHUFPS   $0xdd, X10, X6, X4
	VPSRLDQ   $0x04, X4, X10
	ADDPS     X10, X4
	VUNPCKLPS X4, X1, X10
	VUNPCKHPS X4, X1, X1

	// end DCT8
	// DCT8
	PSHUFD $0x1b, X11, X6
	VADDPS X6, X8, X11
	VSUBPS X6, X8, X8
	DIVPS  dct8<>+0(SB), X8

	// DCT4
	PSHUFD     $0xb4, X11, X11
	PSHUFD     $0xb4, X8, X8
	VPUNPCKLDQ X8, X11, X4
	VPUNPCKHDQ X8, X11, X6
	VADDPS     X6, X4, X11
	VSUBPS     X6, X4, X8
	DIVPS      dct4<>+0(SB), X8
	VPUNPCKLDQ X8, X11, X4
	VPUNPCKHDQ X8, X11, X6

	// DCT2
	PSHUFD    $0xd8, X4, X11
	PSHUFD    $0xd8, X6, X8
	VADDPS    X8, X11, X4
	VSUBPS    X8, X11, X8
	DIVPS     dct2<>+0(SB), X8
	VADDPS    X4, X8, X11
	VBLENDPS  $0x03, X4, X11, X11
	VSHUFPS   $0x88, X8, X11, X4
	VSHUFPS   $0xdd, X8, X11, X6
	VPSRLDQ   $0x04, X6, X8
	ADDPS     X8, X6
	VUNPCKLPS X6, X4, X8
	VUNPCKHPS X6, X4, X11

	// end DCT8
	VPSRLDQ   $0x04, X8, X4
	VPSLLDQ   $0x0c, X11, X6
	VADDPS    X4, X6, X4
	VADDPS    X4, X8, X8
	VPSRLDQ   $0x04, X11, X6
	VADDPS    X6, X11, X11
	VUNPCKLPS X8, X10, X4
	VUNPCKHPS X8, X10, X6
	VUNPCKLPS X11, X1, X8
	VUNPCKHPS X11, X1, X10

	// end DCT16
	// DCT16
	PSHUFD $0x1b, X9, X8
	VADDPS X8, X7, X1
	VSUBPS X8, X7, X6
	DIVPS  dct16<>+16(SB), X6
	PSHUFD $0x1b, X0, X8
	VADDPS X8, X5, X0
	VSUBPS X8, X5, X8
	DIVPS  dct16<>+0(SB), X8

	// DCT8
	PSHUFD $0x1b, X1, X5
	VADDPS X5, X0, X7
	VSUBPS X5, X0, X0
	DIVPS  dct8<>+0(SB), X0

	// DCT4
	PSHUFD     $0xb4, X7, X7
	PSHUFD     $0xb4, X0, X0
	VPUNPCKLDQ X0, X7, X1
	VPUNPCKHDQ X0, X7, X5
	VADDPS     X5, X1, X7
	VSUBPS     X5, X1, X0
	DIVPS      dct4<>+0(SB), X0
	VPUNPCKLDQ X0, X7, X1
	VPUNPCKHDQ X0, X7, X5

	// DCT2
	PSHUFD    $0xd8, X1, X7
	PSHUFD    $0xd8, X5, X0
	VADDPS    X0, X7, X1
	VSUBPS    X0, X7, X0
	DIVPS     dct2<>+0(SB), X0
	VADDPS    X1, X0, X7
	VBLENDPS  $0x03, X1, X7, X7
	VSHUFPS   $0x88, X0, X7, X1
	VSHUFPS   $0xdd, X0, X7, X5
	VPSRLDQ   $0x04, X5, X0
	ADDPS     X0, X5
	VUNPCKLPS X5, X1, X0
	VUNPCKHPS X5, X1, X1

	// end DCT8
	// DCT8
	PSHUFD $0x1b, X6, X6
	VADDPS X6, X8, X7
	VSUBPS X6, X8, X8
	DIVPS  dct8<>+0(SB), X8

	// DCT4
	PSHUFD     $0xb4, X7, X7
	PSHUFD     $0xb4, X8, X8
	VPUNPCKLDQ X8, X7, X5
	VPUNPCKHDQ X8, X7, X6
	VADDPS     X6, X5, X7
	VSUBPS     X6, X5, X8
	DIVPS      dct4<>+0(SB), X8
	VPUNPCKLDQ X8, X7, X5
	VPUNPCKHDQ X8, X7, X6

	// DCT2
	PSHUFD    $0xd8, X5, X7
	PSHUFD    $0xd8, X6, X8
	VADDPS    X8, X7, X5
	VSUBPS    X8, X7, X8
	DIVPS     dct2<>+0(SB), X8
	VADDPS    X5, X8, X7
	VBLENDPS  $0x03, X5, X7, X7
	VSHUFPS   $0x88, X8, X7, X5
	VSHUFPS   $0xdd, X8, X7, X6
	VPSRLDQ   $0x04, X6, X8
	ADDPS     X8, X6
	VUNPCKLPS X6, X5, X8
	VUNPCKHPS X6, X5, X6

	// end DCT8
	VPSRLDQ   $0x04, X8, X5
	VPSLLDQ   $0x0c, X6, X7
	VADDPS    X5, X7, X5
	VADDPS    X5, X8, X8
	VPSRLDQ   $0x04, X6, X7
	VADDPS    X7, X6, X6
	VUNPCKLPS X8, X0, X5
	VUNPCKHPS X8, X0, X7
	VUNPCKLPS X6, X1, X9
	VUNPCKHPS X6, X1, X0

	// end DCT16
	VPSRLDQ   $0x04, X5, X0
	VPSLLDQ   $0x0c, X7, X1
	VADDPS    X0, X1, X0
	VADDPS    X0, X5, X5
	VUNPCKLPS X5, X4, X0
	MOVUPS    X0, (SP)
	VUNPCKHPS X5, X4, X0
	MOVUPS    X0, 16(SP)

	// end DCT32 Unaligned
	// DCT32 Unaligned
	MOVUPS 128(SP), X0
	MOVUPS 240(SP), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X4
	VSUBPS X1, X0, X5
	DIVPS  dct32<>+0(SB), X5
	MOVUPS 144(SP), X0
	MOVUPS 224(SP), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X6
	VSUBPS X1, X0, X7
	DIVPS  dct32<>+16(SB), X7
	MOVUPS 160(SP), X0
	MOVUPS 208(SP), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X8
	VSUBPS X1, X0, X9
	DIVPS  dct32<>+32(SB), X9
	MOVUPS 176(SP), X0
	MOVUPS 192(SP), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X10
	VSUBPS X1, X0, X0
	DIVPS  dct32<>+48(SB), X0

	// DCT16
	PSHUFD $0x1b, X8, X8
	VADDPS X8, X6, X1
	VSUBPS X8, X6, X11
	DIVPS  dct16<>+16(SB), X11
	PSHUFD $0x1b, X10, X8
	VADDPS X8, X4, X10
	VSUBPS X8, X4, X8
	DIVPS  dct16<>+0(SB), X8

	// DCT8
	PSHUFD $0x1b, X1, X4
	VADDPS X4, X10, X6
	VSUBPS X4, X10, X10
	DIVPS  dct8<>+0(SB), X10

	// DCT4
	PSHUFD     $0xb4, X6, X6
	PSHUFD     $0xb4, X10, X10
	VPUNPCKLDQ X10, X6, X1
	VPUNPCKHDQ X10, X6, X4
	VADDPS     X4, X1, X6
	VSUBPS     X4, X1, X10
	DIVPS      dct4<>+0(SB), X10
	VPUNPCKLDQ X10, X6, X1
	VPUNPCKHDQ X10, X6, X4

	// DCT2
	PSHUFD    $0xd8, X1, X6
	PSHUFD    $0xd8, X4, X10
	VADDPS    X10, X6, X1
	VSUBPS    X10, X6, X10
	DIVPS     dct2<>+0(SB), X10
	VADDPS    X1, X10, X6
	VBLENDPS  $0x03, X1, X6, X6
	VSHUFPS   $0x88, X10, X6, X1
	VSHUFPS   $0xdd, X10, X6, X4
	VPSRLDQ   $0x04, X4, X10
	ADDPS     X10, X4
	VUNPCKLPS X4, X1, X10
	VUNPCKHPS X4, X1, X1

	// end DCT8
	// DCT8
	PSHUFD $0x1b, X11, X6
	VADDPS X6, X8, X11
	VSUBPS X6, X8, X8
	DIVPS  dct8<>+0(SB), X8

	// DCT4
	PSHUFD     $0xb4, X11, X11
	PSHUFD     $0xb4, X8, X8
	VPUNPCKLDQ X8, X11, X4
	VPUNPCKHDQ X8, X11, X6
	VADDPS     X6, X4, X11
	VSUBPS     X6, X4, X8
	DIVPS      dct4<>+0(SB), X8
	VPUNPCKLDQ X8, X11, X4
	VPUNPCKHDQ X8, X11, X6

	// DCT2
	PSHUFD    $0xd8, X4, X11
	PSHUFD    $0xd8, X6, X8
	VADDPS    X8, X11, X4
	VSUBPS    X8, X11, X8
	DIVPS     dct2<>+0(SB), X8
	VADDPS    X4, X8, X11
	VBLENDPS  $0x03, X4, X11, X11
	VSHUFPS   $0x88, X8, X11, X4
	VSHUFPS   $0xdd, X8, X11, X6
	VPSRLDQ   $0x04, X6, X8
	ADDPS     X8, X6
	VUNPCKLPS X6, X4, X8
	VUNPCKHPS X6, X4, X11

	// end DCT8
	VPSRLDQ   $0x04, X8, X4
	VPSLLDQ   $0x0c, X11, X6
	VADDPS    X4, X6, X4
	VADDPS    X4, X8, X8
	VPSRLDQ   $0x04, X11, X6
	VADDPS    X6, X11, X11
	VUNPCKLPS X8, X10, X4
	VUNPCKHPS X8, X10, X6
	VUNPCKLPS X11, X1, X8
	VUNPCKHPS X11, X1, X10

	// end DCT16
	// DCT16
	PSHUFD $0x1b, X9, X8
	VADDPS X8, X7, X1
	VSUBPS X8, X7, X6
	DIVPS  dct16<>+16(SB), X6
	PSHUFD $0x1b, X0, X8
	VADDPS X8, X5, X0
	VSUBPS X8, X5, X8
	DIVPS  dct16<>+0(SB), X8

	// DCT8
	PSHUFD $0x1b, X1, X5
	VADDPS X5, X0, X7
	VSUBPS X5, X0, X0
	DIVPS  dct8<>+0(SB), X0

	// DCT4
	PSHUFD     $0xb4, X7, X7
	PSHUFD     $0xb4, X0, X0
	VPUNPCKLDQ X0, X7, X1
	VPUNPCKHDQ X0, X7, X5
	VADDPS     X5, X1, X7
	VSUBPS     X5, X1, X0
	DIVPS      dct4<>+0(SB), X0
	VPUNPCKLDQ X0, X7, X1
	VPUNPCKHDQ X0, X7, X5

	// DCT2
	PSHUFD    $0xd8, X1, X7
	PSHUFD    $0xd8, X5, X0
	VADDPS    X0, X7, X1
	VSUBPS    X0, X7, X0
	DIVPS     dct2<>+0(SB), X0
	VADDPS    X1, X0, X7
	VBLENDPS  $0x03, X1, X7, X7
	VSHUFPS   $0x88, X0, X7, X1
	VSHUFPS   $0xdd, X0, X7, X5
	VPSRLDQ   $0x04, X5, X0
	ADDPS     X0, X5
	VUNPCKLPS X5, X1, X0
	VUNPCKHPS X5, X1, X1

	// end DCT8
	// DCT8
	PSHUFD $0x1b, X6, X6
	VADDPS X6, X8, X7
	VSUBPS X6, X8, X8
	DIVPS  dct8<>+0(SB), X8

	// DCT4
	PSHUFD     $0xb4, X7, X7
	PSHUFD     $0xb4, X8, X8
	VPUNPCKLDQ X8, X7, X5
	VPUNPCKHDQ X8, X7, X6
	VADDPS     X6, X5, X7
	VSUBPS     X6, X5, X8
	DIVPS      dct4<>+0(SB), X8
	VPUNPCKLDQ X8, X7, X5
	VPUNPCKHDQ X8, X7, X6

	// DCT2
	PSHUFD    $0xd8, X5, X7
	PSHUFD    $0xd8, X6, X8
	VADDPS    X8, X7, X5
	VSUBPS    X8, X7, X8
	DIVPS     dct2<>+0(SB), X8
	VADDPS    X5, X8, X7
	VBLENDPS  $0x03, X5, X7, X7
	VSHUFPS   $0x88, X8, X7, X5
	VSHUFPS   $0xdd, X8, X7, X6
	VPSRLDQ   $0x04, X6, X8
	ADDPS     X8, X6
	VUNPCKLPS X6, X5, X8
	VUNPCKHPS X6, X5, X6

	// end DCT8
	VPSRLDQ   $0x04, X8, X5
	VPSLLDQ   $0x0c, X6, X7
	VADDPS    X5, X7, X5
	VADDPS    X5, X8, X8
	VPSRLDQ   $0x04, X6, X7
	VADDPS    X7, X6, X6
	VUNPCKLPS X8, X0, X5
	VUNPCKHPS X8, X0, X7
	VUNPCKLPS X6, X1, X9
	VUNPCKHPS X6, X1, X0

	// end DCT16
	VPSRLDQ   $0x04, X5, X0
	VPSLLDQ   $0x0c, X7, X1
	VADDPS    X0, X1, X0
	VADDPS    X0, X5, X5
	VUNPCKLPS X5, X4, X0
	MOVUPS    X0, 128(SP)
	VUNPCKHPS X5, X4, X0
	MOVUPS    X0, 144(SP)

	// end DCT32 Unaligned
	MOVUPS (SP), X0
	PEXTRD $0x00, X0, ret_0+24(FP)(CX*4)
	PEXTRD $0x01, X0, ret_16+88(FP)(CX*4)
	PEXTRD $0x02, X0, ret_32+152(FP)(CX*4)
	PEXTRD $0x03, X0, ret_48+216(FP)(CX*4)
	MOVUPS 128(SP), X0
	MOVUPS 132(SP), X1
	ADDPS  X0, X1
	PEXTRD $0x00, X1, ret_8+56(FP)(CX*4)
	PEXTRD $0x01, X1, ret_24+120(FP)(CX*4)
	PEXTRD $0x02, X1, ret_40+184(FP)(CX*4)
	PEXTRD $0x03, X1, ret_56+248(FP)(CX*4)

	// End innerloop instructions
	INCL CX
	JMP  i

done:
	RET

// func asmForwardDCT64(input []float32)
// Requires: AVX, AVX2, SSE, SSE2
TEXT ·asmForwardDCT64(SB), NOSPLIT|NOPTR, $0-24
	MOVQ input_base+0(FP), AX
	VZEROUPPER

	// DCT64
	VPMOVZXBD perm<>+0(SB), Y7
	VMOVUPS   (AX), Y0
	VMOVUPS   32(AX), Y1
	VMOVUPS   64(AX), Y2
	VMOVUPS   96(AX), Y3
	VPERMD    128(AX), Y7, Y4
	VPERMD    160(AX), Y7, Y5
	VPERMD    192(AX), Y7, Y6
	VPERMD    224(AX), Y7, Y7
	VADDPS    Y7, Y0, Y8
	VMOVUPS   Y8, (AX)
	VSUBPS    Y7, Y0, Y8
	VDIVPS    dct64<>+0(SB), Y8, Y8
	VMOVUPS   Y8, 128(AX)
	VADDPS    Y6, Y1, Y8
	VMOVUPS   Y8, 32(AX)
	VSUBPS    Y6, Y1, Y8
	VDIVPS    dct64<>+32(SB), Y8, Y8
	VMOVUPS   Y8, 160(AX)
	VADDPS    Y5, Y2, Y8
	VMOVUPS   Y8, 64(AX)
	VSUBPS    Y5, Y2, Y8
	VDIVPS    dct64<>+64(SB), Y8, Y8
	VMOVUPS   Y8, 192(AX)
	VADDPS    Y4, Y3, Y8
	VMOVUPS   Y8, 96(AX)
	VSUBPS    Y4, Y3, Y8
	VDIVPS    dct64<>+96(SB), Y8, Y8
	VMOVUPS   Y8, 224(AX)
	VZEROALL

	// DCT32
	MOVUPS (AX), X0
	MOVUPS 112(AX), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X2
	VSUBPS X1, X0, X3
	DIVPS  dct32<>+0(SB), X3
	MOVUPS 16(AX), X0
	MOVUPS 96(AX), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X4
	VSUBPS X1, X0, X5
	DIVPS  dct32<>+16(SB), X5
	MOVUPS 32(AX), X0
	MOVUPS 80(AX), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X6
	VSUBPS X1, X0, X7
	DIVPS  dct32<>+32(SB), X7
	MOVUPS 48(AX), X0
	MOVUPS 64(AX), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X8
	VSUBPS X1, X0, X0
	DIVPS  dct32<>+48(SB), X0

	// DCT16
	PSHUFD $0x1b, X6, X6
	VADDPS X6, X4, X1
	VSUBPS X6, X4, X9
	DIVPS  dct16<>+16(SB), X9
	PSHUFD $0x1b, X8, X6
	VADDPS X6, X2, X8
	VSUBPS X6, X2, X6
	DIVPS  dct16<>+0(SB), X6

	// DCT8
	PSHUFD $0x1b, X1, X2
	VADDPS X2, X8, X4
	VSUBPS X2, X8, X8
	DIVPS  dct8<>+0(SB), X8

	// DCT4
	PSHUFD     $0xb4, X4, X4
	PSHUFD     $0xb4, X8, X8
	VPUNPCKLDQ X8, X4, X1
	VPUNPCKHDQ X8, X4, X2
	VADDPS     X2, X1, X4
	VSUBPS     X2, X1, X8
	DIVPS      dct4<>+0(SB), X8
	VPUNPCKLDQ X8, X4, X1
	VPUNPCKHDQ X8, X4, X2

	// DCT2
	PSHUFD    $0xd8, X1, X4
	PSHUFD    $0xd8, X2, X8
	VADDPS    X8, X4, X1
	VSUBPS    X8, X4, X8
	DIVPS     dct2<>+0(SB), X8
	VADDPS    X1, X8, X4
	VBLENDPS  $0x03, X1, X4, X4
	VSHUFPS   $0x88, X8, X4, X1
	VSHUFPS   $0xdd, X8, X4, X2
	VPSRLDQ   $0x04, X2, X8
	ADDPS     X8, X2
	VUNPCKLPS X2, X1, X8
	VUNPCKHPS X2, X1, X1

	// end DCT8
	// DCT8
	PSHUFD $0x1b, X9, X4
	VADDPS X4, X6, X9
	VSUBPS X4, X6, X6
	DIVPS  dct8<>+0(SB), X6

	// DCT4
	PSHUFD     $0xb4, X9, X9
	PSHUFD     $0xb4, X6, X6
	VPUNPCKLDQ X6, X9, X2
	VPUNPCKHDQ X6, X9, X4
	VADDPS     X4, X2, X9
	VSUBPS     X4, X2, X6
	DIVPS      dct4<>+0(SB), X6
	VPUNPCKLDQ X6, X9, X2
	VPUNPCKHDQ X6, X9, X4

	// DCT2
	PSHUFD    $0xd8, X2, X9
	PSHUFD    $0xd8, X4, X6
	VADDPS    X6, X9, X2
	VSUBPS    X6, X9, X6
	DIVPS     dct2<>+0(SB), X6
	VADDPS    X2, X6, X9
	VBLENDPS  $0x03, X2, X9, X9
	VSHUFPS   $0x88, X6, X9, X2
	VSHUFPS   $0xdd, X6, X9, X4
	VPSRLDQ   $0x04, X4, X6
	ADDPS     X6, X4
	VUNPCKLPS X4, X2, X6
	VUNPCKHPS X4, X2, X9

	// end DCT8
	VPSRLDQ   $0x04, X6, X2
	VPSLLDQ   $0x0c, X9, X4
	VADDPS    X2, X4, X2
	VADDPS    X2, X6, X6
	VPSRLDQ   $0x04, X9, X4
	VADDPS    X4, X9, X9
	VUNPCKLPS X6, X8, X2
	VUNPCKHPS X6, X8, X4
	VUNPCKLPS X9, X1, X6
	VUNPCKHPS X9, X1, X8

	// end DCT16
	// DCT16
	PSHUFD $0x1b, X7, X7
	VADDPS X7, X5, X1
	VSUBPS X7, X5, X9
	DIVPS  dct16<>+16(SB), X9
	PSHUFD $0x1b, X0, X7
	VADDPS X7, X3, X0
	VSUBPS X7, X3, X7
	DIVPS  dct16<>+0(SB), X7

	// DCT8
	PSHUFD $0x1b, X1, X3
	VADDPS X3, X0, X5
	VSUBPS X3, X0, X0
	DIVPS  dct8<>+0(SB), X0

	// DCT4
	PSHUFD     $0xb4, X5, X5
	PSHUFD     $0xb4, X0, X0
	VPUNPCKLDQ X0, X5, X1
	VPUNPCKHDQ X0, X5, X3
	VADDPS     X3, X1, X5
	VSUBPS     X3, X1, X0
	DIVPS      dct4<>+0(SB), X0
	VPUNPCKLDQ X0, X5, X1
	VPUNPCKHDQ X0, X5, X3

	// DCT2
	PSHUFD    $0xd8, X1, X5
	PSHUFD    $0xd8, X3, X0
	VADDPS    X0, X5, X1
	VSUBPS    X0, X5, X0
	DIVPS     dct2<>+0(SB), X0
	VADDPS    X1, X0, X5
	VBLENDPS  $0x03, X1, X5, X5
	VSHUFPS   $0x88, X0, X5, X1
	VSHUFPS   $0xdd, X0, X5, X3
	VPSRLDQ   $0x04, X3, X0
	ADDPS     X0, X3
	VUNPCKLPS X3, X1, X0
	VUNPCKHPS X3, X1, X1

	// end DCT8
	// DCT8
	PSHUFD $0x1b, X9, X5
	VADDPS X5, X7, X9
	VSUBPS X5, X7, X7
	DIVPS  dct8<>+0(SB), X7

	// DCT4
	PSHUFD     $0xb4, X9, X9
	PSHUFD     $0xb4, X7, X7
	VPUNPCKLDQ X7, X9, X3
	VPUNPCKHDQ X7, X9, X5
	VADDPS     X5, X3, X9
	VSUBPS     X5, X3, X7
	DIVPS      dct4<>+0(SB), X7
	VPUNPCKLDQ X7, X9, X3
	VPUNPCKHDQ X7, X9, X5

	// DCT2
	PSHUFD    $0xd8, X3, X9
	PSHUFD    $0xd8, X5, X7
	VADDPS    X7, X9, X3
	VSUBPS    X7, X9, X7
	DIVPS     dct2<>+0(SB), X7
	VADDPS    X3, X7, X9
	VBLENDPS  $0x03, X3, X9, X9
	VSHUFPS   $0x88, X7, X9, X3
	VSHUFPS   $0xdd, X7, X9, X5
	VPSRLDQ   $0x04, X5, X7
	ADDPS     X7, X5
	VUNPCKLPS X5, X3, X7
	VUNPCKHPS X5, X3, X9

	// end DCT8
	VPSRLDQ   $0x04, X7, X3
	VPSLLDQ   $0x0c, X9, X5
	VADDPS    X3, X5, X3
	VADDPS    X3, X7, X7
	VPSRLDQ   $0x04, X9, X5
	VADDPS    X5, X9, X9
	VUNPCKLPS X7, X0, X3
	VUNPCKHPS X7, X0, X5
	VUNPCKLPS X9, X1, X7
	VUNPCKHPS X9, X1, X0

	// end DCT16
	VPSRLDQ   $0x04, X3, X1
	VPSLLDQ   $0x0c, X5, X9
	VADDPS    X1, X9, X1
	VADDPS    X1, X3, X3
	VUNPCKLPS X3, X2, X1
	MOVUPS    X1, (AX)
	VUNPCKHPS X3, X2, X9
	MOVUPS    X9, 16(AX)
	VPSRLDQ   $0x04, X5, X1
	VPSLLDQ   $0x0c, X7, X9
	VADDPS    X1, X9, X1
	VADDPS    X1, X5, X5
	VUNPCKLPS X5, X4, X1
	MOVUPS    X1, 32(AX)
	VUNPCKHPS X5, X4, X9
	MOVUPS    X9, 48(AX)
	VPSRLDQ   $0x04, X7, X1
	VPSLLDQ   $0x0c, X0, X9
	VADDPS    X1, X9, X1
	VADDPS    X1, X7, X7
	VUNPCKLPS X7, X6, X1
	MOVUPS    X1, 64(AX)
	VUNPCKHPS X7, X6, X9
	MOVUPS    X9, 80(AX)
	VPSRLDQ   $0x04, X0, X9
	VADDPS    X9, X0, X0
	VUNPCKLPS X0, X8, X1
	MOVUPS    X1, 96(AX)
	VUNPCKHPS X0, X8, X9
	MOVUPS    X9, 112(AX)

	// end DCT32
	// DCT32
	MOVUPS 128(AX), X0
	MOVUPS 240(AX), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X2
	VSUBPS X1, X0, X3
	DIVPS  dct32<>+0(SB), X3
	MOVUPS 144(AX), X0
	MOVUPS 224(AX), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X4
	VSUBPS X1, X0, X5
	DIVPS  dct32<>+16(SB), X5
	MOVUPS 160(AX), X0
	MOVUPS 208(AX), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X6
	VSUBPS X1, X0, X7
	DIVPS  dct32<>+32(SB), X7
	MOVUPS 176(AX), X0
	MOVUPS 192(AX), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X8
	VSUBPS X1, X0, X0
	DIVPS  dct32<>+48(SB), X0

	// DCT16
	PSHUFD $0x1b, X6, X6
	VADDPS X6, X4, X1
	VSUBPS X6, X4, X9
	DIVPS  dct16<>+16(SB), X9
	PSHUFD $0x1b, X8, X6
	VADDPS X6, X2, X8
	VSUBPS X6, X2, X6
	DIVPS  dct16<>+0(SB), X6

	// DCT8
	PSHUFD $0x1b, X1, X2
	VADDPS X2, X8, X4
	VSUBPS X2, X8, X8
	DIVPS  dct8<>+0(SB), X8

	// DCT4
	PSHUFD     $0xb4, X4, X4
	PSHUFD     $0xb4, X8, X8
	VPUNPCKLDQ X8, X4, X1
	VPUNPCKHDQ X8, X4, X2
	VADDPS     X2, X1, X4
	VSUBPS     X2, X1, X8
	DIVPS      dct4<>+0(SB), X8
	VPUNPCKLDQ X8, X4, X1
	VPUNPCKHDQ X8, X4, X2

	// DCT2
	PSHUFD    $0xd8, X1, X4
	PSHUFD    $0xd8, X2, X8
	VADDPS    X8, X4, X1
	VSUBPS    X8, X4, X8
	DIVPS     dct2<>+0(SB), X8
	VADDPS    X1, X8, X4
	VBLENDPS  $0x03, X1, X4, X4
	VSHUFPS   $0x88, X8, X4, X1
	VSHUFPS   $0xdd, X8, X4, X2
	VPSRLDQ   $0x04, X2, X8
	ADDPS     X8, X2
	VUNPCKLPS X2, X1, X8
	VUNPCKHPS X2, X1, X1

	// end DCT8
	// DCT8
	PSHUFD $0x1b, X9, X4
	VADDPS X4, X6, X9
	VSUBPS X4, X6, X6
	DIVPS  dct8<>+0(SB), X6

	// DCT4
	PSHUFD     $0xb4, X9, X9
	PSHUFD     $0xb4, X6, X6
	VPUNPCKLDQ X6, X9, X2
	VPUNPCKHDQ X6, X9, X4
	VADDPS     X4, X2, X9
	VSUBPS     X4, X2, X6
	DIVPS      dct4<>+0(SB), X6
	VPUNPCKLDQ X6, X9, X2
	VPUNPCKHDQ X6, X9, X4

	// DCT2
	PSHUFD    $0xd8, X2, X9
	PSHUFD    $0xd8, X4, X6
	VADDPS    X6, X9, X2
	VSUBPS    X6, X9, X6
	DIVPS     dct2<>+0(SB), X6
	VADDPS    X2, X6, X9
	VBLENDPS  $0x03, X2, X9, X9
	VSHUFPS   $0x88, X6, X9, X2
	VSHUFPS   $0xdd, X6, X9, X4
	VPSRLDQ   $0x04, X4, X6
	ADDPS     X6, X4
	VUNPCKLPS X4, X2, X6
	VUNPCKHPS X4, X2, X9

	// end DCT8
	VPSRLDQ   $0x04, X6, X2
	VPSLLDQ   $0x0c, X9, X4
	VADDPS    X2, X4, X2
	VADDPS    X2, X6, X6
	VPSRLDQ   $0x04, X9, X4
	VADDPS    X4, X9, X9
	VUNPCKLPS X6, X8, X2
	VUNPCKHPS X6, X8, X4
	VUNPCKLPS X9, X1, X6
	VUNPCKHPS X9, X1, X8

	// end DCT16
	// DCT16
	PSHUFD $0x1b, X7, X7
	VADDPS X7, X5, X1
	VSUBPS X7, X5, X9
	DIVPS  dct16<>+16(SB), X9
	PSHUFD $0x1b, X0, X7
	VADDPS X7, X3, X0
	VSUBPS X7, X3, X7
	DIVPS  dct16<>+0(SB), X7

	// DCT8
	PSHUFD $0x1b, X1, X3
	VADDPS X3, X0, X5
	VSUBPS X3, X0, X0
	DIVPS  dct8<>+0(SB), X0

	// DCT4
	PSHUFD     $0xb4, X5, X5
	PSHUFD     $0xb4, X0, X0
	VPUNPCKLDQ X0, X5, X1
	VPUNPCKHDQ X0, X5, X3
	VADDPS     X3, X1, X5
	VSUBPS     X3, X1, X0
	DIVPS      dct4<>+0(SB), X0
	VPUNPCKLDQ X0, X5, X1
	VPUNPCKHDQ X0, X5, X3

	// DCT2
	PSHUFD    $0xd8, X1, X5
	PSHUFD    $0xd8, X3, X0
	VADDPS    X0, X5, X1
	VSUBPS    X0, X5, X0
	DIVPS     dct2<>+0(SB), X0
	VADDPS    X1, X0, X5
	VBLENDPS  $0x03, X1, X5, X5
	VSHUFPS   $0x88, X0, X5, X1
	VSHUFPS   $0xdd, X0, X5, X3
	VPSRLDQ   $0x04, X3, X0
	ADDPS     X0, X3
	VUNPCKLPS X3, X1, X0
	VUNPCKHPS X3, X1, X1

	// end DCT8
	// DCT8
	PSHUFD $0x1b, X9, X5
	VADDPS X5, X7, X9
	VSUBPS X5, X7, X7
	DIVPS  dct8<>+0(SB), X7

	// DCT4
	PSHUFD     $0xb4, X9, X9
	PSHUFD     $0xb4, X7, X7
	VPUNPCKLDQ X7, X9, X3
	VPUNPCKHDQ X7, X9, X5
	VADDPS     X5, X3, X9
	VSUBPS     X5, X3, X7
	DIVPS      dct4<>+0(SB), X7
	VPUNPCKLDQ X7, X9, X3
	VPUNPCKHDQ X7, X9, X5

	// DCT2
	PSHUFD    $0xd8, X3, X9
	PSHUFD    $0xd8, X5, X7
	VADDPS    X7, X9, X3
	VSUBPS    X7, X9, X7
	DIVPS     dct2<>+0(SB), X7
	VADDPS    X3, X7, X9
	VBLENDPS  $0x03, X3, X9, X9
	VSHUFPS   $0x88, X7, X9, X3
	VSHUFPS   $0xdd, X7, X9, X5
	VPSRLDQ   $0x04, X5, X7
	ADDPS     X7, X5
	VUNPCKLPS X5, X3, X7
	VUNPCKHPS X5, X3, X9

	// end DCT8
	VPSRLDQ   $0x04, X7, X3
	VPSLLDQ   $0x0c, X9, X5
	VADDPS    X3, X5, X3
	VADDPS    X3, X7, X7
	VPSRLDQ   $0x04, X9, X5
	VADDPS    X5, X9, X9
	VUNPCKLPS X7, X0, X3
	VUNPCKHPS X7, X0, X5
	VUNPCKLPS X9, X1, X7
	VUNPCKHPS X9, X1, X0

	// end DCT16
	VPSRLDQ   $0x04, X3, X1
	VPSLLDQ   $0x0c, X5, X9
	VADDPS    X1, X9, X1
	VADDPS    X1, X3, X3
	VUNPCKLPS X3, X2, X1
	MOVUPS    X1, 128(AX)
	VUNPCKHPS X3, X2, X9
	MOVUPS    X9, 144(AX)
	VPSRLDQ   $0x04, X5, X1
	VPSLLDQ   $0x0c, X7, X9
	VADDPS    X1, X9, X1
	VADDPS    X1, X5, X5
	VUNPCKLPS X5, X4, X1
	MOVUPS    X1, 160(AX)
	VUNPCKHPS X5, X4, X9
	MOVUPS    X9, 176(AX)
	VPSRLDQ   $0x04, X7, X1
	VPSLLDQ   $0x0c, X0, X9
	VADDPS    X1, X9, X1
	VADDPS    X1, X7, X7
	VUNPCKLPS X7, X6, X1
	MOVUPS    X1, 192(AX)
	VUNPCKHPS X7, X6, X9
	MOVUPS    X9, 208(AX)
	VPSRLDQ   $0x04, X0, X9
	VADDPS    X9, X0, X0
	VUNPCKLPS X0, X8, X1
	MOVUPS    X1, 224(AX)
	VUNPCKHPS X0, X8, X9
	MOVUPS    X9, 240(AX)

	// end DCT32
	VZEROUPPER
	MOVL       252(AX), CX
	VMOVUPS    (AX), Y0
	VMOVUPS    32(AX), Y1
	VMOVUPS    64(AX), Y2
	VMOVUPS    96(AX), Y3
	VMOVUPS    128(AX), Y8
	VADDPS     132(AX), Y8, Y4
	VUNPCKLPS  Y4, Y0, Y8
	VUNPCKHPS  Y4, Y0, Y4
	VPERM2F128 $0x02, Y8, Y4, Y0
	VMOVUPS    Y0, (AX)
	VPERM2F128 $0x13, Y8, Y4, Y0
	VMOVUPS    Y0, 32(AX)
	VMOVUPS    160(AX), Y8
	VADDPS     164(AX), Y8, Y4
	VUNPCKLPS  Y4, Y1, Y8
	VUNPCKHPS  Y4, Y1, Y4
	VPERM2F128 $0x02, Y8, Y4, Y0
	VMOVUPS    Y0, 64(AX)
	VPERM2F128 $0x13, Y8, Y4, Y0
	VMOVUPS    Y0, 96(AX)
	VMOVUPS    192(AX), Y8
	VADDPS     196(AX), Y8, Y4
	VUNPCKLPS  Y4, Y2, Y8
	VUNPCKHPS  Y4, Y2, Y4
	VPERM2F128 $0x02, Y8, Y4, Y0
	VMOVUPS    Y0, 128(AX)
	VPERM2F128 $0x13, Y8, Y4, Y0
	VMOVUPS    Y0, 160(AX)
	VMOVUPS    224(AX), Y8
	VPMOVZXBD  perm<>+8(SB), Y7
	VPERMPS    224(AX), Y7, Y4
	VADDPS     Y4, Y8, Y4
	VUNPCKLPS  Y4, Y3, Y8
	VUNPCKHPS  Y4, Y3, Y4
	VPERM2F128 $0x02, Y8, Y4, Y0
	VMOVUPS    Y0, 192(AX)
	VPERM2F128 $0x13, Y8, Y4, Y0
	VMOVUPS    Y0, 224(AX)
	MOVL       CX, 252(AX)
	VZEROUPPER

	// end DCT64
	RET

// func asmForwardDCT256(input []float32)
// Requires: AVX, AVX2, SSE, SSE2
TEXT ·asmForwardDCT256(SB), NOPTR, $1024-24
	MOVQ input_base+0(FP), AX

	// DCT256
	VZEROUPPER
	VPMOVZXBD perm<>+0(SB), Y3
	VMOVUPS   (AX), Y0
	VPERMD    992(AX), Y3, Y1
	VADDPS    Y1, Y0, Y2
	VMOVUPS   Y2, (SP)
	VSUBPS    Y1, Y0, Y2
	VDIVPS    dct256<>+0(SB), Y2, Y2
	VMOVUPS   Y2, 512(SP)
	VMOVUPS   32(AX), Y0
	VPERMD    960(AX), Y3, Y1
	VADDPS    Y1, Y0, Y2
	VMOVUPS   Y2, 32(SP)
	VSUBPS    Y1, Y0, Y2
	VDIVPS    dct256<>+32(SB), Y2, Y2
	VMOVUPS   Y2, 544(SP)
	VMOVUPS   64(AX), Y0
	VPERMD    928(AX), Y3, Y1
	VADDPS    Y1, Y0, Y2
	VMOVUPS   Y2, 64(SP)
	VSUBPS    Y1, Y0, Y2
	VDIVPS    dct256<>+64(SB), Y2, Y2
	VMOVUPS   Y2, 576(SP)
	VMOVUPS   96(AX), Y0
	VPERMD    896(AX), Y3, Y1
	VADDPS    Y1, Y0, Y2
	VMOVUPS   Y2, 96(SP)
	VSUBPS    Y1, Y0, Y2
	VDIVPS    dct256<>+96(SB), Y2, Y2
	VMOVUPS   Y2, 608(SP)
	VMOVUPS   128(AX), Y0
	VPERMD    864(AX), Y3, Y1
	VADDPS    Y1, Y0, Y2
	VMOVUPS   Y2, 128(SP)
	VSUBPS    Y1, Y0, Y2
	VDIVPS    dct256<>+128(SB), Y2, Y2
	VMOVUPS   Y2, 640(SP)
	VMOVUPS   160(AX), Y0
	VPERMD    832(AX), Y3, Y1
	VADDPS    Y1, Y0, Y2
	VMOVUPS   Y2, 160(SP)
	VSUBPS    Y1, Y0, Y2
	VDIVPS    dct256<>+160(SB), Y2, Y2
	VMOVUPS   Y2, 672(SP)
	VMOVUPS   192(AX), Y0
	VPERMD    800(AX), Y3, Y1
	VADDPS    Y1, Y0, Y2
	VMOVUPS   Y2, 192(SP)
	VSUBPS    Y1, Y0, Y2
	VDIVPS    dct256<>+192(SB), Y2, Y2
	VMOVUPS   Y2, 704(SP)
	VMOVUPS   224(AX), Y0
	VPERMD    768(AX), Y3, Y1
	VADDPS    Y1, Y0, Y2
	VMOVUPS   Y2, 224(SP)
	VSUBPS    Y1, Y0, Y2
	VDIVPS    dct256<>+224(SB), Y2, Y2
	VMOVUPS   Y2, 736(SP)
	VMOVUPS   256(AX), Y0
	VPERMD    736(AX), Y3, Y1
	VADDPS    Y1, Y0, Y2
	VMOVUPS   Y2, 256(SP)
	VSUBPS    Y1, Y0, Y2
	VDIVPS    dct256<>+256(SB), Y2, Y2
	VMOVUPS   Y2, 768(SP)
	VMOVUPS   288(AX), Y0
	VPERMD    704(AX), Y3, Y1
	VADDPS    Y1, Y0, Y2
	VMOVUPS   Y2, 288(SP)
	VSUBPS    Y1, Y0, Y2
	VDIVPS    dct256<>+288(SB), Y2, Y2
	VMOVUPS   Y2, 800(SP)
	VMOVUPS   320(AX), Y0
	VPERMD    672(AX), Y3, Y1
	VADDPS    Y1, Y0, Y2
	VMOVUPS   Y2, 320(SP)
	VSUBPS    Y1, Y0, Y2
	VDIVPS    dct256<>+320(SB), Y2, Y2
	VMOVUPS   Y2, 832(SP)
	VMOVUPS   352(AX), Y0
	VPERMD    640(AX), Y3, Y1
	VADDPS    Y1, Y0, Y2
	VMOVUPS   Y2, 352(SP)
	VSUBPS    Y1, Y0, Y2
	VDIVPS    dct256<>+352(SB), Y2, Y2
	VMOVUPS   Y2, 864(SP)
	VMOVUPS   384(AX), Y0
	VPERMD    608(AX), Y3, Y1
	VADDPS    Y1, Y0, Y2
	VMOVUPS   Y2, 384(SP)
	VSUBPS    Y1, Y0, Y2
	VDIVPS    dct256<>+384(SB), Y2, Y2
	VMOVUPS   Y2, 896(SP)
	VMOVUPS   416(AX), Y0
	VPERMD    576(AX), Y3, Y1
	VADDPS    Y1, Y0, Y2
	VMOVUPS   Y2, 416(SP)
	VSUBPS    Y1, Y0, Y2
	VDIVPS    dct256<>+416(SB), Y2, Y2
	VMOVUPS   Y2, 928(SP)
	VMOVUPS   448(AX), Y0
	VPERMD    544(AX), Y3, Y1
	VADDPS    Y1, Y0, Y2
	VMOVUPS   Y2, 448(SP)
	VSUBPS    Y1, Y0, Y2
	VDIVPS    dct256<>+448(SB), Y2, Y2
	VMOVUPS   Y2, 960(SP)
	VMOVUPS   480(AX), Y0
	VPERMD    512(AX), Y3, Y1
	VADDPS    Y1, Y0, Y2
	VMOVUPS   Y2, 480(SP)
	VSUBPS    Y1, Y0, Y2
	VDIVPS    dct256<>+480(SB), Y2, Y2
	VMOVUPS   Y2, 992(SP)
	VZEROALL

	// DCT128
	VZEROUPPER
	VPMOVZXBD perm<>+0(SB), Y3
	VMOVUPS   (SP), Y0
	VPERMD    480(SP), Y3, Y1
	VADDPS    Y1, Y0, Y2
	VMOVUPS   Y2, (AX)
	VSUBPS    Y1, Y0, Y2
	VDIVPS    dct128<>+0(SB), Y2, Y2
	VMOVUPS   Y2, 256(AX)
	VMOVUPS   32(SP), Y0
	VPERMD    448(SP), Y3, Y1
	VADDPS    Y1, Y0, Y2
	VMOVUPS   Y2, 32(AX)
	VSUBPS    Y1, Y0, Y2
	VDIVPS    dct128<>+32(SB), Y2, Y2
	VMOVUPS   Y2, 288(AX)
	VMOVUPS   64(SP), Y0
	VPERMD    416(SP), Y3, Y1
	VADDPS    Y1, Y0, Y2
	VMOVUPS   Y2, 64(AX)
	VSUBPS    Y1, Y0, Y2
	VDIVPS    dct128<>+64(SB), Y2, Y2
	VMOVUPS   Y2, 320(AX)
	VMOVUPS   96(SP), Y0
	VPERMD    384(SP), Y3, Y1
	VADDPS    Y1, Y0, Y2
	VMOVUPS   Y2, 96(AX)
	VSUBPS    Y1, Y0, Y2
	VDIVPS    dct128<>+96(SB), Y2, Y2
	VMOVUPS   Y2, 352(AX)
	VMOVUPS   128(SP), Y0
	VPERMD    352(SP), Y3, Y1
	VADDPS    Y1, Y0, Y2
	VMOVUPS   Y2, 128(AX)
	VSUBPS    Y1, Y0, Y2
	VDIVPS    dct128<>+128(SB), Y2, Y2
	VMOVUPS   Y2, 384(AX)
	VMOVUPS   160(SP), Y0
	VPERMD    320(SP), Y3, Y1
	VADDPS    Y1, Y0, Y2
	VMOVUPS   Y2, 160(AX)
	VSUBPS    Y1, Y0, Y2
	VDIVPS    dct128<>+160(SB), Y2, Y2
	VMOVUPS   Y2, 416(AX)
	VMOVUPS   192(SP), Y0
	VPERMD    288(SP), Y3, Y1
	VADDPS    Y1, Y0, Y2
	VMOVUPS   Y2, 192(AX)
	VSUBPS    Y1, Y0, Y2
	VDIVPS    dct128<>+192(SB), Y2, Y2
	VMOVUPS   Y2, 448(AX)
	VMOVUPS   224(SP), Y0
	VPERMD    256(SP), Y3, Y1
	VADDPS    Y1, Y0, Y2
	VMOVUPS   Y2, 224(AX)
	VSUBPS    Y1, Y0, Y2
	VDIVPS    dct128<>+224(SB), Y2, Y2
	VMOVUPS   Y2, 480(AX)
	VZEROALL
	VZEROUPPER

	// DCT64
	VPMOVZXBD perm<>+0(SB), Y7
	VMOVUPS   (AX), Y0
	VMOVUPS   32(AX), Y1
	VMOVUPS   64(AX), Y2
	VMOVUPS   96(AX), Y3
	VPERMD    128(AX), Y7, Y4
	VPERMD    160(AX), Y7, Y5
	VPERMD    192(AX), Y7, Y6
	VPERMD    224(AX), Y7, Y7
	VADDPS    Y7, Y0, Y8
	VMOVUPS   Y8, (AX)
	VSUBPS    Y7, Y0, Y8
	VDIVPS    dct64<>+0(SB), Y8, Y8
	VMOVUPS   Y8, 128(AX)
	VADDPS    Y6, Y1, Y8
	VMOVUPS   Y8, 32(AX)
	VSUBPS    Y6, Y1, Y8
	VDIVPS    dct64<>+32(SB), Y8, Y8
	VMOVUPS   Y8, 160(AX)
	VADDPS    Y5, Y2, Y8
	VMOVUPS   Y8, 64(AX)
	VSUBPS    Y5, Y2, Y8
	VDIVPS    dct64<>+64(SB), Y8, Y8
	VMOVUPS   Y8, 192(AX)
	VADDPS    Y4, Y3, Y8
	VMOVUPS   Y8, 96(AX)
	VSUBPS    Y4, Y3, Y8
	VDIVPS    dct64<>+96(SB), Y8, Y8
	VMOVUPS   Y8, 224(AX)
	VZEROALL

	// DCT32
	MOVUPS (AX), X0
	MOVUPS 112(AX), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X2
	VSUBPS X1, X0, X3
	DIVPS  dct32<>+0(SB), X3
	MOVUPS 16(AX), X0
	MOVUPS 96(AX), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X4
	VSUBPS X1, X0, X5
	DIVPS  dct32<>+16(SB), X5
	MOVUPS 32(AX), X0
	MOVUPS 80(AX), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X6
	VSUBPS X1, X0, X7
	DIVPS  dct32<>+32(SB), X7
	MOVUPS 48(AX), X0
	MOVUPS 64(AX), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X8
	VSUBPS X1, X0, X0
	DIVPS  dct32<>+48(SB), X0

	// DCT16
	PSHUFD $0x1b, X6, X6
	VADDPS X6, X4, X1
	VSUBPS X6, X4, X9
	DIVPS  dct16<>+16(SB), X9
	PSHUFD $0x1b, X8, X6
	VADDPS X6, X2, X8
	VSUBPS X6, X2, X6
	DIVPS  dct16<>+0(SB), X6

	// DCT8
	PSHUFD $0x1b, X1, X2
	VADDPS X2, X8, X4
	VSUBPS X2, X8, X8
	DIVPS  dct8<>+0(SB), X8

	// DCT4
	PSHUFD     $0xb4, X4, X4
	PSHUFD     $0xb4, X8, X8
	VPUNPCKLDQ X8, X4, X1
	VPUNPCKHDQ X8, X4, X2
	VADDPS     X2, X1, X4
	VSUBPS     X2, X1, X8
	DIVPS      dct4<>+0(SB), X8
	VPUNPCKLDQ X8, X4, X1
	VPUNPCKHDQ X8, X4, X2

	// DCT2
	PSHUFD    $0xd8, X1, X4
	PSHUFD    $0xd8, X2, X8
	VADDPS    X8, X4, X1
	VSUBPS    X8, X4, X8
	DIVPS     dct2<>+0(SB), X8
	VADDPS    X1, X8, X4
	VBLENDPS  $0x03, X1, X4, X4
	VSHUFPS   $0x88, X8, X4, X1
	VSHUFPS   $0xdd, X8, X4, X2
	VPSRLDQ   $0x04, X2, X8
	ADDPS     X8, X2
	VUNPCKLPS X2, X1, X8
	VUNPCKHPS X2, X1, X1

	// end DCT8
	// DCT8
	PSHUFD $0x1b, X9, X4
	VADDPS X4, X6, X9
	VSUBPS X4, X6, X6
	DIVPS  dct8<>+0(SB), X6

	// DCT4
	PSHUFD     $0xb4, X9, X9
	PSHUFD     $0xb4, X6, X6
	VPUNPCKLDQ X6, X9, X2
	VPUNPCKHDQ X6, X9, X4
	VADDPS     X4, X2, X9
	VSUBPS     X4, X2, X6
	DIVPS      dct4<>+0(SB), X6
	VPUNPCKLDQ X6, X9, X2
	VPUNPCKHDQ X6, X9, X4

	// DCT2
	PSHUFD    $0xd8, X2, X9
	PSHUFD    $0xd8, X4, X6
	VADDPS    X6, X9, X2
	VSUBPS    X6, X9, X6
	DIVPS     dct2<>+0(SB), X6
	VADDPS    X2, X6, X9
	VBLENDPS  $0x03, X2, X9, X9
	VSHUFPS   $0x88, X6, X9, X2
	VSHUFPS   $0xdd, X6, X9, X4
	VPSRLDQ   $0x04, X4, X6
	ADDPS     X6, X4
	VUNPCKLPS X4, X2, X6
	VUNPCKHPS X4, X2, X9

	// end DCT8
	VPSRLDQ   $0x04, X6, X2
	VPSLLDQ   $0x0c, X9, X4
	VADDPS    X2, X4, X2
	VADDPS    X2, X6, X6
	VPSRLDQ   $0x04, X9, X4
	VADDPS    X4, X9, X9
	VUNPCKLPS X6, X8, X2
	VUNPCKHPS X6, X8, X4
	VUNPCKLPS X9, X1, X6
	VUNPCKHPS X9, X1, X8

	// end DCT16
	// DCT16
	PSHUFD $0x1b, X7, X7
	VADDPS X7, X5, X1
	VSUBPS X7, X5, X9
	DIVPS  dct16<>+16(SB), X9
	PSHUFD $0x1b, X0, X7
	VADDPS X7, X3, X0
	VSUBPS X7, X3, X7
	DIVPS  dct16<>+0(SB), X7

	// DCT8
	PSHUFD $0x1b, X1, X3
	VADDPS X3, X0, X5
	VSUBPS X3, X0, X0
	DIVPS  dct8<>+0(SB), X0

	// DCT4
	PSHUFD     $0xb4, X5, X5
	PSHUFD     $0xb4, X0, X0
	VPUNPCKLDQ X0, X5, X1
	VPUNPCKHDQ X0, X5, X3
	VADDPS     X3, X1, X5
	VSUBPS     X3, X1, X0
	DIVPS      dct4<>+0(SB), X0
	VPUNPCKLDQ X0, X5, X1
	VPUNPCKHDQ X0, X5, X3

	// DCT2
	PSHUFD    $0xd8, X1, X5
	PSHUFD    $0xd8, X3, X0
	VADDPS    X0, X5, X1
	VSUBPS    X0, X5, X0
	DIVPS     dct2<>+0(SB), X0
	VADDPS    X1, X0, X5
	VBLENDPS  $0x03, X1, X5, X5
	VSHUFPS   $0x88, X0, X5, X1
	VSHUFPS   $0xdd, X0, X5, X3
	VPSRLDQ   $0x04, X3, X0
	ADDPS     X0, X3
	VUNPCKLPS X3, X1, X0
	VUNPCKHPS X3, X1, X1

	// end DCT8
	// DCT8
	PSHUFD $0x1b, X9, X5
	VADDPS X5, X7, X9
	VSUBPS X5, X7, X7
	DIVPS  dct8<>+0(SB), X7

	// DCT4
	PSHUFD     $0xb4, X9, X9
	PSHUFD     $0xb4, X7, X7
	VPUNPCKLDQ X7, X9, X3
	VPUNPCKHDQ X7, X9, X5
	VADDPS     X5, X3, X9
	VSUBPS     X5, X3, X7
	DIVPS      dct4<>+0(SB), X7
	VPUNPCKLDQ X7, X9, X3
	VPUNPCKHDQ X7, X9, X5

	// DCT2
	PSHUFD    $0xd8, X3, X9
	PSHUFD    $0xd8, X5, X7
	VADDPS    X7, X9, X3
	VSUBPS    X7, X9, X7
	DIVPS     dct2<>+0(SB), X7
	VADDPS    X3, X7, X9
	VBLENDPS  $0x03, X3, X9, X9
	VSHUFPS   $0x88, X7, X9, X3
	VSHUFPS   $0xdd, X7, X9, X5
	VPSRLDQ   $0x04, X5, X7
	ADDPS     X7, X5
	VUNPCKLPS X5, X3, X7
	VUNPCKHPS X5, X3, X9

	// end DCT8
	VPSRLDQ   $0x04, X7, X3
	VPSLLDQ   $0x0c, X9, X5
	VADDPS    X3, X5, X3
	VADDPS    X3, X7, X7
	VPSRLDQ   $0x04, X9, X5
	VADDPS    X5, X9, X9
	VUNPCKLPS X7, X0, X3
	VUNPCKHPS X7, X0, X5
	VUNPCKLPS X9, X1, X7
	VUNPCKHPS X9, X1, X0

	// end DCT16
	VPSRLDQ   $0x04, X3, X1
	VPSLLDQ   $0x0c, X5, X9
	VADDPS    X1, X9, X1
	VADDPS    X1, X3, X3
	VUNPCKLPS X3, X2, X1
	MOVUPS    X1, (AX)
	VUNPCKHPS X3, X2, X9
	MOVUPS    X9, 16(AX)
	VPSRLDQ   $0x04, X5, X1
	VPSLLDQ   $0x0c, X7, X9
	VADDPS    X1, X9, X1
	VADDPS    X1, X5, X5
	VUNPCKLPS X5, X4, X1
	MOVUPS    X1, 32(AX)
	VUNPCKHPS X5, X4, X9
	MOVUPS    X9, 48(AX)
	VPSRLDQ   $0x04, X7, X1
	VPSLLDQ   $0x0c, X0, X9
	VADDPS    X1, X9, X1
	VADDPS    X1, X7, X7
	VUNPCKLPS X7, X6, X1
	MOVUPS    X1, 64(AX)
	VUNPCKHPS X7, X6, X9
	MOVUPS    X9, 80(AX)
	VPSRLDQ   $0x04, X0, X9
	VADDPS    X9, X0, X0
	VUNPCKLPS X0, X8, X1
	MOVUPS    X1, 96(AX)
	VUNPCKHPS X0, X8, X9
	MOVUPS    X9, 112(AX)

	// end DCT32
	// DCT32
	MOVUPS 128(AX), X0
	MOVUPS 240(AX), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X2
	VSUBPS X1, X0, X3
	DIVPS  dct32<>+0(SB), X3
	MOVUPS 144(AX), X0
	MOVUPS 224(AX), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X4
	VSUBPS X1, X0, X5
	DIVPS  dct32<>+16(SB), X5
	MOVUPS 160(AX), X0
	MOVUPS 208(AX), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X6
	VSUBPS X1, X0, X7
	DIVPS  dct32<>+32(SB), X7
	MOVUPS 176(AX), X0
	MOVUPS 192(AX), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X8
	VSUBPS X1, X0, X0
	DIVPS  dct32<>+48(SB), X0

	// DCT16
	PSHUFD $0x1b, X6, X6
	VADDPS X6, X4, X1
	VSUBPS X6, X4, X9
	DIVPS  dct16<>+16(SB), X9
	PSHUFD $0x1b, X8, X6
	VADDPS X6, X2, X8
	VSUBPS X6, X2, X6
	DIVPS  dct16<>+0(SB), X6

	// DCT8
	PSHUFD $0x1b, X1, X2
	VADDPS X2, X8, X4
	VSUBPS X2, X8, X8
	DIVPS  dct8<>+0(SB), X8

	// DCT4
	PSHUFD     $0xb4, X4, X4
	PSHUFD     $0xb4, X8, X8
	VPUNPCKLDQ X8, X4, X1
	VPUNPCKHDQ X8, X4, X2
	VADDPS     X2, X1, X4
	VSUBPS     X2, X1, X8
	DIVPS      dct4<>+0(SB), X8
	VPUNPCKLDQ X8, X4, X1
	VPUNPCKHDQ X8, X4, X2

	// DCT2
	PSHUFD    $0xd8, X1, X4
	PSHUFD    $0xd8, X2, X8
	VADDPS    X8, X4, X1
	VSUBPS    X8, X4, X8
	DIVPS     dct2<>+0(SB), X8
	VADDPS    X1, X8, X4
	VBLENDPS  $0x03, X1, X4, X4
	VSHUFPS   $0x88, X8, X4, X1
	VSHUFPS   $0xdd, X8, X4, X2
	VPSRLDQ   $0x04, X2, X8
	ADDPS     X8, X2
	VUNPCKLPS X2, X1, X8
	VUNPCKHPS X2, X1, X1

	// end DCT8
	// DCT8
	PSHUFD $0x1b, X9, X4
	VADDPS X4, X6, X9
	VSUBPS X4, X6, X6
	DIVPS  dct8<>+0(SB), X6

	// DCT4
	PSHUFD     $0xb4, X9, X9
	PSHUFD     $0xb4, X6, X6
	VPUNPCKLDQ X6, X9, X2
	VPUNPCKHDQ X6, X9, X4
	VADDPS     X4, X2, X9
	VSUBPS     X4, X2, X6
	DIVPS      dct4<>+0(SB), X6
	VPUNPCKLDQ X6, X9, X2
	VPUNPCKHDQ X6, X9, X4

	// DCT2
	PSHUFD    $0xd8, X2, X9
	PSHUFD    $0xd8, X4, X6
	VADDPS    X6, X9, X2
	VSUBPS    X6, X9, X6
	DIVPS     dct2<>+0(SB), X6
	VADDPS    X2, X6, X9
	VBLENDPS  $0x03, X2, X9, X9
	VSHUFPS   $0x88, X6, X9, X2
	VSHUFPS   $0xdd, X6, X9, X4
	VPSRLDQ   $0x04, X4, X6
	ADDPS     X6, X4
	VUNPCKLPS X4, X2, X6
	VUNPCKHPS X4, X2, X9

	// end DCT8
	VPSRLDQ   $0x04, X6, X2
	VPSLLDQ   $0x0c, X9, X4
	VADDPS    X2, X4, X2
	VADDPS    X2, X6, X6
	VPSRLDQ   $0x04, X9, X4
	VADDPS    X4, X9, X9
	VUNPCKLPS X6, X8, X2
	VUNPCKHPS X6, X8, X4
	VUNPCKLPS X9, X1, X6
	VUNPCKHPS X9, X1, X8

	// end DCT16
	// DCT16
	PSHUFD $0x1b, X7, X7
	VADDPS X7, X5, X1
	VSUBPS X7, X5, X9
	DIVPS  dct16<>+16(SB), X9
	PSHUFD $0x1b, X0, X7
	VADDPS X7, X3, X0
	VSUBPS X7, X3, X7
	DIVPS  dct16<>+0(SB), X7

	// DCT8
	PSHUFD $0x1b, X1, X3
	VADDPS X3, X0, X5
	VSUBPS X3, X0, X0
	DIVPS  dct8<>+0(SB), X0

	// DCT4
	PSHUFD     $0xb4, X5, X5
	PSHUFD     $0xb4, X0, X0
	VPUNPCKLDQ X0, X5, X1
	VPUNPCKHDQ X0, X5, X3
	VADDPS     X3, X1, X5
	VSUBPS     X3, X1, X0
	DIVPS      dct4<>+0(SB), X0
	VPUNPCKLDQ X0, X5, X1
	VPUNPCKHDQ X0, X5, X3

	// DCT2
	PSHUFD    $0xd8, X1, X5
	PSHUFD    $0xd8, X3, X0
	VADDPS    X0, X5, X1
	VSUBPS    X0, X5, X0
	DIVPS     dct2<>+0(SB), X0
	VADDPS    X1, X0, X5
	VBLENDPS  $0x03, X1, X5, X5
	VSHUFPS   $0x88, X0, X5, X1
	VSHUFPS   $0xdd, X0, X5, X3
	VPSRLDQ   $0x04, X3, X0
	ADDPS     X0, X3
	VUNPCKLPS X3, X1, X0
	VUNPCKHPS X3, X1, X1

	// end DCT8
	// DCT8
	PSHUFD $0x1b, X9, X5
	VADDPS X5, X7, X9
	VSUBPS X5, X7, X7
	DIVPS  dct8<>+0(SB), X7

	// DCT4
	PSHUFD     $0xb4, X9, X9
	PSHUFD     $0xb4, X7, X7
	VPUNPCKLDQ X7, X9, X3
	VPUNPCKHDQ X7, X9, X5
	VADDPS     X5, X3, X9
	VSUBPS     X5, X3, X7
	DIVPS      dct4<>+0(SB), X7
	VPUNPCKLDQ X7, X9, X3
	VPUNPCKHDQ X7, X9, X5

	// DCT2
	PSHUFD    $0xd8, X3, X9
	PSHUFD    $0xd8, X5, X7
	VADDPS    X7, X9, X3
	VSUBPS    X7, X9, X7
	DIVPS     dct2<>+0(SB), X7
	VADDPS    X3, X7, X9
	VBLENDPS  $0x03, X3, X9, X9
	VSHUFPS   $0x88, X7, X9, X3
	VSHUFPS   $0xdd, X7, X9, X5
	VPSRLDQ   $0x04, X5, X7
	ADDPS     X7, X5
	VUNPCKLPS X5, X3, X7
	VUNPCKHPS X5, X3, X9

	// end DCT8
	VPSRLDQ   $0x04, X7, X3
	VPSLLDQ   $0x0c, X9, X5
	VADDPS    X3, X5, X3
	VADDPS    X3, X7, X7
	VPSRLDQ   $0x04, X9, X5
	VADDPS    X5, X9, X9
	VUNPCKLPS X7, X0, X3
	VUNPCKHPS X7, X0, X5
	VUNPCKLPS X9, X1, X7
	VUNPCKHPS X9, X1, X0

	// end DCT16
	VPSRLDQ   $0x04, X3, X1
	VPSLLDQ   $0x0c, X5, X9
	VADDPS    X1, X9, X1
	VADDPS    X1, X3, X3
	VUNPCKLPS X3, X2, X1
	MOVUPS    X1, 128(AX)
	VUNPCKHPS X3, X2, X9
	MOVUPS    X9, 144(AX)
	VPSRLDQ   $0x04, X5, X1
	VPSLLDQ   $0x0c, X7, X9
	VADDPS    X1, X9, X1
	VADDPS    X1, X5, X5
	VUNPCKLPS X5, X4, X1
	MOVUPS    X1, 160(AX)
	VUNPCKHPS X5, X4, X9
	MOVUPS    X9, 176(AX)
	VPSRLDQ   $0x04, X7, X1
	VPSLLDQ   $0x0c, X0, X9
	VADDPS    X1, X9, X1
	VADDPS    X1, X7, X7
	VUNPCKLPS X7, X6, X1
	MOVUPS    X1, 192(AX)
	VUNPCKHPS X7, X6, X9
	MOVUPS    X9, 208(AX)
	VPSRLDQ   $0x04, X0, X9
	VADDPS    X9, X0, X0
	VUNPCKLPS X0, X8, X1
	MOVUPS    X1, 224(AX)
	VUNPCKHPS X0, X8, X9
	MOVUPS    X9, 240(AX)

	// end DCT32
	VZEROUPPER
	MOVL       252(AX), CX
	VMOVUPS    (AX), Y0
	VMOVUPS    32(AX), Y1
	VMOVUPS    64(AX), Y2
	VMOVUPS    96(AX), Y3
	VMOVUPS    128(AX), Y8
	VADDPS     132(AX), Y8, Y4
	VUNPCKLPS  Y4, Y0, Y8
	VUNPCKHPS  Y4, Y0, Y4
	VPERM2F128 $0x02, Y8, Y4, Y0
	VMOVUPS    Y0, (AX)
	VPERM2F128 $0x13, Y8, Y4, Y0
	VMOVUPS    Y0, 32(AX)
	VMOVUPS    160(AX), Y8
	VADDPS     164(AX), Y8, Y4
	VUNPCKLPS  Y4, Y1, Y8
	VUNPCKHPS  Y4, Y1, Y4
	VPERM2F128 $0x02, Y8, Y4, Y0
	VMOVUPS    Y0, 64(AX)
	VPERM2F128 $0x13, Y8, Y4, Y0
	VMOVUPS    Y0, 96(AX)
	VMOVUPS    192(AX), Y8
	VADDPS     196(AX), Y8, Y4
	VUNPCKLPS  Y4, Y2, Y8
	VUNPCKHPS  Y4, Y2, Y4
	VPERM2F128 $0x02, Y8, Y4, Y0
	VMOVUPS    Y0, 128(AX)
	VPERM2F128 $0x13, Y8, Y4, Y0
	VMOVUPS    Y0, 160(AX)
	VMOVUPS    224(AX), Y8
	VPMOVZXBD  perm<>+8(SB), Y7
	VPERMPS    224(AX), Y7, Y4
	VADDPS     Y4, Y8, Y4
	VUNPCKLPS  Y4, Y3, Y8
	VUNPCKHPS  Y4, Y3, Y4
	VPERM2F128 $0x02, Y8, Y4, Y0
	VMOVUPS    Y0, 192(AX)
	VPERM2F128 $0x13, Y8, Y4, Y0
	VMOVUPS    Y0, 224(AX)
	MOVL       CX, 252(AX)
	VZEROUPPER

	// end DCT64
	VZEROUPPER

	// DCT64
	VPMOVZXBD perm<>+0(SB), Y7
	VMOVUPS   256(AX), Y0
	VMOVUPS   288(AX), Y1
	VMOVUPS   320(AX), Y2
	VMOVUPS   352(AX), Y3
	VPERMD    384(AX), Y7, Y4
	VPERMD    416(AX), Y7, Y5
	VPERMD    448(AX), Y7, Y6
	VPERMD    480(AX), Y7, Y7
	VADDPS    Y7, Y0, Y8
	VMOVUPS   Y8, 256(AX)
	VSUBPS    Y7, Y0, Y8
	VDIVPS    dct64<>+0(SB), Y8, Y8
	VMOVUPS   Y8, 384(AX)
	VADDPS    Y6, Y1, Y8
	VMOVUPS   Y8, 288(AX)
	VSUBPS    Y6, Y1, Y8
	VDIVPS    dct64<>+32(SB), Y8, Y8
	VMOVUPS   Y8, 416(AX)
	VADDPS    Y5, Y2, Y8
	VMOVUPS   Y8, 320(AX)
	VSUBPS    Y5, Y2, Y8
	VDIVPS    dct64<>+64(SB), Y8, Y8
	VMOVUPS   Y8, 448(AX)
	VADDPS    Y4, Y3, Y8
	VMOVUPS   Y8, 352(AX)
	VSUBPS    Y4, Y3, Y8
	VDIVPS    dct64<>+96(SB), Y8, Y8
	VMOVUPS   Y8, 480(AX)
	VZEROALL

	// DCT32
	MOVUPS 256(AX), X0
	MOVUPS 368(AX), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X2
	VSUBPS X1, X0, X3
	DIVPS  dct32<>+0(SB), X3
	MOVUPS 272(AX), X0
	MOVUPS 352(AX), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X4
	VSUBPS X1, X0, X5
	DIVPS  dct32<>+16(SB), X5
	MOVUPS 288(AX), X0
	MOVUPS 336(AX), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X6
	VSUBPS X1, X0, X7
	DIVPS  dct32<>+32(SB), X7
	MOVUPS 304(AX), X0
	MOVUPS 320(AX), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X8
	VSUBPS X1, X0, X0
	DIVPS  dct32<>+48(SB), X0

	// DCT16
	PSHUFD $0x1b, X6, X6
	VADDPS X6, X4, X1
	VSUBPS X6, X4, X9
	DIVPS  dct16<>+16(SB), X9
	PSHUFD $0x1b, X8, X6
	VADDPS X6, X2, X8
	VSUBPS X6, X2, X6
	DIVPS  dct16<>+0(SB), X6

	// DCT8
	PSHUFD $0x1b, X1, X2
	VADDPS X2, X8, X4
	VSUBPS X2, X8, X8
	DIVPS  dct8<>+0(SB), X8

	// DCT4
	PSHUFD     $0xb4, X4, X4
	PSHUFD     $0xb4, X8, X8
	VPUNPCKLDQ X8, X4, X1
	VPUNPCKHDQ X8, X4, X2
	VADDPS     X2, X1, X4
	VSUBPS     X2, X1, X8
	DIVPS      dct4<>+0(SB), X8
	VPUNPCKLDQ X8, X4, X1
	VPUNPCKHDQ X8, X4, X2

	// DCT2
	PSHUFD    $0xd8, X1, X4
	PSHUFD    $0xd8, X2, X8
	VADDPS    X8, X4, X1
	VSUBPS    X8, X4, X8
	DIVPS     dct2<>+0(SB), X8
	VADDPS    X1, X8, X4
	VBLENDPS  $0x03, X1, X4, X4
	VSHUFPS   $0x88, X8, X4, X1
	VSHUFPS   $0xdd, X8, X4, X2
	VPSRLDQ   $0x04, X2, X8
	ADDPS     X8, X2
	VUNPCKLPS X2, X1, X8
	VUNPCKHPS X2, X1, X1

	// end DCT8
	// DCT8
	PSHUFD $0x1b, X9, X4
	VADDPS X4, X6, X9
	VSUBPS X4, X6, X6
	DIVPS  dct8<>+0(SB), X6

	// DCT4
	PSHUFD     $0xb4, X9, X9
	PSHUFD     $0xb4, X6, X6
	VPUNPCKLDQ X6, X9, X2
	VPUNPCKHDQ X6, X9, X4
	VADDPS     X4, X2, X9
	VSUBPS     X4, X2, X6
	DIVPS      dct4<>+0(SB), X6
	VPUNPCKLDQ X6, X9, X2
	VPUNPCKHDQ X6, X9, X4

	// DCT2
	PSHUFD    $0xd8, X2, X9
	PSHUFD    $0xd8, X4, X6
	VADDPS    X6, X9, X2
	VSUBPS    X6, X9, X6
	DIVPS     dct2<>+0(SB), X6
	VADDPS    X2, X6, X9
	VBLENDPS  $0x03, X2, X9, X9
	VSHUFPS   $0x88, X6, X9, X2
	VSHUFPS   $0xdd, X6, X9, X4
	VPSRLDQ   $0x04, X4, X6
	ADDPS     X6, X4
	VUNPCKLPS X4, X2, X6
	VUNPCKHPS X4, X2, X9

	// end DCT8
	VPSRLDQ   $0x04, X6, X2
	VPSLLDQ   $0x0c, X9, X4
	VADDPS    X2, X4, X2
	VADDPS    X2, X6, X6
	VPSRLDQ   $0x04, X9, X4
	VADDPS    X4, X9, X9
	VUNPCKLPS X6, X8, X2
	VUNPCKHPS X6, X8, X4
	VUNPCKLPS X9, X1, X6
	VUNPCKHPS X9, X1, X8

	// end DCT16
	// DCT16
	PSHUFD $0x1b, X7, X7
	VADDPS X7, X5, X1
	VSUBPS X7, X5, X9
	DIVPS  dct16<>+16(SB), X9
	PSHUFD $0x1b, X0, X7
	VADDPS X7, X3, X0
	VSUBPS X7, X3, X7
	DIVPS  dct16<>+0(SB), X7

	// DCT8
	PSHUFD $0x1b, X1, X3
	VADDPS X3, X0, X5
	VSUBPS X3, X0, X0
	DIVPS  dct8<>+0(SB), X0

	// DCT4
	PSHUFD     $0xb4, X5, X5
	PSHUFD     $0xb4, X0, X0
	VPUNPCKLDQ X0, X5, X1
	VPUNPCKHDQ X0, X5, X3
	VADDPS     X3, X1, X5
	VSUBPS     X3, X1, X0
	DIVPS      dct4<>+0(SB), X0
	VPUNPCKLDQ X0, X5, X1
	VPUNPCKHDQ X0, X5, X3

	// DCT2
	PSHUFD    $0xd8, X1, X5
	PSHUFD    $0xd8, X3, X0
	VADDPS    X0, X5, X1
	VSUBPS    X0, X5, X0
	DIVPS     dct2<>+0(SB), X0
	VADDPS    X1, X0, X5
	VBLENDPS  $0x03, X1, X5, X5
	VSHUFPS   $0x88, X0, X5, X1
	VSHUFPS   $0xdd, X0, X5, X3
	VPSRLDQ   $0x04, X3, X0
	ADDPS     X0, X3
	VUNPCKLPS X3, X1, X0
	VUNPCKHPS X3, X1, X1

	// end DCT8
	// DCT8
	PSHUFD $0x1b, X9, X5
	VADDPS X5, X7, X9
	VSUBPS X5, X7, X7
	DIVPS  dct8<>+0(SB), X7

	// DCT4
	PSHUFD     $0xb4, X9, X9
	PSHUFD     $0xb4, X7, X7
	VPUNPCKLDQ X7, X9, X3
	VPUNPCKHDQ X7, X9, X5
	VADDPS     X5, X3, X9
	VSUBPS     X5, X3, X7
	DIVPS      dct4<>+0(SB), X7
	VPUNPCKLDQ X7, X9, X3
	VPUNPCKHDQ X7, X9, X5

	// DCT2
	PSHUFD    $0xd8, X3, X9
	PSHUFD    $0xd8, X5, X7
	VADDPS    X7, X9, X3
	VSUBPS    X7, X9, X7
	DIVPS     dct2<>+0(SB), X7
	VADDPS    X3, X7, X9
	VBLENDPS  $0x03, X3, X9, X9
	VSHUFPS   $0x88, X7, X9, X3
	VSHUFPS   $0xdd, X7, X9, X5
	VPSRLDQ   $0x04, X5, X7
	ADDPS     X7, X5
	VUNPCKLPS X5, X3, X7
	VUNPCKHPS X5, X3, X9

	// end DCT8
	VPSRLDQ   $0x04, X7, X3
	VPSLLDQ   $0x0c, X9, X5
	VADDPS    X3, X5, X3
	VADDPS    X3, X7, X7
	VPSRLDQ   $0x04, X9, X5
	VADDPS    X5, X9, X9
	VUNPCKLPS X7, X0, X3
	VUNPCKHPS X7, X0, X5
	VUNPCKLPS X9, X1, X7
	VUNPCKHPS X9, X1, X0

	// end DCT16
	VPSRLDQ   $0x04, X3, X1
	VPSLLDQ   $0x0c, X5, X9
	VADDPS    X1, X9, X1
	VADDPS    X1, X3, X3
	VUNPCKLPS X3, X2, X1
	MOVUPS    X1, 256(AX)
	VUNPCKHPS X3, X2, X9
	MOVUPS    X9, 272(AX)
	VPSRLDQ   $0x04, X5, X1
	VPSLLDQ   $0x0c, X7, X9
	VADDPS    X1, X9, X1
	VADDPS    X1, X5, X5
	VUNPCKLPS X5, X4, X1
	MOVUPS    X1, 288(AX)
	VUNPCKHPS X5, X4, X9
	MOVUPS    X9, 304(AX)
	VPSRLDQ   $0x04, X7, X1
	VPSLLDQ   $0x0c, X0, X9
	VADDPS    X1, X9, X1
	VADDPS    X1, X7, X7
	VUNPCKLPS X7, X6, X1
	MOVUPS    X1, 320(AX)
	VUNPCKHPS X7, X6, X9
	MOVUPS    X9, 336(AX)
	VPSRLDQ   $0x04, X0, X9
	VADDPS    X9, X0, X0
	VUNPCKLPS X0, X8, X1
	MOVUPS    X1, 352(AX)
	VUNPCKHPS X0, X8, X9
	MOVUPS    X9, 368(AX)

	// end DCT32
	// DCT32
	MOVUPS 384(AX), X0
	MOVUPS 496(AX), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X2
	VSUBPS X1, X0, X3
	DIVPS  dct32<>+0(SB), X3
	MOVUPS 400(AX), X0
	MOVUPS 480(AX), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X4
	VSUBPS X1, X0, X5
	DIVPS  dct32<>+16(SB), X5
	MOVUPS 416(AX), X0
	MOVUPS 464(AX), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X6
	VSUBPS X1, X0, X7
	DIVPS  dct32<>+32(SB), X7
	MOVUPS 432(AX), X0
	MOVUPS 448(AX), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X8
	VSUBPS X1, X0, X0
	DIVPS  dct32<>+48(SB), X0

	// DCT16
	PSHUFD $0x1b, X6, X6
	VADDPS X6, X4, X1
	VSUBPS X6, X4, X9
	DIVPS  dct16<>+16(SB), X9
	PSHUFD $0x1b, X8, X6
	VADDPS X6, X2, X8
	VSUBPS X6, X2, X6
	DIVPS  dct16<>+0(SB), X6

	// DCT8
	PSHUFD $0x1b, X1, X2
	VADDPS X2, X8, X4
	VSUBPS X2, X8, X8
	DIVPS  dct8<>+0(SB), X8

	// DCT4
	PSHUFD     $0xb4, X4, X4
	PSHUFD     $0xb4, X8, X8
	VPUNPCKLDQ X8, X4, X1
	VPUNPCKHDQ X8, X4, X2
	VADDPS     X2, X1, X4
	VSUBPS     X2, X1, X8
	DIVPS      dct4<>+0(SB), X8
	VPUNPCKLDQ X8, X4, X1
	VPUNPCKHDQ X8, X4, X2

	// DCT2
	PSHUFD    $0xd8, X1, X4
	PSHUFD    $0xd8, X2, X8
	VADDPS    X8, X4, X1
	VSUBPS    X8, X4, X8
	DIVPS     dct2<>+0(SB), X8
	VADDPS    X1, X8, X4
	VBLENDPS  $0x03, X1, X4, X4
	VSHUFPS   $0x88, X8, X4, X1
	VSHUFPS   $0xdd, X8, X4, X2
	VPSRLDQ   $0x04, X2, X8
	ADDPS     X8, X2
	VUNPCKLPS X2, X1, X8
	VUNPCKHPS X2, X1, X1

	// end DCT8
	// DCT8
	PSHUFD $0x1b, X9, X4
	VADDPS X4, X6, X9
	VSUBPS X4, X6, X6
	DIVPS  dct8<>+0(SB), X6

	// DCT4
	PSHUFD     $0xb4, X9, X9
	PSHUFD     $0xb4, X6, X6
	VPUNPCKLDQ X6, X9, X2
	VPUNPCKHDQ X6, X9, X4
	VADDPS     X4, X2, X9
	VSUBPS     X4, X2, X6
	DIVPS      dct4<>+0(SB), X6
	VPUNPCKLDQ X6, X9, X2
	VPUNPCKHDQ X6, X9, X4

	// DCT2
	PSHUFD    $0xd8, X2, X9
	PSHUFD    $0xd8, X4, X6
	VADDPS    X6, X9, X2
	VSUBPS    X6, X9, X6
	DIVPS     dct2<>+0(SB), X6
	VADDPS    X2, X6, X9
	VBLENDPS  $0x03, X2, X9, X9
	VSHUFPS   $0x88, X6, X9, X2
	VSHUFPS   $0xdd, X6, X9, X4
	VPSRLDQ   $0x04, X4, X6
	ADDPS     X6, X4
	VUNPCKLPS X4, X2, X6
	VUNPCKHPS X4, X2, X9

	// end DCT8
	VPSRLDQ   $0x04, X6, X2
	VPSLLDQ   $0x0c, X9, X4
	VADDPS    X2, X4, X2
	VADDPS    X2, X6, X6
	VPSRLDQ   $0x04, X9, X4
	VADDPS    X4, X9, X9
	VUNPCKLPS X6, X8, X2
	VUNPCKHPS X6, X8, X4
	VUNPCKLPS X9, X1, X6
	VUNPCKHPS X9, X1, X8

	// end DCT16
	// DCT16
	PSHUFD $0x1b, X7, X7
	VADDPS X7, X5, X1
	VSUBPS X7, X5, X9
	DIVPS  dct16<>+16(SB), X9
	PSHUFD $0x1b, X0, X7
	VADDPS X7, X3, X0
	VSUBPS X7, X3, X7
	DIVPS  dct16<>+0(SB), X7

	// DCT8
	PSHUFD $0x1b, X1, X3
	VADDPS X3, X0, X5
	VSUBPS X3, X0, X0
	DIVPS  dct8<>+0(SB), X0

	// DCT4
	PSHUFD     $0xb4, X5, X5
	PSHUFD     $0xb4, X0, X0
	VPUNPCKLDQ X0, X5, X1
	VPUNPCKHDQ X0, X5, X3
	VADDPS     X3, X1, X5
	VSUBPS     X3, X1, X0
	DIVPS      dct4<>+0(SB), X0
	VPUNPCKLDQ X0, X5, X1
	VPUNPCKHDQ X0, X5, X3

	// DCT2
	PSHUFD    $0xd8, X1, X5
	PSHUFD    $0xd8, X3, X0
	VADDPS    X0, X5, X1
	VSUBPS    X0, X5, X0
	DIVPS     dct2<>+0(SB), X0
	VADDPS    X1, X0, X5
	VBLENDPS  $0x03, X1, X5, X5
	VSHUFPS   $0x88, X0, X5, X1
	VSHUFPS   $0xdd, X0, X5, X3
	VPSRLDQ   $0x04, X3, X0
	ADDPS     X0, X3
	VUNPCKLPS X3, X1, X0
	VUNPCKHPS X3, X1, X1

	// end DCT8
	// DCT8
	PSHUFD $0x1b, X9, X5
	VADDPS X5, X7, X9
	VSUBPS X5, X7, X7
	DIVPS  dct8<>+0(SB), X7

	// DCT4
	PSHUFD     $0xb4, X9, X9
	PSHUFD     $0xb4, X7, X7
	VPUNPCKLDQ X7, X9, X3
	VPUNPCKHDQ X7, X9, X5
	VADDPS     X5, X3, X9
	VSUBPS     X5, X3, X7
	DIVPS      dct4<>+0(SB), X7
	VPUNPCKLDQ X7, X9, X3
	VPUNPCKHDQ X7, X9, X5

	// DCT2
	PSHUFD    $0xd8, X3, X9
	PSHUFD    $0xd8, X5, X7
	VADDPS    X7, X9, X3
	VSUBPS    X7, X9, X7
	DIVPS     dct2<>+0(SB), X7
	VADDPS    X3, X7, X9
	VBLENDPS  $0x03, X3, X9, X9
	VSHUFPS   $0x88, X7, X9, X3
	VSHUFPS   $0xdd, X7, X9, X5
	VPSRLDQ   $0x04, X5, X7
	ADDPS     X7, X5
	VUNPCKLPS X5, X3, X7
	VUNPCKHPS X5, X3, X9

	// end DCT8
	VPSRLDQ   $0x04, X7, X3
	VPSLLDQ   $0x0c, X9, X5
	VADDPS    X3, X5, X3
	VADDPS    X3, X7, X7
	VPSRLDQ   $0x04, X9, X5
	VADDPS    X5, X9, X9
	VUNPCKLPS X7, X0, X3
	VUNPCKHPS X7, X0, X5
	VUNPCKLPS X9, X1, X7
	VUNPCKHPS X9, X1, X0

	// end DCT16
	VPSRLDQ   $0x04, X3, X1
	VPSLLDQ   $0x0c, X5, X9
	VADDPS    X1, X9, X1
	VADDPS    X1, X3, X3
	VUNPCKLPS X3, X2, X1
	MOVUPS    X1, 384(AX)
	VUNPCKHPS X3, X2, X9
	MOVUPS    X9, 400(AX)
	VPSRLDQ   $0x04, X5, X1
	VPSLLDQ   $0x0c, X7, X9
	VADDPS    X1, X9, X1
	VADDPS    X1, X5, X5
	VUNPCKLPS X5, X4, X1
	MOVUPS    X1, 416(AX)
	VUNPCKHPS X5, X4, X9
	MOVUPS    X9, 432(AX)
	VPSRLDQ   $0x04, X7, X1
	VPSLLDQ   $0x0c, X0, X9
	VADDPS    X1, X9, X1
	VADDPS    X1, X7, X7
	VUNPCKLPS X7, X6, X1
	MOVUPS    X1, 448(AX)
	VUNPCKHPS X7, X6, X9
	MOVUPS    X9, 464(AX)
	VPSRLDQ   $0x04, X0, X9
	VADDPS    X9, X0, X0
	VUNPCKLPS X0, X8, X1
	MOVUPS    X1, 480(AX)
	VUNPCKHPS X0, X8, X9
	MOVUPS    X9, 496(AX)

	// end DCT32
	VZEROUPPER
	MOVL       508(AX), CX
	VMOVUPS    256(AX), Y0
	VMOVUPS    288(AX), Y1
	VMOVUPS    320(AX), Y2
	VMOVUPS    352(AX), Y3
	VMOVUPS    384(AX), Y8
	VADDPS     388(AX), Y8, Y4
	VUNPCKLPS  Y4, Y0, Y8
	VUNPCKHPS  Y4, Y0, Y4
	VPERM2F128 $0x02, Y8, Y4, Y0
	VMOVUPS    Y0, 256(AX)
	VPERM2F128 $0x13, Y8, Y4, Y0
	VMOVUPS    Y0, 288(AX)
	VMOVUPS    416(AX), Y8
	VADDPS     420(AX), Y8, Y4
	VUNPCKLPS  Y4, Y1, Y8
	VUNPCKHPS  Y4, Y1, Y4
	VPERM2F128 $0x02, Y8, Y4, Y0
	VMOVUPS    Y0, 320(AX)
	VPERM2F128 $0x13, Y8, Y4, Y0
	VMOVUPS    Y0, 352(AX)
	VMOVUPS    448(AX), Y8
	VADDPS     452(AX), Y8, Y4
	VUNPCKLPS  Y4, Y2, Y8
	VUNPCKHPS  Y4, Y2, Y4
	VPERM2F128 $0x02, Y8, Y4, Y0
	VMOVUPS    Y0, 384(AX)
	VPERM2F128 $0x13, Y8, Y4, Y0
	VMOVUPS    Y0, 416(AX)
	VMOVUPS    480(AX), Y8
	VPMOVZXBD  perm<>+8(SB), Y7
	VPERMPS    480(AX), Y7, Y4
	VADDPS     Y4, Y8, Y4
	VUNPCKLPS  Y4, Y3, Y8
	VUNPCKHPS  Y4, Y3, Y4
	VPERM2F128 $0x02, Y8, Y4, Y0
	VMOVUPS    Y0, 448(AX)
	VPERM2F128 $0x13, Y8, Y4, Y0
	VMOVUPS    Y0, 480(AX)
	MOVL       CX, 508(AX)
	VZEROUPPER

	// end DCT64
	VZEROUPPER
	MOVL       508(AX), CX
	VMOVUPS    (AX), Y0
	VMOVUPS    256(AX), Y2
	VADDPS     260(AX), Y2, Y1
	VUNPCKLPS  Y1, Y0, Y2
	VUNPCKHPS  Y1, Y0, Y1
	VPERM2F128 $0x02, Y2, Y1, Y0
	VMOVUPS    Y0, (SP)
	VPERM2F128 $0x13, Y2, Y1, Y0
	VMOVUPS    Y0, 32(SP)
	VMOVUPS    32(AX), Y0
	VMOVUPS    288(AX), Y2
	VADDPS     292(AX), Y2, Y1
	VUNPCKLPS  Y1, Y0, Y2
	VUNPCKHPS  Y1, Y0, Y1
	VPERM2F128 $0x02, Y2, Y1, Y0
	VMOVUPS    Y0, 64(SP)
	VPERM2F128 $0x13, Y2, Y1, Y0
	VMOVUPS    Y0, 96(SP)
	VMOVUPS    64(AX), Y0
	VMOVUPS    320(AX), Y2
	VADDPS     324(AX), Y2, Y1
	VUNPCKLPS  Y1, Y0, Y2
	VUNPCKHPS  Y1, Y0, Y1
	VPERM2F128 $0x02, Y2, Y1, Y0
	VMOVUPS    Y0, 128(SP)
	VPERM2F128 $0x13, Y2, Y1, Y0
	VMOVUPS    Y0, 160(SP)
	VMOVUPS    96(AX), Y0
	VMOVUPS    352(AX), Y2
	VADDPS     356(AX), Y2, Y1
	VUNPCKLPS  Y1, Y0, Y2
	VUNPCKHPS  Y1, Y0, Y1
	VPERM2F128 $0x02, Y2, Y1, Y0
	VMOVUPS    Y0, 192(SP)
	VPERM2F128 $0x13, Y2, Y1, Y0
	VMOVUPS    Y0, 224(SP)
	VMOVUPS    128(AX), Y0
	VMOVUPS    384(AX), Y2
	VADDPS     388(AX), Y2, Y1
	VUNPCKLPS  Y1, Y0, Y2
	VUNPCKHPS  Y1, Y0, Y1
	VPERM2F128 $0x02, Y2, Y1, Y0
	VMOVUPS    Y0, 256(SP)
	VPERM2F128 $0x13, Y2, Y1, Y0
	VMOVUPS    Y0, 288(SP)
	VMOVUPS    160(AX), Y0
	VMOVUPS    416(AX), Y2
	VADDPS     420(AX), Y2, Y1
	VUNPCKLPS  Y1, Y0, Y2
	VUNPCKHPS  Y1, Y0, Y1
	VPERM2F128 $0x02, Y2, Y1, Y0
	VMOVUPS    Y0, 320(SP)
	VPERM2F128 $0x13, Y2, Y1, Y0
	VMOVUPS    Y0, 352(SP)
	VMOVUPS    192(AX), Y0
	VMOVUPS    448(AX), Y2
	VADDPS     452(AX), Y2, Y1
	VUNPCKLPS  Y1, Y0, Y2
	VUNPCKHPS  Y1, Y0, Y1
	VPERM2F128 $0x02, Y2, Y1, Y0
	VMOVUPS    Y0, 384(SP)
	VPERM2F128 $0x13, Y2, Y1, Y0
	VMOVUPS    Y0, 416(SP)
	VMOVUPS    224(AX), Y0
	VMOVUPS    480(AX), Y2
	VPMOVZXBD  perm<>+8(SB), Y3
	VPERMPS    480(AX), Y3, Y1
	VADDPS     Y1, Y2, Y1
	VUNPCKLPS  Y1, Y0, Y2
	VUNPCKHPS  Y1, Y0, Y1
	VPERM2F128 $0x02, Y2, Y1, Y0
	VMOVUPS    Y0, 448(SP)
	VPERM2F128 $0x13, Y2, Y1, Y0
	VMOVUPS    Y0, 480(SP)
	MOVL       CX, 508(SP)
	VZEROUPPER

	// end DCT128
	// DCT128
	VZEROUPPER
	VPMOVZXBD perm<>+0(SB), Y3
	VMOVUPS   512(SP), Y0
	VPERMD    992(SP), Y3, Y1
	VADDPS    Y1, Y0, Y2
	VMOVUPS   Y2, 512(AX)
	VSUBPS    Y1, Y0, Y2
	VDIVPS    dct128<>+0(SB), Y2, Y2
	VMOVUPS   Y2, 768(AX)
	VMOVUPS   544(SP), Y0
	VPERMD    960(SP), Y3, Y1
	VADDPS    Y1, Y0, Y2
	VMOVUPS   Y2, 544(AX)
	VSUBPS    Y1, Y0, Y2
	VDIVPS    dct128<>+32(SB), Y2, Y2
	VMOVUPS   Y2, 800(AX)
	VMOVUPS   576(SP), Y0
	VPERMD    928(SP), Y3, Y1
	VADDPS    Y1, Y0, Y2
	VMOVUPS   Y2, 576(AX)
	VSUBPS    Y1, Y0, Y2
	VDIVPS    dct128<>+64(SB), Y2, Y2
	VMOVUPS   Y2, 832(AX)
	VMOVUPS   608(SP), Y0
	VPERMD    896(SP), Y3, Y1
	VADDPS    Y1, Y0, Y2
	VMOVUPS   Y2, 608(AX)
	VSUBPS    Y1, Y0, Y2
	VDIVPS    dct128<>+96(SB), Y2, Y2
	VMOVUPS   Y2, 864(AX)
	VMOVUPS   640(SP), Y0
	VPERMD    864(SP), Y3, Y1
	VADDPS    Y1, Y0, Y2
	VMOVUPS   Y2, 640(AX)
	VSUBPS    Y1, Y0, Y2
	VDIVPS    dct128<>+128(SB), Y2, Y2
	VMOVUPS   Y2, 896(AX)
	VMOVUPS   672(SP), Y0
	VPERMD    832(SP), Y3, Y1
	VADDPS    Y1, Y0, Y2
	VMOVUPS   Y2, 672(AX)
	VSUBPS    Y1, Y0, Y2
	VDIVPS    dct128<>+160(SB), Y2, Y2
	VMOVUPS   Y2, 928(AX)
	VMOVUPS   704(SP), Y0
	VPERMD    800(SP), Y3, Y1
	VADDPS    Y1, Y0, Y2
	VMOVUPS   Y2, 704(AX)
	VSUBPS    Y1, Y0, Y2
	VDIVPS    dct128<>+192(SB), Y2, Y2
	VMOVUPS   Y2, 960(AX)
	VMOVUPS   736(SP), Y0
	VPERMD    768(SP), Y3, Y1
	VADDPS    Y1, Y0, Y2
	VMOVUPS   Y2, 736(AX)
	VSUBPS    Y1, Y0, Y2
	VDIVPS    dct128<>+224(SB), Y2, Y2
	VMOVUPS   Y2, 992(AX)
	VZEROALL
	VZEROUPPER

	// DCT64
	VPMOVZXBD perm<>+0(SB), Y7
	VMOVUPS   512(AX), Y0
	VMOVUPS   544(AX), Y1
	VMOVUPS   576(AX), Y2
	VMOVUPS   608(AX), Y3
	VPERMD    640(AX), Y7, Y4
	VPERMD    672(AX), Y7, Y5
	VPERMD    704(AX), Y7, Y6
	VPERMD    736(AX), Y7, Y7
	VADDPS    Y7, Y0, Y8
	VMOVUPS   Y8, 512(AX)
	VSUBPS    Y7, Y0, Y8
	VDIVPS    dct64<>+0(SB), Y8, Y8
	VMOVUPS   Y8, 640(AX)
	VADDPS    Y6, Y1, Y8
	VMOVUPS   Y8, 544(AX)
	VSUBPS    Y6, Y1, Y8
	VDIVPS    dct64<>+32(SB), Y8, Y8
	VMOVUPS   Y8, 672(AX)
	VADDPS    Y5, Y2, Y8
	VMOVUPS   Y8, 576(AX)
	VSUBPS    Y5, Y2, Y8
	VDIVPS    dct64<>+64(SB), Y8, Y8
	VMOVUPS   Y8, 704(AX)
	VADDPS    Y4, Y3, Y8
	VMOVUPS   Y8, 608(AX)
	VSUBPS    Y4, Y3, Y8
	VDIVPS    dct64<>+96(SB), Y8, Y8
	VMOVUPS   Y8, 736(AX)
	VZEROALL

	// DCT32
	MOVUPS 512(AX), X0
	MOVUPS 624(AX), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X2
	VSUBPS X1, X0, X3
	DIVPS  dct32<>+0(SB), X3
	MOVUPS 528(AX), X0
	MOVUPS 608(AX), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X4
	VSUBPS X1, X0, X5
	DIVPS  dct32<>+16(SB), X5
	MOVUPS 544(AX), X0
	MOVUPS 592(AX), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X6
	VSUBPS X1, X0, X7
	DIVPS  dct32<>+32(SB), X7
	MOVUPS 560(AX), X0
	MOVUPS 576(AX), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X8
	VSUBPS X1, X0, X0
	DIVPS  dct32<>+48(SB), X0

	// DCT16
	PSHUFD $0x1b, X6, X6
	VADDPS X6, X4, X1
	VSUBPS X6, X4, X9
	DIVPS  dct16<>+16(SB), X9
	PSHUFD $0x1b, X8, X6
	VADDPS X6, X2, X8
	VSUBPS X6, X2, X6
	DIVPS  dct16<>+0(SB), X6

	// DCT8
	PSHUFD $0x1b, X1, X2
	VADDPS X2, X8, X4
	VSUBPS X2, X8, X8
	DIVPS  dct8<>+0(SB), X8

	// DCT4
	PSHUFD     $0xb4, X4, X4
	PSHUFD     $0xb4, X8, X8
	VPUNPCKLDQ X8, X4, X1
	VPUNPCKHDQ X8, X4, X2
	VADDPS     X2, X1, X4
	VSUBPS     X2, X1, X8
	DIVPS      dct4<>+0(SB), X8
	VPUNPCKLDQ X8, X4, X1
	VPUNPCKHDQ X8, X4, X2

	// DCT2
	PSHUFD    $0xd8, X1, X4
	PSHUFD    $0xd8, X2, X8
	VADDPS    X8, X4, X1
	VSUBPS    X8, X4, X8
	DIVPS     dct2<>+0(SB), X8
	VADDPS    X1, X8, X4
	VBLENDPS  $0x03, X1, X4, X4
	VSHUFPS   $0x88, X8, X4, X1
	VSHUFPS   $0xdd, X8, X4, X2
	VPSRLDQ   $0x04, X2, X8
	ADDPS     X8, X2
	VUNPCKLPS X2, X1, X8
	VUNPCKHPS X2, X1, X1

	// end DCT8
	// DCT8
	PSHUFD $0x1b, X9, X4
	VADDPS X4, X6, X9
	VSUBPS X4, X6, X6
	DIVPS  dct8<>+0(SB), X6

	// DCT4
	PSHUFD     $0xb4, X9, X9
	PSHUFD     $0xb4, X6, X6
	VPUNPCKLDQ X6, X9, X2
	VPUNPCKHDQ X6, X9, X4
	VADDPS     X4, X2, X9
	VSUBPS     X4, X2, X6
	DIVPS      dct4<>+0(SB), X6
	VPUNPCKLDQ X6, X9, X2
	VPUNPCKHDQ X6, X9, X4

	// DCT2
	PSHUFD    $0xd8, X2, X9
	PSHUFD    $0xd8, X4, X6
	VADDPS    X6, X9, X2
	VSUBPS    X6, X9, X6
	DIVPS     dct2<>+0(SB), X6
	VADDPS    X2, X6, X9
	VBLENDPS  $0x03, X2, X9, X9
	VSHUFPS   $0x88, X6, X9, X2
	VSHUFPS   $0xdd, X6, X9, X4
	VPSRLDQ   $0x04, X4, X6
	ADDPS     X6, X4
	VUNPCKLPS X4, X2, X6
	VUNPCKHPS X4, X2, X9

	// end DCT8
	VPSRLDQ   $0x04, X6, X2
	VPSLLDQ   $0x0c, X9, X4
	VADDPS    X2, X4, X2
	VADDPS    X2, X6, X6
	VPSRLDQ   $0x04, X9, X4
	VADDPS    X4, X9, X9
	VUNPCKLPS X6, X8, X2
	VUNPCKHPS X6, X8, X4
	VUNPCKLPS X9, X1, X6
	VUNPCKHPS X9, X1, X8

	// end DCT16
	// DCT16
	PSHUFD $0x1b, X7, X7
	VADDPS X7, X5, X1
	VSUBPS X7, X5, X9
	DIVPS  dct16<>+16(SB), X9
	PSHUFD $0x1b, X0, X7
	VADDPS X7, X3, X0
	VSUBPS X7, X3, X7
	DIVPS  dct16<>+0(SB), X7

	// DCT8
	PSHUFD $0x1b, X1, X3
	VADDPS X3, X0, X5
	VSUBPS X3, X0, X0
	DIVPS  dct8<>+0(SB), X0

	// DCT4
	PSHUFD     $0xb4, X5, X5
	PSHUFD     $0xb4, X0, X0
	VPUNPCKLDQ X0, X5, X1
	VPUNPCKHDQ X0, X5, X3
	VADDPS     X3, X1, X5
	VSUBPS     X3, X1, X0
	DIVPS      dct4<>+0(SB), X0
	VPUNPCKLDQ X0, X5, X1
	VPUNPCKHDQ X0, X5, X3

	// DCT2
	PSHUFD    $0xd8, X1, X5
	PSHUFD    $0xd8, X3, X0
	VADDPS    X0, X5, X1
	VSUBPS    X0, X5, X0
	DIVPS     dct2<>+0(SB), X0
	VADDPS    X1, X0, X5
	VBLENDPS  $0x03, X1, X5, X5
	VSHUFPS   $0x88, X0, X5, X1
	VSHUFPS   $0xdd, X0, X5, X3
	VPSRLDQ   $0x04, X3, X0
	ADDPS     X0, X3
	VUNPCKLPS X3, X1, X0
	VUNPCKHPS X3, X1, X1

	// end DCT8
	// DCT8
	PSHUFD $0x1b, X9, X5
	VADDPS X5, X7, X9
	VSUBPS X5, X7, X7
	DIVPS  dct8<>+0(SB), X7

	// DCT4
	PSHUFD     $0xb4, X9, X9
	PSHUFD     $0xb4, X7, X7
	VPUNPCKLDQ X7, X9, X3
	VPUNPCKHDQ X7, X9, X5
	VADDPS     X5, X3, X9
	VSUBPS     X5, X3, X7
	DIVPS      dct4<>+0(SB), X7
	VPUNPCKLDQ X7, X9, X3
	VPUNPCKHDQ X7, X9, X5

	// DCT2
	PSHUFD    $0xd8, X3, X9
	PSHUFD    $0xd8, X5, X7
	VADDPS    X7, X9, X3
	VSUBPS    X7, X9, X7
	DIVPS     dct2<>+0(SB), X7
	VADDPS    X3, X7, X9
	VBLENDPS  $0x03, X3, X9, X9
	VSHUFPS   $0x88, X7, X9, X3
	VSHUFPS   $0xdd, X7, X9, X5
	VPSRLDQ   $0x04, X5, X7
	ADDPS     X7, X5
	VUNPCKLPS X5, X3, X7
	VUNPCKHPS X5, X3, X9

	// end DCT8
	VPSRLDQ   $0x04, X7, X3
	VPSLLDQ   $0x0c, X9, X5
	VADDPS    X3, X5, X3
	VADDPS    X3, X7, X7
	VPSRLDQ   $0x04, X9, X5
	VADDPS    X5, X9, X9
	VUNPCKLPS X7, X0, X3
	VUNPCKHPS X7, X0, X5
	VUNPCKLPS X9, X1, X7
	VUNPCKHPS X9, X1, X0

	// end DCT16
	VPSRLDQ   $0x04, X3, X1
	VPSLLDQ   $0x0c, X5, X9
	VADDPS    X1, X9, X1
	VADDPS    X1, X3, X3
	VUNPCKLPS X3, X2, X1
	MOVUPS    X1, 512(AX)
	VUNPCKHPS X3, X2, X9
	MOVUPS    X9, 528(AX)
	VPSRLDQ   $0x04, X5, X1
	VPSLLDQ   $0x0c, X7, X9
	VADDPS    X1, X9, X1
	VADDPS    X1, X5, X5
	VUNPCKLPS X5, X4, X1
	MOVUPS    X1, 544(AX)
	VUNPCKHPS X5, X4, X9
	MOVUPS    X9, 560(AX)
	VPSRLDQ   $0x04, X7, X1
	VPSLLDQ   $0x0c, X0, X9
	VADDPS    X1, X9, X1
	VADDPS    X1, X7, X7
	VUNPCKLPS X7, X6, X1
	MOVUPS    X1, 576(AX)
	VUNPCKHPS X7, X6, X9
	MOVUPS    X9, 592(AX)
	VPSRLDQ   $0x04, X0, X9
	VADDPS    X9, X0, X0
	VUNPCKLPS X0, X8, X1
	MOVUPS    X1, 608(AX)
	VUNPCKHPS X0, X8, X9
	MOVUPS    X9, 624(AX)

	// end DCT32
	// DCT32
	MOVUPS 640(AX), X0
	MOVUPS 752(AX), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X2
	VSUBPS X1, X0, X3
	DIVPS  dct32<>+0(SB), X3
	MOVUPS 656(AX), X0
	MOVUPS 736(AX), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X4
	VSUBPS X1, X0, X5
	DIVPS  dct32<>+16(SB), X5
	MOVUPS 672(AX), X0
	MOVUPS 720(AX), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X6
	VSUBPS X1, X0, X7
	DIVPS  dct32<>+32(SB), X7
	MOVUPS 688(AX), X0
	MOVUPS 704(AX), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X8
	VSUBPS X1, X0, X0
	DIVPS  dct32<>+48(SB), X0

	// DCT16
	PSHUFD $0x1b, X6, X6
	VADDPS X6, X4, X1
	VSUBPS X6, X4, X9
	DIVPS  dct16<>+16(SB), X9
	PSHUFD $0x1b, X8, X6
	VADDPS X6, X2, X8
	VSUBPS X6, X2, X6
	DIVPS  dct16<>+0(SB), X6

	// DCT8
	PSHUFD $0x1b, X1, X2
	VADDPS X2, X8, X4
	VSUBPS X2, X8, X8
	DIVPS  dct8<>+0(SB), X8

	// DCT4
	PSHUFD     $0xb4, X4, X4
	PSHUFD     $0xb4, X8, X8
	VPUNPCKLDQ X8, X4, X1
	VPUNPCKHDQ X8, X4, X2
	VADDPS     X2, X1, X4
	VSUBPS     X2, X1, X8
	DIVPS      dct4<>+0(SB), X8
	VPUNPCKLDQ X8, X4, X1
	VPUNPCKHDQ X8, X4, X2

	// DCT2
	PSHUFD    $0xd8, X1, X4
	PSHUFD    $0xd8, X2, X8
	VADDPS    X8, X4, X1
	VSUBPS    X8, X4, X8
	DIVPS     dct2<>+0(SB), X8
	VADDPS    X1, X8, X4
	VBLENDPS  $0x03, X1, X4, X4
	VSHUFPS   $0x88, X8, X4, X1
	VSHUFPS   $0xdd, X8, X4, X2
	VPSRLDQ   $0x04, X2, X8
	ADDPS     X8, X2
	VUNPCKLPS X2, X1, X8
	VUNPCKHPS X2, X1, X1

	// end DCT8
	// DCT8
	PSHUFD $0x1b, X9, X4
	VADDPS X4, X6, X9
	VSUBPS X4, X6, X6
	DIVPS  dct8<>+0(SB), X6

	// DCT4
	PSHUFD     $0xb4, X9, X9
	PSHUFD     $0xb4, X6, X6
	VPUNPCKLDQ X6, X9, X2
	VPUNPCKHDQ X6, X9, X4
	VADDPS     X4, X2, X9
	VSUBPS     X4, X2, X6
	DIVPS      dct4<>+0(SB), X6
	VPUNPCKLDQ X6, X9, X2
	VPUNPCKHDQ X6, X9, X4

	// DCT2
	PSHUFD    $0xd8, X2, X9
	PSHUFD    $0xd8, X4, X6
	VADDPS    X6, X9, X2
	VSUBPS    X6, X9, X6
	DIVPS     dct2<>+0(SB), X6
	VADDPS    X2, X6, X9
	VBLENDPS  $0x03, X2, X9, X9
	VSHUFPS   $0x88, X6, X9, X2
	VSHUFPS   $0xdd, X6, X9, X4
	VPSRLDQ   $0x04, X4, X6
	ADDPS     X6, X4
	VUNPCKLPS X4, X2, X6
	VUNPCKHPS X4, X2, X9

	// end DCT8
	VPSRLDQ   $0x04, X6, X2
	VPSLLDQ   $0x0c, X9, X4
	VADDPS    X2, X4, X2
	VADDPS    X2, X6, X6
	VPSRLDQ   $0x04, X9, X4
	VADDPS    X4, X9, X9
	VUNPCKLPS X6, X8, X2
	VUNPCKHPS X6, X8, X4
	VUNPCKLPS X9, X1, X6
	VUNPCKHPS X9, X1, X8

	// end DCT16
	// DCT16
	PSHUFD $0x1b, X7, X7
	VADDPS X7, X5, X1
	VSUBPS X7, X5, X9
	DIVPS  dct16<>+16(SB), X9
	PSHUFD $0x1b, X0, X7
	VADDPS X7, X3, X0
	VSUBPS X7, X3, X7
	DIVPS  dct16<>+0(SB), X7

	// DCT8
	PSHUFD $0x1b, X1, X3
	VADDPS X3, X0, X5
	VSUBPS X3, X0, X0
	DIVPS  dct8<>+0(SB), X0

	// DCT4
	PSHUFD     $0xb4, X5, X5
	PSHUFD     $0xb4, X0, X0
	VPUNPCKLDQ X0, X5, X1
	VPUNPCKHDQ X0, X5, X3
	VADDPS     X3, X1, X5
	VSUBPS     X3, X1, X0
	DIVPS      dct4<>+0(SB), X0
	VPUNPCKLDQ X0, X5, X1
	VPUNPCKHDQ X0, X5, X3

	// DCT2
	PSHUFD    $0xd8, X1, X5
	PSHUFD    $0xd8, X3, X0
	VADDPS    X0, X5, X1
	VSUBPS    X0, X5, X0
	DIVPS     dct2<>+0(SB), X0
	VADDPS    X1, X0, X5
	VBLENDPS  $0x03, X1, X5, X5
	VSHUFPS   $0x88, X0, X5, X1
	VSHUFPS   $0xdd, X0, X5, X3
	VPSRLDQ   $0x04, X3, X0
	ADDPS     X0, X3
	VUNPCKLPS X3, X1, X0
	VUNPCKHPS X3, X1, X1

	// end DCT8
	// DCT8
	PSHUFD $0x1b, X9, X5
	VADDPS X5, X7, X9
	VSUBPS X5, X7, X7
	DIVPS  dct8<>+0(SB), X7

	// DCT4
	PSHUFD     $0xb4, X9, X9
	PSHUFD     $0xb4, X7, X7
	VPUNPCKLDQ X7, X9, X3
	VPUNPCKHDQ X7, X9, X5
	VADDPS     X5, X3, X9
	VSUBPS     X5, X3, X7
	DIVPS      dct4<>+0(SB), X7
	VPUNPCKLDQ X7, X9, X3
	VPUNPCKHDQ X7, X9, X5

	// DCT2
	PSHUFD    $0xd8, X3, X9
	PSHUFD    $0xd8, X5, X7
	VADDPS    X7, X9, X3
	VSUBPS    X7, X9, X7
	DIVPS     dct2<>+0(SB), X7
	VADDPS    X3, X7, X9
	VBLENDPS  $0x03, X3, X9, X9
	VSHUFPS   $0x88, X7, X9, X3
	VSHUFPS   $0xdd, X7, X9, X5
	VPSRLDQ   $0x04, X5, X7
	ADDPS     X7, X5
	VUNPCKLPS X5, X3, X7
	VUNPCKHPS X5, X3, X9

	// end DCT8
	VPSRLDQ   $0x04, X7, X3
	VPSLLDQ   $0x0c, X9, X5
	VADDPS    X3, X5, X3
	VADDPS    X3, X7, X7
	VPSRLDQ   $0x04, X9, X5
	VADDPS    X5, X9, X9
	VUNPCKLPS X7, X0, X3
	VUNPCKHPS X7, X0, X5
	VUNPCKLPS X9, X1, X7
	VUNPCKHPS X9, X1, X0

	// end DCT16
	VPSRLDQ   $0x04, X3, X1
	VPSLLDQ   $0x0c, X5, X9
	VADDPS    X1, X9, X1
	VADDPS    X1, X3, X3
	VUNPCKLPS X3, X2, X1
	MOVUPS    X1, 640(AX)
	VUNPCKHPS X3, X2, X9
	MOVUPS    X9, 656(AX)
	VPSRLDQ   $0x04, X5, X1
	VPSLLDQ   $0x0c, X7, X9
	VADDPS    X1, X9, X1
	VADDPS    X1, X5, X5
	VUNPCKLPS X5, X4, X1
	MOVUPS    X1, 672(AX)
	VUNPCKHPS X5, X4, X9
	MOVUPS    X9, 688(AX)
	VPSRLDQ   $0x04, X7, X1
	VPSLLDQ   $0x0c, X0, X9
	VADDPS    X1, X9, X1
	VADDPS    X1, X7, X7
	VUNPCKLPS X7, X6, X1
	MOVUPS    X1, 704(AX)
	VUNPCKHPS X7, X6, X9
	MOVUPS    X9, 720(AX)
	VPSRLDQ   $0x04, X0, X9
	VADDPS    X9, X0, X0
	VUNPCKLPS X0, X8, X1
	MOVUPS    X1, 736(AX)
	VUNPCKHPS X0, X8, X9
	MOVUPS    X9, 752(AX)

	// end DCT32
	VZEROUPPER
	MOVL       764(AX), CX
	VMOVUPS    512(AX), Y0
	VMOVUPS    544(AX), Y1
	VMOVUPS    576(AX), Y2
	VMOVUPS    608(AX), Y3
	VMOVUPS    640(AX), Y8
	VADDPS     644(AX), Y8, Y4
	VUNPCKLPS  Y4, Y0, Y8
	VUNPCKHPS  Y4, Y0, Y4
	VPERM2F128 $0x02, Y8, Y4, Y0
	VMOVUPS    Y0, 512(AX)
	VPERM2F128 $0x13, Y8, Y4, Y0
	VMOVUPS    Y0, 544(AX)
	VMOVUPS    672(AX), Y8
	VADDPS     676(AX), Y8, Y4
	VUNPCKLPS  Y4, Y1, Y8
	VUNPCKHPS  Y4, Y1, Y4
	VPERM2F128 $0x02, Y8, Y4, Y0
	VMOVUPS    Y0, 576(AX)
	VPERM2F128 $0x13, Y8, Y4, Y0
	VMOVUPS    Y0, 608(AX)
	VMOVUPS    704(AX), Y8
	VADDPS     708(AX), Y8, Y4
	VUNPCKLPS  Y4, Y2, Y8
	VUNPCKHPS  Y4, Y2, Y4
	VPERM2F128 $0x02, Y8, Y4, Y0
	VMOVUPS    Y0, 640(AX)
	VPERM2F128 $0x13, Y8, Y4, Y0
	VMOVUPS    Y0, 672(AX)
	VMOVUPS    736(AX), Y8
	VPMOVZXBD  perm<>+8(SB), Y7
	VPERMPS    736(AX), Y7, Y4
	VADDPS     Y4, Y8, Y4
	VUNPCKLPS  Y4, Y3, Y8
	VUNPCKHPS  Y4, Y3, Y4
	VPERM2F128 $0x02, Y8, Y4, Y0
	VMOVUPS    Y0, 704(AX)
	VPERM2F128 $0x13, Y8, Y4, Y0
	VMOVUPS    Y0, 736(AX)
	MOVL       CX, 764(AX)
	VZEROUPPER

	// end DCT64
	VZEROUPPER

	// DCT64
	VPMOVZXBD perm<>+0(SB), Y7
	VMOVUPS   768(AX), Y0
	VMOVUPS   800(AX), Y1
	VMOVUPS   832(AX), Y2
	VMOVUPS   864(AX), Y3
	VPERMD    896(AX), Y7, Y4
	VPERMD    928(AX), Y7, Y5
	VPERMD    960(AX), Y7, Y6
	VPERMD    992(AX), Y7, Y7
	VADDPS    Y7, Y0, Y8
	VMOVUPS   Y8, 768(AX)
	VSUBPS    Y7, Y0, Y8
	VDIVPS    dct64<>+0(SB), Y8, Y8
	VMOVUPS   Y8, 896(AX)
	VADDPS    Y6, Y1, Y8
	VMOVUPS   Y8, 800(AX)
	VSUBPS    Y6, Y1, Y8
	VDIVPS    dct64<>+32(SB), Y8, Y8
	VMOVUPS   Y8, 928(AX)
	VADDPS    Y5, Y2, Y8
	VMOVUPS   Y8, 832(AX)
	VSUBPS    Y5, Y2, Y8
	VDIVPS    dct64<>+64(SB), Y8, Y8
	VMOVUPS   Y8, 960(AX)
	VADDPS    Y4, Y3, Y8
	VMOVUPS   Y8, 864(AX)
	VSUBPS    Y4, Y3, Y8
	VDIVPS    dct64<>+96(SB), Y8, Y8
	VMOVUPS   Y8, 992(AX)
	VZEROALL

	// DCT32
	MOVUPS 768(AX), X0
	MOVUPS 880(AX), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X2
	VSUBPS X1, X0, X3
	DIVPS  dct32<>+0(SB), X3
	MOVUPS 784(AX), X0
	MOVUPS 864(AX), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X4
	VSUBPS X1, X0, X5
	DIVPS  dct32<>+16(SB), X5
	MOVUPS 800(AX), X0
	MOVUPS 848(AX), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X6
	VSUBPS X1, X0, X7
	DIVPS  dct32<>+32(SB), X7
	MOVUPS 816(AX), X0
	MOVUPS 832(AX), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X8
	VSUBPS X1, X0, X0
	DIVPS  dct32<>+48(SB), X0

	// DCT16
	PSHUFD $0x1b, X6, X6
	VADDPS X6, X4, X1
	VSUBPS X6, X4, X9
	DIVPS  dct16<>+16(SB), X9
	PSHUFD $0x1b, X8, X6
	VADDPS X6, X2, X8
	VSUBPS X6, X2, X6
	DIVPS  dct16<>+0(SB), X6

	// DCT8
	PSHUFD $0x1b, X1, X2
	VADDPS X2, X8, X4
	VSUBPS X2, X8, X8
	DIVPS  dct8<>+0(SB), X8

	// DCT4
	PSHUFD     $0xb4, X4, X4
	PSHUFD     $0xb4, X8, X8
	VPUNPCKLDQ X8, X4, X1
	VPUNPCKHDQ X8, X4, X2
	VADDPS     X2, X1, X4
	VSUBPS     X2, X1, X8
	DIVPS      dct4<>+0(SB), X8
	VPUNPCKLDQ X8, X4, X1
	VPUNPCKHDQ X8, X4, X2

	// DCT2
	PSHUFD    $0xd8, X1, X4
	PSHUFD    $0xd8, X2, X8
	VADDPS    X8, X4, X1
	VSUBPS    X8, X4, X8
	DIVPS     dct2<>+0(SB), X8
	VADDPS    X1, X8, X4
	VBLENDPS  $0x03, X1, X4, X4
	VSHUFPS   $0x88, X8, X4, X1
	VSHUFPS   $0xdd, X8, X4, X2
	VPSRLDQ   $0x04, X2, X8
	ADDPS     X8, X2
	VUNPCKLPS X2, X1, X8
	VUNPCKHPS X2, X1, X1

	// end DCT8
	// DCT8
	PSHUFD $0x1b, X9, X4
	VADDPS X4, X6, X9
	VSUBPS X4, X6, X6
	DIVPS  dct8<>+0(SB), X6

	// DCT4
	PSHUFD     $0xb4, X9, X9
	PSHUFD     $0xb4, X6, X6
	VPUNPCKLDQ X6, X9, X2
	VPUNPCKHDQ X6, X9, X4
	VADDPS     X4, X2, X9
	VSUBPS     X4, X2, X6
	DIVPS      dct4<>+0(SB), X6
	VPUNPCKLDQ X6, X9, X2
	VPUNPCKHDQ X6, X9, X4

	// DCT2
	PSHUFD    $0xd8, X2, X9
	PSHUFD    $0xd8, X4, X6
	VADDPS    X6, X9, X2
	VSUBPS    X6, X9, X6
	DIVPS     dct2<>+0(SB), X6
	VADDPS    X2, X6, X9
	VBLENDPS  $0x03, X2, X9, X9
	VSHUFPS   $0x88, X6, X9, X2
	VSHUFPS   $0xdd, X6, X9, X4
	VPSRLDQ   $0x04, X4, X6
	ADDPS     X6, X4
	VUNPCKLPS X4, X2, X6
	VUNPCKHPS X4, X2, X9

	// end DCT8
	VPSRLDQ   $0x04, X6, X2
	VPSLLDQ   $0x0c, X9, X4
	VADDPS    X2, X4, X2
	VADDPS    X2, X6, X6
	VPSRLDQ   $0x04, X9, X4
	VADDPS    X4, X9, X9
	VUNPCKLPS X6, X8, X2
	VUNPCKHPS X6, X8, X4
	VUNPCKLPS X9, X1, X6
	VUNPCKHPS X9, X1, X8

	// end DCT16
	// DCT16
	PSHUFD $0x1b, X7, X7
	VADDPS X7, X5, X1
	VSUBPS X7, X5, X9
	DIVPS  dct16<>+16(SB), X9
	PSHUFD $0x1b, X0, X7
	VADDPS X7, X3, X0
	VSUBPS X7, X3, X7
	DIVPS  dct16<>+0(SB), X7

	// DCT8
	PSHUFD $0x1b, X1, X3
	VADDPS X3, X0, X5
	VSUBPS X3, X0, X0
	DIVPS  dct8<>+0(SB), X0

	// DCT4
	PSHUFD     $0xb4, X5, X5
	PSHUFD     $0xb4, X0, X0
	VPUNPCKLDQ X0, X5, X1
	VPUNPCKHDQ X0, X5, X3
	VADDPS     X3, X1, X5
	VSUBPS     X3, X1, X0
	DIVPS      dct4<>+0(SB), X0
	VPUNPCKLDQ X0, X5, X1
	VPUNPCKHDQ X0, X5, X3

	// DCT2
	PSHUFD    $0xd8, X1, X5
	PSHUFD    $0xd8, X3, X0
	VADDPS    X0, X5, X1
	VSUBPS    X0, X5, X0
	DIVPS     dct2<>+0(SB), X0
	VADDPS    X1, X0, X5
	VBLENDPS  $0x03, X1, X5, X5
	VSHUFPS   $0x88, X0, X5, X1
	VSHUFPS   $0xdd, X0, X5, X3
	VPSRLDQ   $0x04, X3, X0
	ADDPS     X0, X3
	VUNPCKLPS X3, X1, X0
	VUNPCKHPS X3, X1, X1

	// end DCT8
	// DCT8
	PSHUFD $0x1b, X9, X5
	VADDPS X5, X7, X9
	VSUBPS X5, X7, X7
	DIVPS  dct8<>+0(SB), X7

	// DCT4
	PSHUFD     $0xb4, X9, X9
	PSHUFD     $0xb4, X7, X7
	VPUNPCKLDQ X7, X9, X3
	VPUNPCKHDQ X7, X9, X5
	VADDPS     X5, X3, X9
	VSUBPS     X5, X3, X7
	DIVPS      dct4<>+0(SB), X7
	VPUNPCKLDQ X7, X9, X3
	VPUNPCKHDQ X7, X9, X5

	// DCT2
	PSHUFD    $0xd8, X3, X9
	PSHUFD    $0xd8, X5, X7
	VADDPS    X7, X9, X3
	VSUBPS    X7, X9, X7
	DIVPS     dct2<>+0(SB), X7
	VADDPS    X3, X7, X9
	VBLENDPS  $0x03, X3, X9, X9
	VSHUFPS   $0x88, X7, X9, X3
	VSHUFPS   $0xdd, X7, X9, X5
	VPSRLDQ   $0x04, X5, X7
	ADDPS     X7, X5
	VUNPCKLPS X5, X3, X7
	VUNPCKHPS X5, X3, X9

	// end DCT8
	VPSRLDQ   $0x04, X7, X3
	VPSLLDQ   $0x0c, X9, X5
	VADDPS    X3, X5, X3
	VADDPS    X3, X7, X7
	VPSRLDQ   $0x04, X9, X5
	VADDPS    X5, X9, X9
	VUNPCKLPS X7, X0, X3
	VUNPCKHPS X7, X0, X5
	VUNPCKLPS X9, X1, X7
	VUNPCKHPS X9, X1, X0

	// end DCT16
	VPSRLDQ   $0x04, X3, X1
	VPSLLDQ   $0x0c, X5, X9
	VADDPS    X1, X9, X1
	VADDPS    X1, X3, X3
	VUNPCKLPS X3, X2, X1
	MOVUPS    X1, 768(AX)
	VUNPCKHPS X3, X2, X9
	MOVUPS    X9, 784(AX)
	VPSRLDQ   $0x04, X5, X1
	VPSLLDQ   $0x0c, X7, X9
	VADDPS    X1, X9, X1
	VADDPS    X1, X5, X5
	VUNPCKLPS X5, X4, X1
	MOVUPS    X1, 800(AX)
	VUNPCKHPS X5, X4, X9
	MOVUPS    X9, 816(AX)
	VPSRLDQ   $0x04, X7, X1
	VPSLLDQ   $0x0c, X0, X9
	VADDPS    X1, X9, X1
	VADDPS    X1, X7, X7
	VUNPCKLPS X7, X6, X1
	MOVUPS    X1, 832(AX)
	VUNPCKHPS X7, X6, X9
	MOVUPS    X9, 848(AX)
	VPSRLDQ   $0x04, X0, X9
	VADDPS    X9, X0, X0
	VUNPCKLPS X0, X8, X1
	MOVUPS    X1, 864(AX)
	VUNPCKHPS X0, X8, X9
	MOVUPS    X9, 880(AX)

	// end DCT32
	// DCT32
	MOVUPS 896(AX), X0
	MOVUPS 1008(AX), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X2
	VSUBPS X1, X0, X3
	DIVPS  dct32<>+0(SB), X3
	MOVUPS 912(AX), X0
	MOVUPS 992(AX), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X4
	VSUBPS X1, X0, X5
	DIVPS  dct32<>+16(SB), X5
	MOVUPS 928(AX), X0
	MOVUPS 976(AX), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X6
	VSUBPS X1, X0, X7
	DIVPS  dct32<>+32(SB), X7
	MOVUPS 944(AX), X0
	MOVUPS 960(AX), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X8
	VSUBPS X1, X0, X0
	DIVPS  dct32<>+48(SB), X0

	// DCT16
	PSHUFD $0x1b, X6, X6
	VADDPS X6, X4, X1
	VSUBPS X6, X4, X9
	DIVPS  dct16<>+16(SB), X9
	PSHUFD $0x1b, X8, X6
	VADDPS X6, X2, X8
	VSUBPS X6, X2, X6
	DIVPS  dct16<>+0(SB), X6

	// DCT8
	PSHUFD $0x1b, X1, X2
	VADDPS X2, X8, X4
	VSUBPS X2, X8, X8
	DIVPS  dct8<>+0(SB), X8

	// DCT4
	PSHUFD     $0xb4, X4, X4
	PSHUFD     $0xb4, X8, X8
	VPUNPCKLDQ X8, X4, X1
	VPUNPCKHDQ X8, X4, X2
	VADDPS     X2, X1, X4
	VSUBPS     X2, X1, X8
	DIVPS      dct4<>+0(SB), X8
	VPUNPCKLDQ X8, X4, X1
	VPUNPCKHDQ X8, X4, X2

	// DCT2
	PSHUFD    $0xd8, X1, X4
	PSHUFD    $0xd8, X2, X8
	VADDPS    X8, X4, X1
	VSUBPS    X8, X4, X8
	DIVPS     dct2<>+0(SB), X8
	VADDPS    X1, X8, X4
	VBLENDPS  $0x03, X1, X4, X4
	VSHUFPS   $0x88, X8, X4, X1
	VSHUFPS   $0xdd, X8, X4, X2
	VPSRLDQ   $0x04, X2, X8
	ADDPS     X8, X2
	VUNPCKLPS X2, X1, X8
	VUNPCKHPS X2, X1, X1

	// end DCT8
	// DCT8
	PSHUFD $0x1b, X9, X4
	VADDPS X4, X6, X9
	VSUBPS X4, X6, X6
	DIVPS  dct8<>+0(SB), X6

	// DCT4
	PSHUFD     $0xb4, X9, X9
	PSHUFD     $0xb4, X6, X6
	VPUNPCKLDQ X6, X9, X2
	VPUNPCKHDQ X6, X9, X4
	VADDPS     X4, X2, X9
	VSUBPS     X4, X2, X6
	DIVPS      dct4<>+0(SB), X6
	VPUNPCKLDQ X6, X9, X2
	VPUNPCKHDQ X6, X9, X4

	// DCT2
	PSHUFD    $0xd8, X2, X9
	PSHUFD    $0xd8, X4, X6
	VADDPS    X6, X9, X2
	VSUBPS    X6, X9, X6
	DIVPS     dct2<>+0(SB), X6
	VADDPS    X2, X6, X9
	VBLENDPS  $0x03, X2, X9, X9
	VSHUFPS   $0x88, X6, X9, X2
	VSHUFPS   $0xdd, X6, X9, X4
	VPSRLDQ   $0x04, X4, X6
	ADDPS     X6, X4
	VUNPCKLPS X4, X2, X6
	VUNPCKHPS X4, X2, X9

	// end DCT8
	VPSRLDQ   $0x04, X6, X2
	VPSLLDQ   $0x0c, X9, X4
	VADDPS    X2, X4, X2
	VADDPS    X2, X6, X6
	VPSRLDQ   $0x04, X9, X4
	VADDPS    X4, X9, X9
	VUNPCKLPS X6, X8, X2
	VUNPCKHPS X6, X8, X4
	VUNPCKLPS X9, X1, X6
	VUNPCKHPS X9, X1, X8

	// end DCT16
	// DCT16
	PSHUFD $0x1b, X7, X7
	VADDPS X7, X5, X1
	VSUBPS X7, X5, X9
	DIVPS  dct16<>+16(SB), X9
	PSHUFD $0x1b, X0, X7
	VADDPS X7, X3, X0
	VSUBPS X7, X3, X7
	DIVPS  dct16<>+0(SB), X7

	// DCT8
	PSHUFD $0x1b, X1, X3
	VADDPS X3, X0, X5
	VSUBPS X3, X0, X0
	DIVPS  dct8<>+0(SB), X0

	// DCT4
	PSHUFD     $0xb4, X5, X5
	PSHUFD     $0xb4, X0, X0
	VPUNPCKLDQ X0, X5, X1
	VPUNPCKHDQ X0, X5, X3
	VADDPS     X3, X1, X5
	VSUBPS     X3, X1, X0
	DIVPS      dct4<>+0(SB), X0
	VPUNPCKLDQ X0, X5, X1
	VPUNPCKHDQ X0, X5, X3

	// DCT2
	PSHUFD    $0xd8, X1, X5
	PSHUFD    $0xd8, X3, X0
	VADDPS    X0, X5, X1
	VSUBPS    X0, X5, X0
	DIVPS     dct2<>+0(SB), X0
	VADDPS    X1, X0, X5
	VBLENDPS  $0x03, X1, X5, X5
	VSHUFPS   $0x88, X0, X5, X1
	VSHUFPS   $0xdd, X0, X5, X3
	VPSRLDQ   $0x04, X3, X0
	ADDPS     X0, X3
	VUNPCKLPS X3, X1, X0
	VUNPCKHPS X3, X1, X1

	// end DCT8
	// DCT8
	PSHUFD $0x1b, X9, X5
	VADDPS X5, X7, X9
	VSUBPS X5, X7, X7
	DIVPS  dct8<>+0(SB), X7

	// DCT4
	PSHUFD     $0xb4, X9, X9
	PSHUFD     $0xb4, X7, X7
	VPUNPCKLDQ X7, X9, X3
	VPUNPCKHDQ X7, X9, X5
	VADDPS     X5, X3, X9
	VSUBPS     X5, X3, X7
	DIVPS      dct4<>+0(SB), X7
	VPUNPCKLDQ X7, X9, X3
	VPUNPCKHDQ X7, X9, X5

	// DCT2
	PSHUFD    $0xd8, X3, X9
	PSHUFD    $0xd8, X5, X7
	VADDPS    X7, X9, X3
	VSUBPS    X7, X9, X7
	DIVPS     dct2<>+0(SB), X7
	VADDPS    X3, X7, X9
	VBLENDPS  $0x03, X3, X9, X9
	VSHUFPS   $0x88, X7, X9, X3
	VSHUFPS   $0xdd, X7, X9, X5
	VPSRLDQ   $0x04, X5, X7
	ADDPS     X7, X5
	VUNPCKLPS X5, X3, X7
	VUNPCKHPS X5, X3, X9

	// end DCT8
	VPSRLDQ   $0x04, X7, X3
	VPSLLDQ   $0x0c, X9, X5
	VADDPS    X3, X5, X3
	VADDPS    X3, X7, X7
	VPSRLDQ   $0x04, X9, X5
	VADDPS    X5, X9, X9
	VUNPCKLPS X7, X0, X3
	VUNPCKHPS X7, X0, X5
	VUNPCKLPS X9, X1, X7
	VUNPCKHPS X9, X1, X0

	// end DCT16
	VPSRLDQ   $0x04, X3, X1
	VPSLLDQ   $0x0c, X5, X9
	VADDPS    X1, X9, X1
	VADDPS    X1, X3, X3
	VUNPCKLPS X3, X2, X1
	MOVUPS    X1, 896(AX)
	VUNPCKHPS X3, X2, X9
	MOVUPS    X9, 912(AX)
	VPSRLDQ   $0x04, X5, X1
	VPSLLDQ   $0x0c, X7, X9
	VADDPS    X1, X9, X1
	VADDPS    X1, X5, X5
	VUNPCKLPS X5, X4, X1
	MOVUPS    X1, 928(AX)
	VUNPCKHPS X5, X4, X9
	MOVUPS    X9, 944(AX)
	VPSRLDQ   $0x04, X7, X1
	VPSLLDQ   $0x0c, X0, X9
	VADDPS    X1, X9, X1
	VADDPS    X1, X7, X7
	VUNPCKLPS X7, X6, X1
	MOVUPS    X1, 960(AX)
	VUNPCKHPS X7, X6, X9
	MOVUPS    X9, 976(AX)
	VPSRLDQ   $0x04, X0, X9
	VADDPS    X9, X0, X0
	VUNPCKLPS X0, X8, X1
	MOVUPS    X1, 992(AX)
	VUNPCKHPS X0, X8, X9
	MOVUPS    X9, 1008(AX)

	// end DCT32
	VZEROUPPER
	MOVL       1020(AX), CX
	VMOVUPS    768(AX), Y0
	VMOVUPS    800(AX), Y1
	VMOVUPS    832(AX), Y2
	VMOVUPS    864(AX), Y3
	VMOVUPS    896(AX), Y8
	VADDPS     900(AX), Y8, Y4
	VUNPCKLPS  Y4, Y0, Y8
	VUNPCKHPS  Y4, Y0, Y4
	VPERM2F128 $0x02, Y8, Y4, Y0
	VMOVUPS    Y0, 768(AX)
	VPERM2F128 $0x13, Y8, Y4, Y0
	VMOVUPS    Y0, 800(AX)
	VMOVUPS    928(AX), Y8
	VADDPS     932(AX), Y8, Y4
	VUNPCKLPS  Y4, Y1, Y8
	VUNPCKHPS  Y4, Y1, Y4
	VPERM2F128 $0x02, Y8, Y4, Y0
	VMOVUPS    Y0, 832(AX)
	VPERM2F128 $0x13, Y8, Y4, Y0
	VMOVUPS    Y0, 864(AX)
	VMOVUPS    960(AX), Y8
	VADDPS     964(AX), Y8, Y4
	VUNPCKLPS  Y4, Y2, Y8
	VUNPCKHPS  Y4, Y2, Y4
	VPERM2F128 $0x02, Y8, Y4, Y0
	VMOVUPS    Y0, 896(AX)
	VPERM2F128 $0x13, Y8, Y4, Y0
	VMOVUPS    Y0, 928(AX)
	VMOVUPS    992(AX), Y8
	VPMOVZXBD  perm<>+8(SB), Y7
	VPERMPS    992(AX), Y7, Y4
	VADDPS     Y4, Y8, Y4
	VUNPCKLPS  Y4, Y3, Y8
	VUNPCKHPS  Y4, Y3, Y4
	VPERM2F128 $0x02, Y8, Y4, Y0
	VMOVUPS    Y0, 960(AX)
	VPERM2F128 $0x13, Y8, Y4, Y0
	VMOVUPS    Y0, 992(AX)
	MOVL       CX, 1020(AX)
	VZEROUPPER

	// end DCT64
	VZEROUPPER
	MOVL       1020(AX), CX
	VMOVUPS    512(AX), Y0
	VMOVUPS    768(AX), Y2
	VADDPS     772(AX), Y2, Y1
	VUNPCKLPS  Y1, Y0, Y2
	VUNPCKHPS  Y1, Y0, Y1
	VPERM2F128 $0x02, Y2, Y1, Y0
	VMOVUPS    Y0, 512(SP)
	VPERM2F128 $0x13, Y2, Y1, Y0
	VMOVUPS    Y0, 544(SP)
	VMOVUPS    544(AX), Y0
	VMOVUPS    800(AX), Y2
	VADDPS     804(AX), Y2, Y1
	VUNPCKLPS  Y1, Y0, Y2
	VUNPCKHPS  Y1, Y0, Y1
	VPERM2F128 $0x02, Y2, Y1, Y0
	VMOVUPS    Y0, 576(SP)
	VPERM2F128 $0x13, Y2, Y1, Y0
	VMOVUPS    Y0, 608(SP)
	VMOVUPS    576(AX), Y0
	VMOVUPS    832(AX), Y2
	VADDPS     836(AX), Y2, Y1
	VUNPCKLPS  Y1, Y0, Y2
	VUNPCKHPS  Y1, Y0, Y1
	VPERM2F128 $0x02, Y2, Y1, Y0
	VMOVUPS    Y0, 640(SP)
	VPERM2F128 $0x13, Y2, Y1, Y0
	VMOVUPS    Y0, 672(SP)
	VMOVUPS    608(AX), Y0
	VMOVUPS    864(AX), Y2
	VADDPS     868(AX), Y2, Y1
	VUNPCKLPS  Y1, Y0, Y2
	VUNPCKHPS  Y1, Y0, Y1
	VPERM2F128 $0x02, Y2, Y1, Y0
	VMOVUPS    Y0, 704(SP)
	VPERM2F128 $0x13, Y2, Y1, Y0
	VMOVUPS    Y0, 736(SP)
	VMOVUPS    640(AX), Y0
	VMOVUPS    896(AX), Y2
	VADDPS     900(AX), Y2, Y1
	VUNPCKLPS  Y1, Y0, Y2
	VUNPCKHPS  Y1, Y0, Y1
	VPERM2F128 $0x02, Y2, Y1, Y0
	VMOVUPS    Y0, 768(SP)
	VPERM2F128 $0x13, Y2, Y1, Y0
	VMOVUPS    Y0, 800(SP)
	VMOVUPS    672(AX), Y0
	VMOVUPS    928(AX), Y2
	VADDPS     932(AX), Y2, Y1
	VUNPCKLPS  Y1, Y0, Y2
	VUNPCKHPS  Y1, Y0, Y1
	VPERM2F128 $0x02, Y2, Y1, Y0
	VMOVUPS    Y0, 832(SP)
	VPERM2F128 $0x13, Y2, Y1, Y0
	VMOVUPS    Y0, 864(SP)
	VMOVUPS    704(AX), Y0
	VMOVUPS    960(AX), Y2
	VADDPS     964(AX), Y2, Y1
	VUNPCKLPS  Y1, Y0, Y2
	VUNPCKHPS  Y1, Y0, Y1
	VPERM2F128 $0x02, Y2, Y1, Y0
	VMOVUPS    Y0, 896(SP)
	VPERM2F128 $0x13, Y2, Y1, Y0
	VMOVUPS    Y0, 928(SP)
	VMOVUPS    736(AX), Y0
	VMOVUPS    992(AX), Y2
	VPMOVZXBD  perm<>+8(SB), Y3
	VPERMPS    992(AX), Y3, Y1
	VADDPS     Y1, Y2, Y1
	VUNPCKLPS  Y1, Y0, Y2
	VUNPCKHPS  Y1, Y0, Y1
	VPERM2F128 $0x02, Y2, Y1, Y0
	VMOVUPS    Y0, 960(SP)
	VPERM2F128 $0x13, Y2, Y1, Y0
	VMOVUPS    Y0, 992(SP)
	MOVL       CX, 1020(SP)
	VZEROUPPER

	// end DCT128
	VZEROUPPER
	MOVL       1020(SP), CX
	VMOVUPS    (SP), Y0
	VMOVUPS    512(SP), Y2
	VADDPS     516(SP), Y2, Y1
	VUNPCKLPS  Y1, Y0, Y2
	VUNPCKHPS  Y1, Y0, Y1
	VPERM2F128 $0x02, Y2, Y1, Y0
	VMOVUPS    Y0, (AX)
	VPERM2F128 $0x13, Y2, Y1, Y0
	VMOVUPS    Y0, 32(AX)
	VMOVUPS    32(SP), Y0
	VMOVUPS    544(SP), Y2
	VADDPS     548(SP), Y2, Y1
	VUNPCKLPS  Y1, Y0, Y2
	VUNPCKHPS  Y1, Y0, Y1
	VPERM2F128 $0x02, Y2, Y1, Y0
	VMOVUPS    Y0, 64(AX)
	VPERM2F128 $0x13, Y2, Y1, Y0
	VMOVUPS    Y0, 96(AX)
	VMOVUPS    64(SP), Y0
	VMOVUPS    576(SP), Y2
	VADDPS     580(SP), Y2, Y1
	VUNPCKLPS  Y1, Y0, Y2
	VUNPCKHPS  Y1, Y0, Y1
	VPERM2F128 $0x02, Y2, Y1, Y0
	VMOVUPS    Y0, 128(AX)
	VPERM2F128 $0x13, Y2, Y1, Y0
	VMOVUPS    Y0, 160(AX)
	VMOVUPS    96(SP), Y0
	VMOVUPS    608(SP), Y2
	VADDPS     612(SP), Y2, Y1
	VUNPCKLPS  Y1, Y0, Y2
	VUNPCKHPS  Y1, Y0, Y1
	VPERM2F128 $0x02, Y2, Y1, Y0
	VMOVUPS    Y0, 192(AX)
	VPERM2F128 $0x13, Y2, Y1, Y0
	VMOVUPS    Y0, 224(AX)
	VMOVUPS    128(SP), Y0
	VMOVUPS    640(SP), Y2
	VADDPS     644(SP), Y2, Y1
	VUNPCKLPS  Y1, Y0, Y2
	VUNPCKHPS  Y1, Y0, Y1
	VPERM2F128 $0x02, Y2, Y1, Y0
	VMOVUPS    Y0, 256(AX)
	VPERM2F128 $0x13, Y2, Y1, Y0
	VMOVUPS    Y0, 288(AX)
	VMOVUPS    160(SP), Y0
	VMOVUPS    672(SP), Y2
	VADDPS     676(SP), Y2, Y1
	VUNPCKLPS  Y1, Y0, Y2
	VUNPCKHPS  Y1, Y0, Y1
	VPERM2F128 $0x02, Y2, Y1, Y0
	VMOVUPS    Y0, 320(AX)
	VPERM2F128 $0x13, Y2, Y1, Y0
	VMOVUPS    Y0, 352(AX)
	VMOVUPS    192(SP), Y0
	VMOVUPS    704(SP), Y2
	VADDPS     708(SP), Y2, Y1
	VUNPCKLPS  Y1, Y0, Y2
	VUNPCKHPS  Y1, Y0, Y1
	VPERM2F128 $0x02, Y2, Y1, Y0
	VMOVUPS    Y0, 384(AX)
	VPERM2F128 $0x13, Y2, Y1, Y0
	VMOVUPS    Y0, 416(AX)
	VMOVUPS    224(SP), Y0
	VMOVUPS    736(SP), Y2
	VADDPS     740(SP), Y2, Y1
	VUNPCKLPS  Y1, Y0, Y2
	VUNPCKHPS  Y1, Y0, Y1
	VPERM2F128 $0x02, Y2, Y1, Y0
	VMOVUPS    Y0, 448(AX)
	VPERM2F128 $0x13, Y2, Y1, Y0
	VMOVUPS    Y0, 480(AX)
	VMOVUPS    256(SP), Y0
	VMOVUPS    768(SP), Y2
	VADDPS     772(SP), Y2, Y1
	VUNPCKLPS  Y1, Y0, Y2
	VUNPCKHPS  Y1, Y0, Y1
	VPERM2F128 $0x02, Y2, Y1, Y0
	VMOVUPS    Y0, 512(AX)
	VPERM2F128 $0x13, Y2, Y1, Y0
	VMOVUPS    Y0, 544(AX)
	VMOVUPS    288(SP), Y0
	VMOVUPS    800(SP), Y2
	VADDPS     804(SP), Y2, Y1
	VUNPCKLPS  Y1, Y0, Y2
	VUNPCKHPS  Y1, Y0, Y1
	VPERM2F128 $0x02, Y2, Y1, Y0
	VMOVUPS    Y0, 576(AX)
	VPERM2F128 $0x13, Y2, Y1, Y0
	VMOVUPS    Y0, 608(AX)
	VMOVUPS    320(SP), Y0
	VMOVUPS    832(SP), Y2
	VADDPS     836(SP), Y2, Y1
	VUNPCKLPS  Y1, Y0, Y2
	VUNPCKHPS  Y1, Y0, Y1
	VPERM2F128 $0x02, Y2, Y1, Y0
	VMOVUPS    Y0, 640(AX)
	VPERM2F128 $0x13, Y2, Y1, Y0
	VMOVUPS    Y0, 672(AX)
	VMOVUPS    352(SP), Y0
	VMOVUPS    864(SP), Y2
	VADDPS     868(SP), Y2, Y1
	VUNPCKLPS  Y1, Y0, Y2
	VUNPCKHPS  Y1, Y0, Y1
	VPERM2F128 $0x02, Y2, Y1, Y0
	VMOVUPS    Y0, 704(AX)
	VPERM2F128 $0x13, Y2, Y1, Y0
	VMOVUPS    Y0, 736(AX)
	VMOVUPS    384(SP), Y0
	VMOVUPS    896(SP), Y2
	VADDPS     900(SP), Y2, Y1
	VUNPCKLPS  Y1, Y0, Y2
	VUNPCKHPS  Y1, Y0, Y1
	VPERM2F128 $0x02, Y2, Y1, Y0
	VMOVUPS    Y0, 768(AX)
	VPERM2F128 $0x13, Y2, Y1, Y0
	VMOVUPS    Y0, 800(AX)
	VMOVUPS    416(SP), Y0
	VMOVUPS    928(SP), Y2
	VADDPS     932(SP), Y2, Y1
	VUNPCKLPS  Y1, Y0, Y2
	VUNPCKHPS  Y1, Y0, Y1
	VPERM2F128 $0x02, Y2, Y1, Y0
	VMOVUPS    Y0, 832(AX)
	VPERM2F128 $0x13, Y2, Y1, Y0
	VMOVUPS    Y0, 864(AX)
	VMOVUPS    448(SP), Y0
	VMOVUPS    960(SP), Y2
	VADDPS     964(SP), Y2, Y1
	VUNPCKLPS  Y1, Y0, Y2
	VUNPCKHPS  Y1, Y0, Y1
	VPERM2F128 $0x02, Y2, Y1, Y0
	VMOVUPS    Y0, 896(AX)
	VPERM2F128 $0x13, Y2, Y1, Y0
	VMOVUPS    Y0, 928(AX)
	VMOVUPS    480(SP), Y0
	VMOVUPS    992(SP), Y2
	VPMOVZXBD  perm<>+8(SB), Y3
	VPERMPS    992(SP), Y3, Y1
	VADDPS     Y1, Y2, Y1
	VUNPCKLPS  Y1, Y0, Y2
	VUNPCKHPS  Y1, Y0, Y1
	VPERM2F128 $0x02, Y2, Y1, Y0
	VMOVUPS    Y0, 960(AX)
	VPERM2F128 $0x13, Y2, Y1, Y0
	VMOVUPS    Y0, 992(AX)
	MOVL       CX, 1020(AX)
	VZEROUPPER

	// end DCT256
	RET

// func AsmYCbCrToGray(pixels []float32, minX int, minY int, maxX int, maxY int, sY []uint8, sCb []uint8, sCr []uint8, yStride int, cStride int)
// Requires: AVX, AVX2
TEXT ·AsmYCbCrToGray(SB), NOSPLIT|NOPTR, $0-144
	MOVQ         yStride+128(FP), AX
	MOVQ         cStride+136(FP), BX
	MOVQ         maxY+48(FP), R8
	MOVQ         maxX+40(FP), R9
	MOVQ         sY_base+56(FP), R10
	MOVQ         sCb_base+80(FP), R11
	MOVQ         sCr_base+104(FP), R12
	MOVQ         pixels_base+0(FP), R13
	VZEROUPPER
	VPBROADCASTD constyCbCrGray<>+0(SB), Y0
	VPBROADCASTD constyCbCrGray<>+4(SB), Y1
	VPBROADCASTD constyCbCrGray<>+8(SB), Y2
	VPBROADCASTD constyCbCrGray<>+12(SB), Y3
	VPBROADCASTD constyCbCrGray<>+16(SB), Y4
	VPBROADCASTD constyCbCrGray<>+20(SB), Y5
	VPBROADCASTD constyCbCrGray<>+24(SB), Y6
	VPBROADCASTD constyCbCrGray<>+28(SB), Y7
	VPBROADCASTD constyCbCrGray<>+32(SB), Y8
	XORQ         R14, R14
	XORQ         R15, R15

y:
	CMPQ  R14, R8
	JE    done
	MOVQ  AX, CX
	IMULQ R14, CX
	MOVQ  BX, SI
	IMULQ R14, SI

x:
	CMPQ R15, R9
	JE   xDone

	// Start innerloop instructions
	MOVQ      CX, DX
	ADDQ      R15, DX
	MOVQ      SI, DI
	ADDQ      R15, DI
	VPMOVZXBD (R10)(DX*1), Y9
	VPMOVZXBD (R11)(DI*1), Y10
	VPMOVZXBD (R12)(DI*1), Y11
	VPMULLD   Y1, Y9, Y9
	VPSUBD    Y0, Y10, Y10
	VPSUBD    Y0, Y11, Y11
	VPMULLD   Y2, Y11, Y12
	VPADDD    Y9, Y12, Y12
	VPSRAD    $0x08, Y12, Y12
	VCVTDQ2PS Y12, Y12
	VMULPS    Y6, Y12, Y12
	VPMULLD   Y3, Y11, Y11
	VPMULLD   Y4, Y10, Y13
	VPSUBQ    Y13, Y9, Y13
	VPSUBQ    Y11, Y13, Y13
	VPSRAD    $0x08, Y13, Y13
	VCVTDQ2PS Y13, Y13
	VMULPS    Y7, Y13, Y13
	VPMULLD   Y5, Y10, Y10
	VPADDD    Y9, Y10, Y10
	VPSRAD    $0x08, Y10, Y10
	VCVTDQ2PS Y10, Y10
	VMULPS    Y8, Y10, Y10
	VADDPS    Y12, Y10, Y9
	VADDPS    Y13, Y9, Y9
	VMOVAPS   Y9, (R13)(DX*4)

	// End innerloop instructions
	ADDQ $0x08, R15
	JMP  x

xDone:
	XORQ R15, R15
	INCQ R14
	JMP  y

done:
	VZEROUPPER
	RET
