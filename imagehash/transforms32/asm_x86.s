// Code generated by command: go run asm.go -out asm_x86.s -stubs asm_x86.go. DO NOT EDIT.

#include "textflag.h"

DATA dct64<>+0(SB)/4, $(1.9993976)
DATA dct64<>+4(SB)/4, $(1.9945809)
DATA dct64<>+8(SB)/4, $(1.9849591)
DATA dct64<>+12(SB)/4, $(1.9705553)
DATA dct64<>+16(SB)/4, $(1.9514042)
DATA dct64<>+20(SB)/4, $(1.9275521)
DATA dct64<>+24(SB)/4, $(1.8990563)
DATA dct64<>+28(SB)/4, $(1.8659856)
DATA dct64<>+32(SB)/4, $(1.8284196)
DATA dct64<>+36(SB)/4, $(1.7864486)
DATA dct64<>+40(SB)/4, $(1.7401739)
DATA dct64<>+44(SB)/4, $(1.6897072)
DATA dct64<>+48(SB)/4, $(1.6351696)
DATA dct64<>+52(SB)/4, $(1.5766928)
DATA dct64<>+56(SB)/4, $(1.5144176)
DATA dct64<>+60(SB)/4, $(1.4484942)
DATA dct64<>+64(SB)/4, $(1.3790811)
DATA dct64<>+68(SB)/4, $(1.3063457)
DATA dct64<>+72(SB)/4, $(1.2304631)
DATA dct64<>+76(SB)/4, $(1.1516163)
DATA dct64<>+80(SB)/4, $(1.0699953)
DATA dct64<>+84(SB)/4, $(0.9857964)
DATA dct64<>+88(SB)/4, $(0.8992227)
DATA dct64<>+92(SB)/4, $(0.8104826)
DATA dct64<>+96(SB)/4, $(0.7197901)
DATA dct64<>+100(SB)/4, $(0.6273635)
DATA dct64<>+104(SB)/4, $(0.5334255)
DATA dct64<>+108(SB)/4, $(0.43820247)
DATA dct64<>+112(SB)/4, $(0.34192377)
DATA dct64<>+116(SB)/4, $(0.24482135)
DATA dct64<>+120(SB)/4, $(0.14712913)
DATA dct64<>+124(SB)/4, $(0.049082458)
GLOBL dct64<>(SB), RODATA|NOPTR, $128

DATA dct32<>+0(SB)/4, $(1.9975909)
DATA dct32<>+4(SB)/4, $(1.978353)
DATA dct32<>+8(SB)/4, $(1.9400625)
DATA dct32<>+12(SB)/4, $(1.8830881)
DATA dct32<>+16(SB)/4, $(1.8079786)
DATA dct32<>+20(SB)/4, $(1.7154572)
DATA dct32<>+24(SB)/4, $(1.606415)
DATA dct32<>+28(SB)/4, $(1.4819022)
DATA dct32<>+32(SB)/4, $(1.343118)
DATA dct32<>+36(SB)/4, $(1.1913986)
DATA dct32<>+40(SB)/4, $(1.0282055)
DATA dct32<>+44(SB)/4, $(0.85511017)
DATA dct32<>+48(SB)/4, $(0.6737797)
DATA dct32<>+52(SB)/4, $(0.48596036)
DATA dct32<>+56(SB)/4, $(0.29346094)
DATA dct32<>+60(SB)/4, $(0.09813535)
GLOBL dct32<>(SB), RODATA|NOPTR, $64

DATA dct16<>+0(SB)/4, $(1.9903694)
DATA dct16<>+4(SB)/4, $(1.9138807)
DATA dct16<>+8(SB)/4, $(1.7638426)
DATA dct16<>+12(SB)/4, $(1.5460209)
DATA dct16<>+16(SB)/4, $(1.2687865)
DATA dct16<>+20(SB)/4, $(0.9427935)
DATA dct16<>+24(SB)/4, $(0.5805693)
DATA dct16<>+28(SB)/4, $(0.19603428)
GLOBL dct16<>(SB), RODATA|NOPTR, $32

DATA dct8<>+0(SB)/4, $(1.9615705)
DATA dct8<>+4(SB)/4, $(1.6629392)
DATA dct8<>+8(SB)/4, $(1.1111405)
DATA dct8<>+12(SB)/4, $(0.39018065)
GLOBL dct8<>(SB), RODATA|NOPTR, $16

DATA dct4<>+0(SB)/4, $(1.847759)
DATA dct4<>+4(SB)/4, $(1.847759)
DATA dct4<>+8(SB)/4, $(0.76536685)
DATA dct4<>+12(SB)/4, $(0.76536685)
GLOBL dct4<>(SB), RODATA|NOPTR, $16

DATA dct2<>+0(SB)/4, $(1.4142135)
DATA dct2<>+4(SB)/4, $(1.4142135)
DATA dct2<>+8(SB)/4, $(1.4142135)
DATA dct2<>+12(SB)/4, $(1.4142135)
GLOBL dct2<>(SB), RODATA|NOPTR, $16

DATA gather<>+0(SB)/4, $0x00000000
DATA gather<>+4(SB)/4, $0x00000040
DATA gather<>+8(SB)/4, $0x00000080
DATA gather<>+12(SB)/4, $0x000000c0
DATA gather<>+16(SB)/4, $0x00000100
DATA gather<>+20(SB)/4, $0x00000140
DATA gather<>+24(SB)/4, $0x00000180
DATA gather<>+28(SB)/4, $0x000001c0
DATA gather<>+32(SB)/4, $0x00000001
GLOBL gather<>(SB), RODATA|NOPTR, $36

DATA perm<>+0(SB)/1, $0x07
DATA perm<>+1(SB)/1, $0x06
DATA perm<>+2(SB)/1, $0x05
DATA perm<>+3(SB)/1, $0x04
DATA perm<>+4(SB)/1, $0x03
DATA perm<>+5(SB)/1, $0x02
DATA perm<>+6(SB)/1, $0x01
DATA perm<>+7(SB)/1, $0x00
GLOBL perm<>(SB), RODATA|NOPTR, $8

DATA pg4<>+0(SB)/4, $+128
DATA pg4<>+4(SB)/4, $+65793
DATA pg4<>+8(SB)/4, $+91881
DATA pg4<>+12(SB)/4, $+46802
DATA pg4<>+16(SB)/4, $+22554
DATA pg4<>+20(SB)/4, $+116130
DATA pg4<>+24(SB)/4, $(257)
DATA pg4<>+28(SB)/4, $(0.29783657)
DATA pg4<>+32(SB)/4, $(0.58471596)
DATA pg4<>+36(SB)/4, $(0.114)
GLOBL pg4<>(SB), RODATA|NOPTR, $40

DATA pg8<>+0(SB)/4, $+128
DATA pg8<>+4(SB)/4, $+65793
DATA pg8<>+8(SB)/4, $+91881
DATA pg8<>+12(SB)/4, $+46802
DATA pg8<>+16(SB)/4, $+22554
DATA pg8<>+20(SB)/4, $+116130
DATA pg8<>+24(SB)/4, $(257)
DATA pg8<>+28(SB)/4, $(0.29783657)
DATA pg8<>+32(SB)/4, $(0.58471596)
DATA pg8<>+36(SB)/4, $(0.114)
GLOBL pg8<>(SB), RODATA|NOPTR, $40

// func asmDCT2DHash64(input []float32) [64]float32
// Requires: AVX, AVX2, SSE, SSE2, SSE4.1
TEXT ·asmDCT2DHash64(SB), NOSPLIT, $272-280
	MOVQ input_base+0(FP), AX
	XORL CX, CX
	XORL DX, DX
	XORL BX, BX

j:
	CMPL DX, $0x40
	JE   i

	// Start innerloop instructions
	MOVL      $0x00000040, BX
	IMULL     DX, BX
	VZEROUPPER
	VPMOVZXBD perm<>+0(SB), Y9
	VMOVAPS   (AX)(BX*4), Y0
	VMOVAPS   32(AX)(BX*4), Y1
	VMOVAPS   64(AX)(BX*4), Y4
	VMOVAPS   96(AX)(BX*4), Y5
	VPERMD    128(AX)(BX*4), Y9, Y6
	VPERMD    160(AX)(BX*4), Y9, Y7
	VPERMD    192(AX)(BX*4), Y9, Y8
	VPERMD    224(AX)(BX*4), Y9, Y9
	VADDPS    Y9, Y0, Y10
	VMOVAPS   Y10, (AX)(BX*4)
	VSUBPS    Y9, Y0, Y10
	VDIVPS    dct64<>+0(SB), Y10, Y10
	VMOVAPS   Y10, 128(AX)(BX*4)
	VADDPS    Y8, Y1, Y10
	VMOVAPS   Y10, 32(AX)(BX*4)
	VSUBPS    Y8, Y1, Y10
	VDIVPS    dct64<>+32(SB), Y10, Y10
	VMOVAPS   Y10, 160(AX)(BX*4)
	VADDPS    Y7, Y4, Y10
	VMOVAPS   Y10, 64(AX)(BX*4)
	VSUBPS    Y7, Y4, Y10
	VDIVPS    dct64<>+64(SB), Y10, Y10
	VMOVAPS   Y10, 192(AX)(BX*4)
	VADDPS    Y6, Y5, Y10
	VMOVAPS   Y10, 96(AX)(BX*4)
	VSUBPS    Y6, Y5, Y10
	VDIVPS    dct64<>+96(SB), Y10, Y10
	VMOVAPS   Y10, 224(AX)(BX*4)
	VZEROUPPER

	// DCT32
	MOVAPS (AX)(BX*4), X0
	PSHUFD $0x1b, 112(AX)(BX*4), X1
	VADDPS X1, X0, X4
	VSUBPS X1, X0, X5
	DIVPS  dct32<>+0(SB), X5
	MOVAPS 16(AX)(BX*4), X0
	PSHUFD $0x1b, 96(AX)(BX*4), X1
	VADDPS X1, X0, X6
	VSUBPS X1, X0, X7
	DIVPS  dct32<>+16(SB), X7
	MOVAPS 32(AX)(BX*4), X0
	PSHUFD $0x1b, 80(AX)(BX*4), X1
	VADDPS X1, X0, X8
	VSUBPS X1, X0, X9
	DIVPS  dct32<>+32(SB), X9
	MOVAPS 48(AX)(BX*4), X0
	PSHUFD $0x1b, 64(AX)(BX*4), X1
	VADDPS X1, X0, X10
	VSUBPS X1, X0, X0
	DIVPS  dct32<>+48(SB), X0

	// DCT16
	PSHUFD $0x1b, X8, X8
	VADDPS X8, X6, X1
	VSUBPS X8, X6, X11
	DIVPS  dct16<>+16(SB), X11
	PSHUFD $0x1b, X10, X8
	VADDPS X8, X4, X10
	VSUBPS X8, X4, X8
	DIVPS  dct16<>+0(SB), X8

	// DCT8
	PSHUFD $0x1b, X1, X4
	VADDPS X4, X10, X6
	VSUBPS X4, X10, X10
	DIVPS  dct8<>+0(SB), X10

	// DCT4
	PSHUFD     $0xb4, X6, X6
	PSHUFD     $0xb4, X10, X10
	VPUNPCKLDQ X10, X6, X1
	VPUNPCKHDQ X10, X6, X4
	VADDPS     X4, X1, X6
	VSUBPS     X4, X1, X10
	DIVPS      dct4<>+0(SB), X10
	VPUNPCKLDQ X10, X6, X1
	VPUNPCKHDQ X10, X6, X4

	// DCT2
	PSHUFD    $0xd8, X1, X6
	PSHUFD    $0xd8, X4, X10
	VADDPS    X10, X6, X1
	VSUBPS    X10, X6, X10
	DIVPS     dct2<>+0(SB), X10
	VADDPS    X1, X10, X6
	VBLENDPS  $0x03, X1, X6, X6
	VSHUFPS   $0x88, X10, X6, X1
	VSHUFPS   $0xdd, X10, X6, X4
	VPSRLDQ   $0x04, X4, X10
	ADDPS     X10, X4
	VUNPCKLPS X4, X1, X10
	VUNPCKHPS X4, X1, X1

	// end DCT8
	// DCT8
	PSHUFD $0x1b, X11, X6
	VADDPS X6, X8, X11
	VSUBPS X6, X8, X8
	DIVPS  dct8<>+0(SB), X8

	// DCT4
	PSHUFD     $0xb4, X11, X11
	PSHUFD     $0xb4, X8, X8
	VPUNPCKLDQ X8, X11, X4
	VPUNPCKHDQ X8, X11, X6
	VADDPS     X6, X4, X11
	VSUBPS     X6, X4, X8
	DIVPS      dct4<>+0(SB), X8
	VPUNPCKLDQ X8, X11, X4
	VPUNPCKHDQ X8, X11, X6

	// DCT2
	PSHUFD    $0xd8, X4, X11
	PSHUFD    $0xd8, X6, X8
	VADDPS    X8, X11, X4
	VSUBPS    X8, X11, X8
	DIVPS     dct2<>+0(SB), X8
	VADDPS    X4, X8, X11
	VBLENDPS  $0x03, X4, X11, X11
	VSHUFPS   $0x88, X8, X11, X4
	VSHUFPS   $0xdd, X8, X11, X6
	VPSRLDQ   $0x04, X6, X8
	ADDPS     X8, X6
	VUNPCKLPS X6, X4, X8
	VUNPCKHPS X6, X4, X11

	// end DCT8
	VPSRLDQ   $0x04, X8, X4
	VPSLLDQ   $0x0c, X11, X6
	VADDPS    X4, X6, X4
	VADDPS    X4, X8, X8
	VPSRLDQ   $0x04, X11, X6
	VADDPS    X6, X11, X11
	VUNPCKLPS X8, X10, X4
	VUNPCKHPS X8, X10, X6
	VUNPCKLPS X11, X1, X8
	VUNPCKHPS X11, X1, X10

	// end DCT16
	// DCT16
	PSHUFD $0x1b, X9, X9
	VADDPS X9, X7, X1
	VSUBPS X9, X7, X11
	DIVPS  dct16<>+16(SB), X11
	PSHUFD $0x1b, X0, X9
	VADDPS X9, X5, X0
	VSUBPS X9, X5, X9
	DIVPS  dct16<>+0(SB), X9

	// DCT8
	PSHUFD $0x1b, X1, X5
	VADDPS X5, X0, X7
	VSUBPS X5, X0, X0
	DIVPS  dct8<>+0(SB), X0

	// DCT4
	PSHUFD     $0xb4, X7, X7
	PSHUFD     $0xb4, X0, X0
	VPUNPCKLDQ X0, X7, X1
	VPUNPCKHDQ X0, X7, X5
	VADDPS     X5, X1, X7
	VSUBPS     X5, X1, X0
	DIVPS      dct4<>+0(SB), X0
	VPUNPCKLDQ X0, X7, X1
	VPUNPCKHDQ X0, X7, X5

	// DCT2
	PSHUFD    $0xd8, X1, X7
	PSHUFD    $0xd8, X5, X0
	VADDPS    X0, X7, X1
	VSUBPS    X0, X7, X0
	DIVPS     dct2<>+0(SB), X0
	VADDPS    X1, X0, X7
	VBLENDPS  $0x03, X1, X7, X7
	VSHUFPS   $0x88, X0, X7, X1
	VSHUFPS   $0xdd, X0, X7, X5
	VPSRLDQ   $0x04, X5, X0
	ADDPS     X0, X5
	VUNPCKLPS X5, X1, X0
	VUNPCKHPS X5, X1, X1

	// end DCT8
	// DCT8
	PSHUFD $0x1b, X11, X7
	VADDPS X7, X9, X11
	VSUBPS X7, X9, X9
	DIVPS  dct8<>+0(SB), X9

	// DCT4
	PSHUFD     $0xb4, X11, X11
	PSHUFD     $0xb4, X9, X9
	VPUNPCKLDQ X9, X11, X5
	VPUNPCKHDQ X9, X11, X7
	VADDPS     X7, X5, X11
	VSUBPS     X7, X5, X9
	DIVPS      dct4<>+0(SB), X9
	VPUNPCKLDQ X9, X11, X5
	VPUNPCKHDQ X9, X11, X7

	// DCT2
	PSHUFD    $0xd8, X5, X11
	PSHUFD    $0xd8, X7, X9
	VADDPS    X9, X11, X5
	VSUBPS    X9, X11, X9
	DIVPS     dct2<>+0(SB), X9
	VADDPS    X5, X9, X11
	VBLENDPS  $0x03, X5, X11, X11
	VSHUFPS   $0x88, X9, X11, X5
	VSHUFPS   $0xdd, X9, X11, X7
	VPSRLDQ   $0x04, X7, X9
	ADDPS     X9, X7
	VUNPCKLPS X7, X5, X9
	VUNPCKHPS X7, X5, X11

	// end DCT8
	VPSRLDQ   $0x04, X9, X5
	VPSLLDQ   $0x0c, X11, X7
	VADDPS    X5, X7, X5
	VADDPS    X5, X9, X9
	VPSRLDQ   $0x04, X11, X7
	VADDPS    X7, X11, X11
	VUNPCKLPS X9, X0, X5
	VUNPCKHPS X9, X0, X7
	VUNPCKLPS X11, X1, X9
	VUNPCKHPS X11, X1, X0

	// end DCT16
	VPSRLDQ   $0x04, X5, X1
	VPSLLDQ   $0x0c, X7, X11
	VADDPS    X1, X11, X1
	VADDPS    X1, X5, X5
	VUNPCKLPS X5, X4, X1
	MOVAPS    X1, (AX)(BX*4)
	VUNPCKHPS X5, X4, X11
	MOVAPS    X11, 16(AX)(BX*4)
	VPSRLDQ   $0x04, X7, X1
	VPSLLDQ   $0x0c, X9, X11
	VADDPS    X1, X11, X1
	VADDPS    X1, X7, X7
	VUNPCKLPS X7, X6, X1
	MOVAPS    X1, 32(AX)(BX*4)
	VUNPCKHPS X7, X6, X11
	MOVAPS    X11, 48(AX)(BX*4)
	VPSRLDQ   $0x04, X9, X1
	VPSLLDQ   $0x0c, X0, X11
	VADDPS    X1, X11, X1
	VADDPS    X1, X9, X9
	VUNPCKLPS X9, X8, X1
	MOVAPS    X1, 64(AX)(BX*4)
	VUNPCKHPS X9, X8, X11
	MOVAPS    X11, 80(AX)(BX*4)
	VPSRLDQ   $0x04, X0, X11
	VADDPS    X11, X0, X0
	VUNPCKLPS X0, X10, X1
	MOVAPS    X1, 96(AX)(BX*4)
	VUNPCKHPS X0, X10, X11
	MOVAPS    X11, 112(AX)(BX*4)

	// end DCT32
	// DCT32
	MOVAPS 128(AX)(BX*4), X0
	PSHUFD $0x1b, 240(AX)(BX*4), X1
	VADDPS X1, X0, X4
	VSUBPS X1, X0, X5
	DIVPS  dct32<>+0(SB), X5
	MOVAPS 144(AX)(BX*4), X0
	PSHUFD $0x1b, 224(AX)(BX*4), X1
	VADDPS X1, X0, X6
	VSUBPS X1, X0, X7
	DIVPS  dct32<>+16(SB), X7
	MOVAPS 160(AX)(BX*4), X0
	PSHUFD $0x1b, 208(AX)(BX*4), X1
	VADDPS X1, X0, X8
	VSUBPS X1, X0, X9
	DIVPS  dct32<>+32(SB), X9
	MOVAPS 176(AX)(BX*4), X0
	PSHUFD $0x1b, 192(AX)(BX*4), X1
	VADDPS X1, X0, X10
	VSUBPS X1, X0, X0
	DIVPS  dct32<>+48(SB), X0

	// DCT16
	PSHUFD $0x1b, X8, X8
	VADDPS X8, X6, X1
	VSUBPS X8, X6, X11
	DIVPS  dct16<>+16(SB), X11
	PSHUFD $0x1b, X10, X8
	VADDPS X8, X4, X10
	VSUBPS X8, X4, X8
	DIVPS  dct16<>+0(SB), X8

	// DCT8
	PSHUFD $0x1b, X1, X4
	VADDPS X4, X10, X6
	VSUBPS X4, X10, X10
	DIVPS  dct8<>+0(SB), X10

	// DCT4
	PSHUFD     $0xb4, X6, X6
	PSHUFD     $0xb4, X10, X10
	VPUNPCKLDQ X10, X6, X1
	VPUNPCKHDQ X10, X6, X4
	VADDPS     X4, X1, X6
	VSUBPS     X4, X1, X10
	DIVPS      dct4<>+0(SB), X10
	VPUNPCKLDQ X10, X6, X1
	VPUNPCKHDQ X10, X6, X4

	// DCT2
	PSHUFD    $0xd8, X1, X6
	PSHUFD    $0xd8, X4, X10
	VADDPS    X10, X6, X1
	VSUBPS    X10, X6, X10
	DIVPS     dct2<>+0(SB), X10
	VADDPS    X1, X10, X6
	VBLENDPS  $0x03, X1, X6, X6
	VSHUFPS   $0x88, X10, X6, X1
	VSHUFPS   $0xdd, X10, X6, X4
	VPSRLDQ   $0x04, X4, X10
	ADDPS     X10, X4
	VUNPCKLPS X4, X1, X10
	VUNPCKHPS X4, X1, X1

	// end DCT8
	// DCT8
	PSHUFD $0x1b, X11, X6
	VADDPS X6, X8, X11
	VSUBPS X6, X8, X8
	DIVPS  dct8<>+0(SB), X8

	// DCT4
	PSHUFD     $0xb4, X11, X11
	PSHUFD     $0xb4, X8, X8
	VPUNPCKLDQ X8, X11, X4
	VPUNPCKHDQ X8, X11, X6
	VADDPS     X6, X4, X11
	VSUBPS     X6, X4, X8
	DIVPS      dct4<>+0(SB), X8
	VPUNPCKLDQ X8, X11, X4
	VPUNPCKHDQ X8, X11, X6

	// DCT2
	PSHUFD    $0xd8, X4, X11
	PSHUFD    $0xd8, X6, X8
	VADDPS    X8, X11, X4
	VSUBPS    X8, X11, X8
	DIVPS     dct2<>+0(SB), X8
	VADDPS    X4, X8, X11
	VBLENDPS  $0x03, X4, X11, X11
	VSHUFPS   $0x88, X8, X11, X4
	VSHUFPS   $0xdd, X8, X11, X6
	VPSRLDQ   $0x04, X6, X8
	ADDPS     X8, X6
	VUNPCKLPS X6, X4, X8
	VUNPCKHPS X6, X4, X11

	// end DCT8
	VPSRLDQ   $0x04, X8, X4
	VPSLLDQ   $0x0c, X11, X6
	VADDPS    X4, X6, X4
	VADDPS    X4, X8, X8
	VPSRLDQ   $0x04, X11, X6
	VADDPS    X6, X11, X11
	VUNPCKLPS X8, X10, X4
	VUNPCKHPS X8, X10, X6
	VUNPCKLPS X11, X1, X8
	VUNPCKHPS X11, X1, X10

	// end DCT16
	// DCT16
	PSHUFD $0x1b, X9, X9
	VADDPS X9, X7, X1
	VSUBPS X9, X7, X11
	DIVPS  dct16<>+16(SB), X11
	PSHUFD $0x1b, X0, X9
	VADDPS X9, X5, X0
	VSUBPS X9, X5, X9
	DIVPS  dct16<>+0(SB), X9

	// DCT8
	PSHUFD $0x1b, X1, X5
	VADDPS X5, X0, X7
	VSUBPS X5, X0, X0
	DIVPS  dct8<>+0(SB), X0

	// DCT4
	PSHUFD     $0xb4, X7, X7
	PSHUFD     $0xb4, X0, X0
	VPUNPCKLDQ X0, X7, X1
	VPUNPCKHDQ X0, X7, X5
	VADDPS     X5, X1, X7
	VSUBPS     X5, X1, X0
	DIVPS      dct4<>+0(SB), X0
	VPUNPCKLDQ X0, X7, X1
	VPUNPCKHDQ X0, X7, X5

	// DCT2
	PSHUFD    $0xd8, X1, X7
	PSHUFD    $0xd8, X5, X0
	VADDPS    X0, X7, X1
	VSUBPS    X0, X7, X0
	DIVPS     dct2<>+0(SB), X0
	VADDPS    X1, X0, X7
	VBLENDPS  $0x03, X1, X7, X7
	VSHUFPS   $0x88, X0, X7, X1
	VSHUFPS   $0xdd, X0, X7, X5
	VPSRLDQ   $0x04, X5, X0
	ADDPS     X0, X5
	VUNPCKLPS X5, X1, X0
	VUNPCKHPS X5, X1, X1

	// end DCT8
	// DCT8
	PSHUFD $0x1b, X11, X7
	VADDPS X7, X9, X11
	VSUBPS X7, X9, X9
	DIVPS  dct8<>+0(SB), X9

	// DCT4
	PSHUFD     $0xb4, X11, X11
	PSHUFD     $0xb4, X9, X9
	VPUNPCKLDQ X9, X11, X5
	VPUNPCKHDQ X9, X11, X7
	VADDPS     X7, X5, X11
	VSUBPS     X7, X5, X9
	DIVPS      dct4<>+0(SB), X9
	VPUNPCKLDQ X9, X11, X5
	VPUNPCKHDQ X9, X11, X7

	// DCT2
	PSHUFD    $0xd8, X5, X11
	PSHUFD    $0xd8, X7, X9
	VADDPS    X9, X11, X5
	VSUBPS    X9, X11, X9
	DIVPS     dct2<>+0(SB), X9
	VADDPS    X5, X9, X11
	VBLENDPS  $0x03, X5, X11, X11
	VSHUFPS   $0x88, X9, X11, X5
	VSHUFPS   $0xdd, X9, X11, X7
	VPSRLDQ   $0x04, X7, X9
	ADDPS     X9, X7
	VUNPCKLPS X7, X5, X9
	VUNPCKHPS X7, X5, X11

	// end DCT8
	VPSRLDQ   $0x04, X9, X5
	VPSLLDQ   $0x0c, X11, X7
	VADDPS    X5, X7, X5
	VADDPS    X5, X9, X9
	VPSRLDQ   $0x04, X11, X7
	VADDPS    X7, X11, X11
	VUNPCKLPS X9, X0, X5
	VUNPCKHPS X9, X0, X7
	VUNPCKLPS X11, X1, X9
	VUNPCKHPS X11, X1, X0

	// end DCT16
	VPSRLDQ   $0x04, X5, X1
	VPSLLDQ   $0x0c, X7, X11
	VADDPS    X1, X11, X1
	VADDPS    X1, X5, X5
	VUNPCKLPS X5, X4, X1
	MOVAPS    X1, 128(AX)(BX*4)
	VUNPCKHPS X5, X4, X11
	MOVAPS    X11, 144(AX)(BX*4)
	VPSRLDQ   $0x04, X7, X1
	VPSLLDQ   $0x0c, X9, X11
	VADDPS    X1, X11, X1
	VADDPS    X1, X7, X7
	VUNPCKLPS X7, X6, X1
	MOVAPS    X1, 160(AX)(BX*4)
	VUNPCKHPS X7, X6, X11
	MOVAPS    X11, 176(AX)(BX*4)
	VPSRLDQ   $0x04, X9, X1
	VPSLLDQ   $0x0c, X0, X11
	VADDPS    X1, X11, X1
	VADDPS    X1, X9, X9
	VUNPCKLPS X9, X8, X1
	MOVAPS    X1, 192(AX)(BX*4)
	VUNPCKHPS X9, X8, X11
	MOVAPS    X11, 208(AX)(BX*4)
	VPSRLDQ   $0x04, X0, X11
	VADDPS    X11, X0, X0
	VUNPCKLPS X0, X10, X1
	MOVAPS    X1, 224(AX)(BX*4)
	VUNPCKHPS X0, X10, X11
	MOVAPS    X11, 240(AX)(BX*4)

	// end DCT32
	VZEROUPPER
	VMOVUPS    (AX)(BX*4), Y0
	VMOVUPS    32(AX)(BX*4), Y1
	VMOVUPS    64(AX)(BX*4), Y4
	VMOVUPS    96(AX)(BX*4), Y5
	MOVL       252(AX)(BX*4), SI
	VMOVUPS    128(AX)(BX*4), Y10
	VADDPS     132(AX)(BX*4), Y10, Y6
	VUNPCKLPS  Y6, Y0, Y10
	VUNPCKHPS  Y6, Y0, Y6
	VPERM2F128 $0x02, Y10, Y6, Y0
	VMOVUPS    Y0, (AX)(BX*4)
	VPERM2F128 $0x13, Y10, Y6, Y0
	VMOVUPS    Y0, 32(AX)(BX*4)
	VMOVUPS    160(AX)(BX*4), Y10
	VADDPS     164(AX)(BX*4), Y10, Y6
	VUNPCKLPS  Y6, Y1, Y10
	VUNPCKHPS  Y6, Y1, Y6
	VPERM2F128 $0x02, Y10, Y6, Y0
	VMOVUPS    Y0, 64(AX)(BX*4)
	VPERM2F128 $0x13, Y10, Y6, Y0
	VMOVUPS    Y0, 96(AX)(BX*4)
	VMOVUPS    192(AX)(BX*4), Y10
	VADDPS     196(AX)(BX*4), Y10, Y6
	VUNPCKLPS  Y6, Y4, Y10
	VUNPCKHPS  Y6, Y4, Y6
	VPERM2F128 $0x02, Y10, Y6, Y0
	VMOVUPS    Y0, 128(AX)(BX*4)
	VPERM2F128 $0x13, Y10, Y6, Y0
	VMOVUPS    Y0, 160(AX)(BX*4)
	VMOVUPS    224(AX)(BX*4), Y10
	VADDPS     228(AX)(BX*4), Y10, Y6
	VUNPCKLPS  Y6, Y5, Y10
	VUNPCKHPS  Y6, Y5, Y6
	VPERM2F128 $0x02, Y10, Y6, Y0
	VMOVUPS    Y0, 192(AX)(BX*4)
	VPERM2F128 $0x13, Y10, Y6, Y0
	VMOVUPS    Y0, 224(AX)(BX*4)
	MOVL       SI, 252(AX)(BX*4)
	VZEROUPPER

	// End innerloop instructions
	INCL DX
	JMP  j

i:
	CMPL CX, $0x08
	JE   done

	// Start innerloop instructions
	// --Loop load DCT64 values
	MOVL         CX, 256(SP)
	VZEROUPPER
	VPMOVZXBD    perm<>+0(SB), Y0
	VPBROADCASTD 256(SP), Y1
	VPADDD       gather<>+0(SB), Y1, Y1
	VPCMPEQD     Y4, Y4, Y4
	VPGATHERDD   Y4, (AX)(Y1*4), Y2
	VPCMPEQD     Y4, Y4, Y4
	VPGATHERDD   Y4, 14336(AX)(Y1*4), Y3
	VPERMD       Y3, Y0, Y3
	VADDPS       Y3, Y2, Y4
	VMOVUPS      Y4, (SP)
	VSUBPS       Y3, Y2, Y4
	VDIVPS       dct64<>+0(SB), Y4, Y4
	VMOVUPS      Y4, 128(SP)
	VPCMPEQD     Y4, Y4, Y4
	VPGATHERDD   Y4, 2048(AX)(Y1*4), Y2
	VPCMPEQD     Y4, Y4, Y4
	VPGATHERDD   Y4, 12288(AX)(Y1*4), Y3
	VPERMD       Y3, Y0, Y3
	VADDPS       Y3, Y2, Y4
	VMOVUPS      Y4, 32(SP)
	VSUBPS       Y3, Y2, Y4
	VDIVPS       dct64<>+32(SB), Y4, Y4
	VMOVUPS      Y4, 160(SP)
	VPCMPEQD     Y4, Y4, Y4
	VPGATHERDD   Y4, 4096(AX)(Y1*4), Y2
	VPCMPEQD     Y4, Y4, Y4
	VPGATHERDD   Y4, 10240(AX)(Y1*4), Y3
	VPERMD       Y3, Y0, Y3
	VADDPS       Y3, Y2, Y4
	VMOVUPS      Y4, 64(SP)
	VSUBPS       Y3, Y2, Y4
	VDIVPS       dct64<>+64(SB), Y4, Y4
	VMOVUPS      Y4, 192(SP)
	VPCMPEQD     Y4, Y4, Y4
	VPGATHERDD   Y4, 6144(AX)(Y1*4), Y2
	VPCMPEQD     Y4, Y4, Y4
	VPGATHERDD   Y4, 8192(AX)(Y1*4), Y3
	VPERMD       Y3, Y0, Y3
	VADDPS       Y3, Y2, Y4
	VMOVUPS      Y4, 96(SP)
	VSUBPS       Y3, Y2, Y4
	VDIVPS       dct64<>+96(SB), Y4, Y4
	VMOVUPS      Y4, 224(SP)

	// --Loop load DCT64 values
	VZEROUPPER

	// DCT32 Unaligned
	MOVUPS (SP), X0
	MOVUPS 112(SP), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X4
	VSUBPS X1, X0, X5
	DIVPS  dct32<>+0(SB), X5
	MOVUPS 16(SP), X0
	MOVUPS 96(SP), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X6
	VSUBPS X1, X0, X7
	DIVPS  dct32<>+16(SB), X7
	MOVUPS 32(SP), X0
	MOVUPS 80(SP), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X8
	VSUBPS X1, X0, X9
	DIVPS  dct32<>+32(SB), X9
	MOVUPS 48(SP), X0
	MOVUPS 64(SP), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X10
	VSUBPS X1, X0, X0
	DIVPS  dct32<>+48(SB), X0

	// DCT16
	PSHUFD $0x1b, X8, X8
	VADDPS X8, X6, X1
	VSUBPS X8, X6, X11
	DIVPS  dct16<>+16(SB), X11
	PSHUFD $0x1b, X10, X8
	VADDPS X8, X4, X10
	VSUBPS X8, X4, X8
	DIVPS  dct16<>+0(SB), X8

	// DCT8
	PSHUFD $0x1b, X1, X4
	VADDPS X4, X10, X6
	VSUBPS X4, X10, X10
	DIVPS  dct8<>+0(SB), X10

	// DCT4
	PSHUFD     $0xb4, X6, X6
	PSHUFD     $0xb4, X10, X10
	VPUNPCKLDQ X10, X6, X1
	VPUNPCKHDQ X10, X6, X4
	VADDPS     X4, X1, X6
	VSUBPS     X4, X1, X10
	DIVPS      dct4<>+0(SB), X10
	VPUNPCKLDQ X10, X6, X1
	VPUNPCKHDQ X10, X6, X4

	// DCT2
	PSHUFD    $0xd8, X1, X6
	PSHUFD    $0xd8, X4, X10
	VADDPS    X10, X6, X1
	VSUBPS    X10, X6, X10
	DIVPS     dct2<>+0(SB), X10
	VADDPS    X1, X10, X6
	VBLENDPS  $0x03, X1, X6, X6
	VSHUFPS   $0x88, X10, X6, X1
	VSHUFPS   $0xdd, X10, X6, X4
	VPSRLDQ   $0x04, X4, X10
	ADDPS     X10, X4
	VUNPCKLPS X4, X1, X10
	VUNPCKHPS X4, X1, X1

	// end DCT8
	// DCT8
	PSHUFD $0x1b, X11, X6
	VADDPS X6, X8, X11
	VSUBPS X6, X8, X8
	DIVPS  dct8<>+0(SB), X8

	// DCT4
	PSHUFD     $0xb4, X11, X11
	PSHUFD     $0xb4, X8, X8
	VPUNPCKLDQ X8, X11, X4
	VPUNPCKHDQ X8, X11, X6
	VADDPS     X6, X4, X11
	VSUBPS     X6, X4, X8
	DIVPS      dct4<>+0(SB), X8
	VPUNPCKLDQ X8, X11, X4
	VPUNPCKHDQ X8, X11, X6

	// DCT2
	PSHUFD    $0xd8, X4, X11
	PSHUFD    $0xd8, X6, X8
	VADDPS    X8, X11, X4
	VSUBPS    X8, X11, X8
	DIVPS     dct2<>+0(SB), X8
	VADDPS    X4, X8, X11
	VBLENDPS  $0x03, X4, X11, X11
	VSHUFPS   $0x88, X8, X11, X4
	VSHUFPS   $0xdd, X8, X11, X6
	VPSRLDQ   $0x04, X6, X8
	ADDPS     X8, X6
	VUNPCKLPS X6, X4, X8
	VUNPCKHPS X6, X4, X11

	// end DCT8
	VPSRLDQ   $0x04, X8, X4
	VPSLLDQ   $0x0c, X11, X6
	VADDPS    X4, X6, X4
	VADDPS    X4, X8, X8
	VPSRLDQ   $0x04, X11, X6
	VADDPS    X6, X11, X11
	VUNPCKLPS X8, X10, X4
	VUNPCKHPS X8, X10, X6
	VUNPCKLPS X11, X1, X8
	VUNPCKHPS X11, X1, X10

	// end DCT16
	// DCT16
	PSHUFD $0x1b, X9, X8
	VADDPS X8, X7, X1
	VSUBPS X8, X7, X6
	DIVPS  dct16<>+16(SB), X6
	PSHUFD $0x1b, X0, X8
	VADDPS X8, X5, X0
	VSUBPS X8, X5, X8
	DIVPS  dct16<>+0(SB), X8

	// DCT8
	PSHUFD $0x1b, X1, X5
	VADDPS X5, X0, X7
	VSUBPS X5, X0, X0
	DIVPS  dct8<>+0(SB), X0

	// DCT4
	PSHUFD     $0xb4, X7, X7
	PSHUFD     $0xb4, X0, X0
	VPUNPCKLDQ X0, X7, X1
	VPUNPCKHDQ X0, X7, X5
	VADDPS     X5, X1, X7
	VSUBPS     X5, X1, X0
	DIVPS      dct4<>+0(SB), X0
	VPUNPCKLDQ X0, X7, X1
	VPUNPCKHDQ X0, X7, X5

	// DCT2
	PSHUFD    $0xd8, X1, X7
	PSHUFD    $0xd8, X5, X0
	VADDPS    X0, X7, X1
	VSUBPS    X0, X7, X0
	DIVPS     dct2<>+0(SB), X0
	VADDPS    X1, X0, X7
	VBLENDPS  $0x03, X1, X7, X7
	VSHUFPS   $0x88, X0, X7, X1
	VSHUFPS   $0xdd, X0, X7, X5
	VPSRLDQ   $0x04, X5, X0
	ADDPS     X0, X5
	VUNPCKLPS X5, X1, X0
	VUNPCKHPS X5, X1, X1

	// end DCT8
	// DCT8
	PSHUFD $0x1b, X6, X6
	VADDPS X6, X8, X7
	VSUBPS X6, X8, X8
	DIVPS  dct8<>+0(SB), X8

	// DCT4
	PSHUFD     $0xb4, X7, X7
	PSHUFD     $0xb4, X8, X8
	VPUNPCKLDQ X8, X7, X5
	VPUNPCKHDQ X8, X7, X6
	VADDPS     X6, X5, X7
	VSUBPS     X6, X5, X8
	DIVPS      dct4<>+0(SB), X8
	VPUNPCKLDQ X8, X7, X5
	VPUNPCKHDQ X8, X7, X6

	// DCT2
	PSHUFD    $0xd8, X5, X7
	PSHUFD    $0xd8, X6, X8
	VADDPS    X8, X7, X5
	VSUBPS    X8, X7, X8
	DIVPS     dct2<>+0(SB), X8
	VADDPS    X5, X8, X7
	VBLENDPS  $0x03, X5, X7, X7
	VSHUFPS   $0x88, X8, X7, X5
	VSHUFPS   $0xdd, X8, X7, X6
	VPSRLDQ   $0x04, X6, X8
	ADDPS     X8, X6
	VUNPCKLPS X6, X5, X8
	VUNPCKHPS X6, X5, X6

	// end DCT8
	VPSRLDQ   $0x04, X8, X5
	VPSLLDQ   $0x0c, X6, X7
	VADDPS    X5, X7, X5
	VADDPS    X5, X8, X8
	VPSRLDQ   $0x04, X6, X7
	VADDPS    X7, X6, X6
	VUNPCKLPS X8, X0, X5
	VUNPCKHPS X8, X0, X7
	VUNPCKLPS X6, X1, X9
	VUNPCKHPS X6, X1, X0

	// end DCT16
	VPSRLDQ   $0x04, X5, X0
	VPSLLDQ   $0x0c, X7, X1
	VADDPS    X0, X1, X0
	VADDPS    X0, X5, X5
	VUNPCKLPS X5, X4, X0
	MOVUPS    X0, (SP)
	VUNPCKHPS X5, X4, X0
	MOVUPS    X0, 16(SP)

	// end DCT32 Unaligned
	// DCT32 Unaligned
	MOVUPS 128(SP), X0
	MOVUPS 240(SP), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X4
	VSUBPS X1, X0, X5
	DIVPS  dct32<>+0(SB), X5
	MOVUPS 144(SP), X0
	MOVUPS 224(SP), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X6
	VSUBPS X1, X0, X7
	DIVPS  dct32<>+16(SB), X7
	MOVUPS 160(SP), X0
	MOVUPS 208(SP), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X8
	VSUBPS X1, X0, X9
	DIVPS  dct32<>+32(SB), X9
	MOVUPS 176(SP), X0
	MOVUPS 192(SP), X1
	PSHUFD $0x1b, X1, X1
	VADDPS X1, X0, X10
	VSUBPS X1, X0, X0
	DIVPS  dct32<>+48(SB), X0

	// DCT16
	PSHUFD $0x1b, X8, X8
	VADDPS X8, X6, X1
	VSUBPS X8, X6, X11
	DIVPS  dct16<>+16(SB), X11
	PSHUFD $0x1b, X10, X8
	VADDPS X8, X4, X10
	VSUBPS X8, X4, X8
	DIVPS  dct16<>+0(SB), X8

	// DCT8
	PSHUFD $0x1b, X1, X4
	VADDPS X4, X10, X6
	VSUBPS X4, X10, X10
	DIVPS  dct8<>+0(SB), X10

	// DCT4
	PSHUFD     $0xb4, X6, X6
	PSHUFD     $0xb4, X10, X10
	VPUNPCKLDQ X10, X6, X1
	VPUNPCKHDQ X10, X6, X4
	VADDPS     X4, X1, X6
	VSUBPS     X4, X1, X10
	DIVPS      dct4<>+0(SB), X10
	VPUNPCKLDQ X10, X6, X1
	VPUNPCKHDQ X10, X6, X4

	// DCT2
	PSHUFD    $0xd8, X1, X6
	PSHUFD    $0xd8, X4, X10
	VADDPS    X10, X6, X1
	VSUBPS    X10, X6, X10
	DIVPS     dct2<>+0(SB), X10
	VADDPS    X1, X10, X6
	VBLENDPS  $0x03, X1, X6, X6
	VSHUFPS   $0x88, X10, X6, X1
	VSHUFPS   $0xdd, X10, X6, X4
	VPSRLDQ   $0x04, X4, X10
	ADDPS     X10, X4
	VUNPCKLPS X4, X1, X10
	VUNPCKHPS X4, X1, X1

	// end DCT8
	// DCT8
	PSHUFD $0x1b, X11, X6
	VADDPS X6, X8, X11
	VSUBPS X6, X8, X8
	DIVPS  dct8<>+0(SB), X8

	// DCT4
	PSHUFD     $0xb4, X11, X11
	PSHUFD     $0xb4, X8, X8
	VPUNPCKLDQ X8, X11, X4
	VPUNPCKHDQ X8, X11, X6
	VADDPS     X6, X4, X11
	VSUBPS     X6, X4, X8
	DIVPS      dct4<>+0(SB), X8
	VPUNPCKLDQ X8, X11, X4
	VPUNPCKHDQ X8, X11, X6

	// DCT2
	PSHUFD    $0xd8, X4, X11
	PSHUFD    $0xd8, X6, X8
	VADDPS    X8, X11, X4
	VSUBPS    X8, X11, X8
	DIVPS     dct2<>+0(SB), X8
	VADDPS    X4, X8, X11
	VBLENDPS  $0x03, X4, X11, X11
	VSHUFPS   $0x88, X8, X11, X4
	VSHUFPS   $0xdd, X8, X11, X6
	VPSRLDQ   $0x04, X6, X8
	ADDPS     X8, X6
	VUNPCKLPS X6, X4, X8
	VUNPCKHPS X6, X4, X11

	// end DCT8
	VPSRLDQ   $0x04, X8, X4
	VPSLLDQ   $0x0c, X11, X6
	VADDPS    X4, X6, X4
	VADDPS    X4, X8, X8
	VPSRLDQ   $0x04, X11, X6
	VADDPS    X6, X11, X11
	VUNPCKLPS X8, X10, X4
	VUNPCKHPS X8, X10, X6
	VUNPCKLPS X11, X1, X8
	VUNPCKHPS X11, X1, X10

	// end DCT16
	// DCT16
	PSHUFD $0x1b, X9, X8
	VADDPS X8, X7, X1
	VSUBPS X8, X7, X6
	DIVPS  dct16<>+16(SB), X6
	PSHUFD $0x1b, X0, X8
	VADDPS X8, X5, X0
	VSUBPS X8, X5, X8
	DIVPS  dct16<>+0(SB), X8

	// DCT8
	PSHUFD $0x1b, X1, X5
	VADDPS X5, X0, X7
	VSUBPS X5, X0, X0
	DIVPS  dct8<>+0(SB), X0

	// DCT4
	PSHUFD     $0xb4, X7, X7
	PSHUFD     $0xb4, X0, X0
	VPUNPCKLDQ X0, X7, X1
	VPUNPCKHDQ X0, X7, X5
	VADDPS     X5, X1, X7
	VSUBPS     X5, X1, X0
	DIVPS      dct4<>+0(SB), X0
	VPUNPCKLDQ X0, X7, X1
	VPUNPCKHDQ X0, X7, X5

	// DCT2
	PSHUFD    $0xd8, X1, X7
	PSHUFD    $0xd8, X5, X0
	VADDPS    X0, X7, X1
	VSUBPS    X0, X7, X0
	DIVPS     dct2<>+0(SB), X0
	VADDPS    X1, X0, X7
	VBLENDPS  $0x03, X1, X7, X7
	VSHUFPS   $0x88, X0, X7, X1
	VSHUFPS   $0xdd, X0, X7, X5
	VPSRLDQ   $0x04, X5, X0
	ADDPS     X0, X5
	VUNPCKLPS X5, X1, X0
	VUNPCKHPS X5, X1, X1

	// end DCT8
	// DCT8
	PSHUFD $0x1b, X6, X6
	VADDPS X6, X8, X7
	VSUBPS X6, X8, X8
	DIVPS  dct8<>+0(SB), X8

	// DCT4
	PSHUFD     $0xb4, X7, X7
	PSHUFD     $0xb4, X8, X8
	VPUNPCKLDQ X8, X7, X5
	VPUNPCKHDQ X8, X7, X6
	VADDPS     X6, X5, X7
	VSUBPS     X6, X5, X8
	DIVPS      dct4<>+0(SB), X8
	VPUNPCKLDQ X8, X7, X5
	VPUNPCKHDQ X8, X7, X6

	// DCT2
	PSHUFD    $0xd8, X5, X7
	PSHUFD    $0xd8, X6, X8
	VADDPS    X8, X7, X5
	VSUBPS    X8, X7, X8
	DIVPS     dct2<>+0(SB), X8
	VADDPS    X5, X8, X7
	VBLENDPS  $0x03, X5, X7, X7
	VSHUFPS   $0x88, X8, X7, X5
	VSHUFPS   $0xdd, X8, X7, X6
	VPSRLDQ   $0x04, X6, X8
	ADDPS     X8, X6
	VUNPCKLPS X6, X5, X8
	VUNPCKHPS X6, X5, X6

	// end DCT8
	VPSRLDQ   $0x04, X8, X5
	VPSLLDQ   $0x0c, X6, X7
	VADDPS    X5, X7, X5
	VADDPS    X5, X8, X8
	VPSRLDQ   $0x04, X6, X7
	VADDPS    X7, X6, X6
	VUNPCKLPS X8, X0, X5
	VUNPCKHPS X8, X0, X7
	VUNPCKLPS X6, X1, X9
	VUNPCKHPS X6, X1, X0

	// end DCT16
	VPSRLDQ   $0x04, X5, X0
	VPSLLDQ   $0x0c, X7, X1
	VADDPS    X0, X1, X0
	VADDPS    X0, X5, X5
	VUNPCKLPS X5, X4, X0
	MOVUPS    X0, 128(SP)
	VUNPCKHPS X5, X4, X0
	MOVUPS    X0, 144(SP)

	// end DCT32 Unaligned
	MOVUPS (SP), X0
	PEXTRD $0x00, X0, ret_0+24(FP)(CX*4)
	PEXTRD $0x01, X0, ret_0+88(FP)(CX*4)
	PEXTRD $0x02, X0, ret_0+152(FP)(CX*4)
	PEXTRD $0x03, X0, ret_0+216(FP)(CX*4)
	MOVUPS 128(SP), X0
	MOVUPS 132(SP), X1
	ADDPS  X0, X1
	PEXTRD $0x00, X1, ret_0+56(FP)(CX*4)
	PEXTRD $0x01, X1, ret_0+120(FP)(CX*4)
	PEXTRD $0x02, X1, ret_0+184(FP)(CX*4)
	PEXTRD $0x03, X1, ret_0+248(FP)(CX*4)

	// End innerloop instructions
	INCL CX
	JMP  i

done:
	RET

// func asmForwardDCT64(input []float32)
// Requires: AVX, AVX2, SSE, SSE2
TEXT ·asmForwardDCT64(SB), NOSPLIT, $0-24
	MOVQ      input_base+0(FP), AX
	VZEROUPPER
	VPMOVZXBD perm<>+0(SB), Y7
	VMOVAPS   (AX), Y0
	VMOVAPS   32(AX), Y1
	VMOVAPS   64(AX), Y2
	VMOVAPS   96(AX), Y3
	VPERMD    128(AX), Y7, Y4
	VPERMD    160(AX), Y7, Y5
	VPERMD    192(AX), Y7, Y6
	VPERMD    224(AX), Y7, Y7
	VADDPS    Y7, Y0, Y8
	VMOVAPS   Y8, (AX)
	VSUBPS    Y7, Y0, Y8
	VDIVPS    dct64<>+0(SB), Y8, Y8
	VMOVAPS   Y8, 128(AX)
	VADDPS    Y6, Y1, Y8
	VMOVAPS   Y8, 32(AX)
	VSUBPS    Y6, Y1, Y8
	VDIVPS    dct64<>+32(SB), Y8, Y8
	VMOVAPS   Y8, 160(AX)
	VADDPS    Y5, Y2, Y8
	VMOVAPS   Y8, 64(AX)
	VSUBPS    Y5, Y2, Y8
	VDIVPS    dct64<>+64(SB), Y8, Y8
	VMOVAPS   Y8, 192(AX)
	VADDPS    Y4, Y3, Y8
	VMOVAPS   Y8, 96(AX)
	VSUBPS    Y4, Y3, Y8
	VDIVPS    dct64<>+96(SB), Y8, Y8
	VMOVAPS   Y8, 224(AX)
	VZEROALL

	// DCT32
	MOVAPS (AX), X0
	PSHUFD $0x1b, 112(AX), X1
	VADDPS X1, X0, X2
	VSUBPS X1, X0, X3
	DIVPS  dct32<>+0(SB), X3
	MOVAPS 16(AX), X0
	PSHUFD $0x1b, 96(AX), X1
	VADDPS X1, X0, X4
	VSUBPS X1, X0, X5
	DIVPS  dct32<>+16(SB), X5
	MOVAPS 32(AX), X0
	PSHUFD $0x1b, 80(AX), X1
	VADDPS X1, X0, X6
	VSUBPS X1, X0, X7
	DIVPS  dct32<>+32(SB), X7
	MOVAPS 48(AX), X0
	PSHUFD $0x1b, 64(AX), X1
	VADDPS X1, X0, X8
	VSUBPS X1, X0, X0
	DIVPS  dct32<>+48(SB), X0

	// DCT16
	PSHUFD $0x1b, X6, X6
	VADDPS X6, X4, X1
	VSUBPS X6, X4, X9
	DIVPS  dct16<>+16(SB), X9
	PSHUFD $0x1b, X8, X6
	VADDPS X6, X2, X8
	VSUBPS X6, X2, X6
	DIVPS  dct16<>+0(SB), X6

	// DCT8
	PSHUFD $0x1b, X1, X2
	VADDPS X2, X8, X4
	VSUBPS X2, X8, X8
	DIVPS  dct8<>+0(SB), X8

	// DCT4
	PSHUFD     $0xb4, X4, X4
	PSHUFD     $0xb4, X8, X8
	VPUNPCKLDQ X8, X4, X1
	VPUNPCKHDQ X8, X4, X2
	VADDPS     X2, X1, X4
	VSUBPS     X2, X1, X8
	DIVPS      dct4<>+0(SB), X8
	VPUNPCKLDQ X8, X4, X1
	VPUNPCKHDQ X8, X4, X2

	// DCT2
	PSHUFD    $0xd8, X1, X4
	PSHUFD    $0xd8, X2, X8
	VADDPS    X8, X4, X1
	VSUBPS    X8, X4, X8
	DIVPS     dct2<>+0(SB), X8
	VADDPS    X1, X8, X4
	VBLENDPS  $0x03, X1, X4, X4
	VSHUFPS   $0x88, X8, X4, X1
	VSHUFPS   $0xdd, X8, X4, X2
	VPSRLDQ   $0x04, X2, X8
	ADDPS     X8, X2
	VUNPCKLPS X2, X1, X8
	VUNPCKHPS X2, X1, X1

	// end DCT8
	// DCT8
	PSHUFD $0x1b, X9, X4
	VADDPS X4, X6, X9
	VSUBPS X4, X6, X6
	DIVPS  dct8<>+0(SB), X6

	// DCT4
	PSHUFD     $0xb4, X9, X9
	PSHUFD     $0xb4, X6, X6
	VPUNPCKLDQ X6, X9, X2
	VPUNPCKHDQ X6, X9, X4
	VADDPS     X4, X2, X9
	VSUBPS     X4, X2, X6
	DIVPS      dct4<>+0(SB), X6
	VPUNPCKLDQ X6, X9, X2
	VPUNPCKHDQ X6, X9, X4

	// DCT2
	PSHUFD    $0xd8, X2, X9
	PSHUFD    $0xd8, X4, X6
	VADDPS    X6, X9, X2
	VSUBPS    X6, X9, X6
	DIVPS     dct2<>+0(SB), X6
	VADDPS    X2, X6, X9
	VBLENDPS  $0x03, X2, X9, X9
	VSHUFPS   $0x88, X6, X9, X2
	VSHUFPS   $0xdd, X6, X9, X4
	VPSRLDQ   $0x04, X4, X6
	ADDPS     X6, X4
	VUNPCKLPS X4, X2, X6
	VUNPCKHPS X4, X2, X9

	// end DCT8
	VPSRLDQ   $0x04, X6, X2
	VPSLLDQ   $0x0c, X9, X4
	VADDPS    X2, X4, X2
	VADDPS    X2, X6, X6
	VPSRLDQ   $0x04, X9, X4
	VADDPS    X4, X9, X9
	VUNPCKLPS X6, X8, X2
	VUNPCKHPS X6, X8, X4
	VUNPCKLPS X9, X1, X6
	VUNPCKHPS X9, X1, X8

	// end DCT16
	// DCT16
	PSHUFD $0x1b, X7, X7
	VADDPS X7, X5, X1
	VSUBPS X7, X5, X9
	DIVPS  dct16<>+16(SB), X9
	PSHUFD $0x1b, X0, X7
	VADDPS X7, X3, X0
	VSUBPS X7, X3, X7
	DIVPS  dct16<>+0(SB), X7

	// DCT8
	PSHUFD $0x1b, X1, X3
	VADDPS X3, X0, X5
	VSUBPS X3, X0, X0
	DIVPS  dct8<>+0(SB), X0

	// DCT4
	PSHUFD     $0xb4, X5, X5
	PSHUFD     $0xb4, X0, X0
	VPUNPCKLDQ X0, X5, X1
	VPUNPCKHDQ X0, X5, X3
	VADDPS     X3, X1, X5
	VSUBPS     X3, X1, X0
	DIVPS      dct4<>+0(SB), X0
	VPUNPCKLDQ X0, X5, X1
	VPUNPCKHDQ X0, X5, X3

	// DCT2
	PSHUFD    $0xd8, X1, X5
	PSHUFD    $0xd8, X3, X0
	VADDPS    X0, X5, X1
	VSUBPS    X0, X5, X0
	DIVPS     dct2<>+0(SB), X0
	VADDPS    X1, X0, X5
	VBLENDPS  $0x03, X1, X5, X5
	VSHUFPS   $0x88, X0, X5, X1
	VSHUFPS   $0xdd, X0, X5, X3
	VPSRLDQ   $0x04, X3, X0
	ADDPS     X0, X3
	VUNPCKLPS X3, X1, X0
	VUNPCKHPS X3, X1, X1

	// end DCT8
	// DCT8
	PSHUFD $0x1b, X9, X5
	VADDPS X5, X7, X9
	VSUBPS X5, X7, X7
	DIVPS  dct8<>+0(SB), X7

	// DCT4
	PSHUFD     $0xb4, X9, X9
	PSHUFD     $0xb4, X7, X7
	VPUNPCKLDQ X7, X9, X3
	VPUNPCKHDQ X7, X9, X5
	VADDPS     X5, X3, X9
	VSUBPS     X5, X3, X7
	DIVPS      dct4<>+0(SB), X7
	VPUNPCKLDQ X7, X9, X3
	VPUNPCKHDQ X7, X9, X5

	// DCT2
	PSHUFD    $0xd8, X3, X9
	PSHUFD    $0xd8, X5, X7
	VADDPS    X7, X9, X3
	VSUBPS    X7, X9, X7
	DIVPS     dct2<>+0(SB), X7
	VADDPS    X3, X7, X9
	VBLENDPS  $0x03, X3, X9, X9
	VSHUFPS   $0x88, X7, X9, X3
	VSHUFPS   $0xdd, X7, X9, X5
	VPSRLDQ   $0x04, X5, X7
	ADDPS     X7, X5
	VUNPCKLPS X5, X3, X7
	VUNPCKHPS X5, X3, X9

	// end DCT8
	VPSRLDQ   $0x04, X7, X3
	VPSLLDQ   $0x0c, X9, X5
	VADDPS    X3, X5, X3
	VADDPS    X3, X7, X7
	VPSRLDQ   $0x04, X9, X5
	VADDPS    X5, X9, X9
	VUNPCKLPS X7, X0, X3
	VUNPCKHPS X7, X0, X5
	VUNPCKLPS X9, X1, X7
	VUNPCKHPS X9, X1, X0

	// end DCT16
	VPSRLDQ   $0x04, X3, X1
	VPSLLDQ   $0x0c, X5, X9
	VADDPS    X1, X9, X1
	VADDPS    X1, X3, X3
	VUNPCKLPS X3, X2, X1
	MOVAPS    X1, (AX)
	VUNPCKHPS X3, X2, X9
	MOVAPS    X9, 16(AX)
	VPSRLDQ   $0x04, X5, X1
	VPSLLDQ   $0x0c, X7, X9
	VADDPS    X1, X9, X1
	VADDPS    X1, X5, X5
	VUNPCKLPS X5, X4, X1
	MOVAPS    X1, 32(AX)
	VUNPCKHPS X5, X4, X9
	MOVAPS    X9, 48(AX)
	VPSRLDQ   $0x04, X7, X1
	VPSLLDQ   $0x0c, X0, X9
	VADDPS    X1, X9, X1
	VADDPS    X1, X7, X7
	VUNPCKLPS X7, X6, X1
	MOVAPS    X1, 64(AX)
	VUNPCKHPS X7, X6, X9
	MOVAPS    X9, 80(AX)
	VPSRLDQ   $0x04, X0, X9
	VADDPS    X9, X0, X0
	VUNPCKLPS X0, X8, X1
	MOVAPS    X1, 96(AX)
	VUNPCKHPS X0, X8, X9
	MOVAPS    X9, 112(AX)

	// end DCT32
	// DCT32
	MOVAPS 128(AX), X0
	PSHUFD $0x1b, 240(AX), X1
	VADDPS X1, X0, X2
	VSUBPS X1, X0, X3
	DIVPS  dct32<>+0(SB), X3
	MOVAPS 144(AX), X0
	PSHUFD $0x1b, 224(AX), X1
	VADDPS X1, X0, X4
	VSUBPS X1, X0, X5
	DIVPS  dct32<>+16(SB), X5
	MOVAPS 160(AX), X0
	PSHUFD $0x1b, 208(AX), X1
	VADDPS X1, X0, X6
	VSUBPS X1, X0, X7
	DIVPS  dct32<>+32(SB), X7
	MOVAPS 176(AX), X0
	PSHUFD $0x1b, 192(AX), X1
	VADDPS X1, X0, X8
	VSUBPS X1, X0, X0
	DIVPS  dct32<>+48(SB), X0

	// DCT16
	PSHUFD $0x1b, X6, X6
	VADDPS X6, X4, X1
	VSUBPS X6, X4, X9
	DIVPS  dct16<>+16(SB), X9
	PSHUFD $0x1b, X8, X6
	VADDPS X6, X2, X8
	VSUBPS X6, X2, X6
	DIVPS  dct16<>+0(SB), X6

	// DCT8
	PSHUFD $0x1b, X1, X2
	VADDPS X2, X8, X4
	VSUBPS X2, X8, X8
	DIVPS  dct8<>+0(SB), X8

	// DCT4
	PSHUFD     $0xb4, X4, X4
	PSHUFD     $0xb4, X8, X8
	VPUNPCKLDQ X8, X4, X1
	VPUNPCKHDQ X8, X4, X2
	VADDPS     X2, X1, X4
	VSUBPS     X2, X1, X8
	DIVPS      dct4<>+0(SB), X8
	VPUNPCKLDQ X8, X4, X1
	VPUNPCKHDQ X8, X4, X2

	// DCT2
	PSHUFD    $0xd8, X1, X4
	PSHUFD    $0xd8, X2, X8
	VADDPS    X8, X4, X1
	VSUBPS    X8, X4, X8
	DIVPS     dct2<>+0(SB), X8
	VADDPS    X1, X8, X4
	VBLENDPS  $0x03, X1, X4, X4
	VSHUFPS   $0x88, X8, X4, X1
	VSHUFPS   $0xdd, X8, X4, X2
	VPSRLDQ   $0x04, X2, X8
	ADDPS     X8, X2
	VUNPCKLPS X2, X1, X8
	VUNPCKHPS X2, X1, X1

	// end DCT8
	// DCT8
	PSHUFD $0x1b, X9, X4
	VADDPS X4, X6, X9
	VSUBPS X4, X6, X6
	DIVPS  dct8<>+0(SB), X6

	// DCT4
	PSHUFD     $0xb4, X9, X9
	PSHUFD     $0xb4, X6, X6
	VPUNPCKLDQ X6, X9, X2
	VPUNPCKHDQ X6, X9, X4
	VADDPS     X4, X2, X9
	VSUBPS     X4, X2, X6
	DIVPS      dct4<>+0(SB), X6
	VPUNPCKLDQ X6, X9, X2
	VPUNPCKHDQ X6, X9, X4

	// DCT2
	PSHUFD    $0xd8, X2, X9
	PSHUFD    $0xd8, X4, X6
	VADDPS    X6, X9, X2
	VSUBPS    X6, X9, X6
	DIVPS     dct2<>+0(SB), X6
	VADDPS    X2, X6, X9
	VBLENDPS  $0x03, X2, X9, X9
	VSHUFPS   $0x88, X6, X9, X2
	VSHUFPS   $0xdd, X6, X9, X4
	VPSRLDQ   $0x04, X4, X6
	ADDPS     X6, X4
	VUNPCKLPS X4, X2, X6
	VUNPCKHPS X4, X2, X9

	// end DCT8
	VPSRLDQ   $0x04, X6, X2
	VPSLLDQ   $0x0c, X9, X4
	VADDPS    X2, X4, X2
	VADDPS    X2, X6, X6
	VPSRLDQ   $0x04, X9, X4
	VADDPS    X4, X9, X9
	VUNPCKLPS X6, X8, X2
	VUNPCKHPS X6, X8, X4
	VUNPCKLPS X9, X1, X6
	VUNPCKHPS X9, X1, X8

	// end DCT16
	// DCT16
	PSHUFD $0x1b, X7, X7
	VADDPS X7, X5, X1
	VSUBPS X7, X5, X9
	DIVPS  dct16<>+16(SB), X9
	PSHUFD $0x1b, X0, X7
	VADDPS X7, X3, X0
	VSUBPS X7, X3, X7
	DIVPS  dct16<>+0(SB), X7

	// DCT8
	PSHUFD $0x1b, X1, X3
	VADDPS X3, X0, X5
	VSUBPS X3, X0, X0
	DIVPS  dct8<>+0(SB), X0

	// DCT4
	PSHUFD     $0xb4, X5, X5
	PSHUFD     $0xb4, X0, X0
	VPUNPCKLDQ X0, X5, X1
	VPUNPCKHDQ X0, X5, X3
	VADDPS     X3, X1, X5
	VSUBPS     X3, X1, X0
	DIVPS      dct4<>+0(SB), X0
	VPUNPCKLDQ X0, X5, X1
	VPUNPCKHDQ X0, X5, X3

	// DCT2
	PSHUFD    $0xd8, X1, X5
	PSHUFD    $0xd8, X3, X0
	VADDPS    X0, X5, X1
	VSUBPS    X0, X5, X0
	DIVPS     dct2<>+0(SB), X0
	VADDPS    X1, X0, X5
	VBLENDPS  $0x03, X1, X5, X5
	VSHUFPS   $0x88, X0, X5, X1
	VSHUFPS   $0xdd, X0, X5, X3
	VPSRLDQ   $0x04, X3, X0
	ADDPS     X0, X3
	VUNPCKLPS X3, X1, X0
	VUNPCKHPS X3, X1, X1

	// end DCT8
	// DCT8
	PSHUFD $0x1b, X9, X5
	VADDPS X5, X7, X9
	VSUBPS X5, X7, X7
	DIVPS  dct8<>+0(SB), X7

	// DCT4
	PSHUFD     $0xb4, X9, X9
	PSHUFD     $0xb4, X7, X7
	VPUNPCKLDQ X7, X9, X3
	VPUNPCKHDQ X7, X9, X5
	VADDPS     X5, X3, X9
	VSUBPS     X5, X3, X7
	DIVPS      dct4<>+0(SB), X7
	VPUNPCKLDQ X7, X9, X3
	VPUNPCKHDQ X7, X9, X5

	// DCT2
	PSHUFD    $0xd8, X3, X9
	PSHUFD    $0xd8, X5, X7
	VADDPS    X7, X9, X3
	VSUBPS    X7, X9, X7
	DIVPS     dct2<>+0(SB), X7
	VADDPS    X3, X7, X9
	VBLENDPS  $0x03, X3, X9, X9
	VSHUFPS   $0x88, X7, X9, X3
	VSHUFPS   $0xdd, X7, X9, X5
	VPSRLDQ   $0x04, X5, X7
	ADDPS     X7, X5
	VUNPCKLPS X5, X3, X7
	VUNPCKHPS X5, X3, X9

	// end DCT8
	VPSRLDQ   $0x04, X7, X3
	VPSLLDQ   $0x0c, X9, X5
	VADDPS    X3, X5, X3
	VADDPS    X3, X7, X7
	VPSRLDQ   $0x04, X9, X5
	VADDPS    X5, X9, X9
	VUNPCKLPS X7, X0, X3
	VUNPCKHPS X7, X0, X5
	VUNPCKLPS X9, X1, X7
	VUNPCKHPS X9, X1, X0

	// end DCT16
	VPSRLDQ   $0x04, X3, X1
	VPSLLDQ   $0x0c, X5, X9
	VADDPS    X1, X9, X1
	VADDPS    X1, X3, X3
	VUNPCKLPS X3, X2, X1
	MOVAPS    X1, 128(AX)
	VUNPCKHPS X3, X2, X9
	MOVAPS    X9, 144(AX)
	VPSRLDQ   $0x04, X5, X1
	VPSLLDQ   $0x0c, X7, X9
	VADDPS    X1, X9, X1
	VADDPS    X1, X5, X5
	VUNPCKLPS X5, X4, X1
	MOVAPS    X1, 160(AX)
	VUNPCKHPS X5, X4, X9
	MOVAPS    X9, 176(AX)
	VPSRLDQ   $0x04, X7, X1
	VPSLLDQ   $0x0c, X0, X9
	VADDPS    X1, X9, X1
	VADDPS    X1, X7, X7
	VUNPCKLPS X7, X6, X1
	MOVAPS    X1, 192(AX)
	VUNPCKHPS X7, X6, X9
	MOVAPS    X9, 208(AX)
	VPSRLDQ   $0x04, X0, X9
	VADDPS    X9, X0, X0
	VUNPCKLPS X0, X8, X1
	MOVAPS    X1, 224(AX)
	VUNPCKHPS X0, X8, X9
	MOVAPS    X9, 240(AX)

	// end DCT32
	VZEROUPPER
	MOVL       252(AX), CX
	VMOVUPS    (AX), Y0
	VMOVUPS    32(AX), Y1
	VMOVUPS    64(AX), Y2
	VMOVUPS    96(AX), Y3
	VMOVUPS    128(AX), Y8
	VADDPS     132(AX), Y8, Y4
	VUNPCKLPS  Y4, Y0, Y8
	VUNPCKHPS  Y4, Y0, Y4
	VPERM2F128 $0x02, Y8, Y4, Y0
	VMOVUPS    Y0, (AX)
	VPERM2F128 $0x13, Y8, Y4, Y0
	VMOVUPS    Y0, 32(AX)
	VMOVUPS    160(AX), Y8
	VADDPS     164(AX), Y8, Y4
	VUNPCKLPS  Y4, Y1, Y8
	VUNPCKHPS  Y4, Y1, Y4
	VPERM2F128 $0x02, Y8, Y4, Y0
	VMOVUPS    Y0, 64(AX)
	VPERM2F128 $0x13, Y8, Y4, Y0
	VMOVUPS    Y0, 96(AX)
	VMOVUPS    192(AX), Y8
	VADDPS     196(AX), Y8, Y4
	VUNPCKLPS  Y4, Y2, Y8
	VUNPCKHPS  Y4, Y2, Y4
	VPERM2F128 $0x02, Y8, Y4, Y0
	VMOVUPS    Y0, 128(AX)
	VPERM2F128 $0x13, Y8, Y4, Y0
	VMOVUPS    Y0, 160(AX)
	VMOVUPS    224(AX), Y8
	VADDPS     228(AX), Y8, Y4
	VUNPCKLPS  Y4, Y3, Y8
	VUNPCKHPS  Y4, Y3, Y4
	VPERM2F128 $0x02, Y8, Y4, Y0
	VMOVUPS    Y0, 192(AX)
	VPERM2F128 $0x13, Y8, Y4, Y0
	VMOVUPS    Y0, 224(AX)
	MOVL       CX, 252(AX)
	VZEROUPPER
	RET

// func AsmYCbCrToGray8(pixels []float32, minX int, minY int, maxX int, maxY int, sY []uint8, sCb []uint8, sCr []uint8, yStride int, cStride int) uint64
// Requires: AVX, AVX2
TEXT ·AsmYCbCrToGray8(SB), NOSPLIT, $0-152
	VPBROADCASTD pg8<>+0(SB), Y0
	VPBROADCASTD pg8<>+4(SB), Y1
	VPBROADCASTD pg8<>+8(SB), Y2
	VPBROADCASTD pg8<>+12(SB), Y3
	VPBROADCASTD pg8<>+16(SB), Y4
	VPBROADCASTD pg8<>+20(SB), Y5
	VPBROADCASTD pg8<>+24(SB), Y6
	VPBROADCASTD pg8<>+28(SB), Y6
	VPBROADCASTD pg8<>+32(SB), Y7
	VPBROADCASTD pg8<>+36(SB), Y8
	MOVQ         yStride+128(FP), AX
	MOVQ         cStride+136(FP), BX
	MOVQ         maxY+48(FP), R8
	MOVQ         maxX+40(FP), R9
	MOVQ         sY_base+56(FP), R10
	MOVQ         sCb_base+80(FP), R11
	MOVQ         sCr_base+104(FP), R12
	MOVQ         pixels_base+0(FP), R13
	XORQ         CX, CX
	XORQ         R14, R14
	XORQ         R15, R15

y:
	CMPQ  R14, R8
	JE    done
	MOVQ  AX, CX
	IMULQ R14, CX
	MOVQ  BX, SI
	IMULQ R14, SI

x:
	CMPQ R15, R9
	JE   xDone

	// Start innerloop instructions
	MOVQ      CX, DX
	ADDQ      R15, DX
	MOVQ      SI, DI
	ADDQ      R15, DI
	VPMOVZXBD (R10)(DX*1), Y9
	VPMOVZXBD (R11)(DI*1), Y10
	VPMOVZXBD (R12)(DI*1), Y11
	VPMULLD   Y1, Y9, Y9
	VPSUBD    Y0, Y10, Y10
	VPSUBD    Y0, Y11, Y11
	VPMULLD   Y2, Y11, Y12
	VPADDD    Y9, Y12, Y12
	VPSRAD    $0x08, Y12, Y12
	VCVTDQ2PS Y12, Y12
	VMULPS    Y6, Y12, Y12
	VPMULLD   Y3, Y11, Y11
	VPMULLD   Y4, Y10, Y13
	VPSUBQ    Y13, Y9, Y13
	VPSUBQ    Y11, Y13, Y13
	VPSRAD    $0x08, Y13, Y13
	VCVTDQ2PS Y13, Y13
	VMULPS    Y7, Y13, Y13
	VPMULLD   Y5, Y10, Y10
	VPADDD    Y9, Y10, Y10
	VPSRAD    $0x08, Y10, Y10
	VCVTDQ2PS Y10, Y10
	VMULPS    Y8, Y10, Y10
	VADDPS    Y12, Y10, Y10
	VADDPS    Y13, Y10, Y10
	VMOVAPS   Y10, (R13)(DX*4)

	// End innerloop instructions
	ADDQ $0x08, R15
	JMP  x

xDone:
	XORQ R15, R15
	INCQ R14
	JMP  y

done:
	RET
