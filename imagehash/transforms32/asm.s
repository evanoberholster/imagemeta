// Code generated by command: go run asm.go -out asm.s -stubs stub.go. DO NOT EDIT.

#include "textflag.h"

DATA dct64<>+0(SB)/4, $(1.9993976)
DATA dct64<>+4(SB)/4, $(1.9945809)
DATA dct64<>+8(SB)/4, $(1.9849591)
DATA dct64<>+12(SB)/4, $(1.9705553)
DATA dct64<>+16(SB)/4, $(1.9514042)
DATA dct64<>+20(SB)/4, $(1.9275521)
DATA dct64<>+24(SB)/4, $(1.8990563)
DATA dct64<>+28(SB)/4, $(1.8659856)
DATA dct64<>+32(SB)/4, $(1.8284196)
DATA dct64<>+36(SB)/4, $(1.7864486)
DATA dct64<>+40(SB)/4, $(1.7401739)
DATA dct64<>+44(SB)/4, $(1.6897072)
DATA dct64<>+48(SB)/4, $(1.6351696)
DATA dct64<>+52(SB)/4, $(1.5766928)
DATA dct64<>+56(SB)/4, $(1.5144176)
DATA dct64<>+60(SB)/4, $(1.4484942)
DATA dct64<>+64(SB)/4, $(1.3790811)
DATA dct64<>+68(SB)/4, $(1.3063457)
DATA dct64<>+72(SB)/4, $(1.2304631)
DATA dct64<>+76(SB)/4, $(1.1516163)
DATA dct64<>+80(SB)/4, $(1.0699953)
DATA dct64<>+84(SB)/4, $(0.9857964)
DATA dct64<>+88(SB)/4, $(0.8992227)
DATA dct64<>+92(SB)/4, $(0.8104826)
DATA dct64<>+96(SB)/4, $(0.7197901)
DATA dct64<>+100(SB)/4, $(0.6273635)
DATA dct64<>+104(SB)/4, $(0.5334255)
DATA dct64<>+108(SB)/4, $(0.43820247)
DATA dct64<>+112(SB)/4, $(0.34192377)
DATA dct64<>+116(SB)/4, $(0.24482135)
DATA dct64<>+120(SB)/4, $(0.14712913)
DATA dct64<>+124(SB)/4, $(0.049082458)
GLOBL dct64<>(SB), RODATA|NOPTR, $128

DATA dct32<>+0(SB)/4, $(1.9975909)
DATA dct32<>+4(SB)/4, $(1.978353)
DATA dct32<>+8(SB)/4, $(1.9400625)
DATA dct32<>+12(SB)/4, $(1.8830881)
DATA dct32<>+16(SB)/4, $(1.8079786)
DATA dct32<>+20(SB)/4, $(1.7154572)
DATA dct32<>+24(SB)/4, $(1.606415)
DATA dct32<>+28(SB)/4, $(1.4819022)
DATA dct32<>+32(SB)/4, $(1.343118)
DATA dct32<>+36(SB)/4, $(1.1913986)
DATA dct32<>+40(SB)/4, $(1.0282055)
DATA dct32<>+44(SB)/4, $(0.85511017)
DATA dct32<>+48(SB)/4, $(0.6737797)
DATA dct32<>+52(SB)/4, $(0.48596036)
DATA dct32<>+56(SB)/4, $(0.29346094)
DATA dct32<>+60(SB)/4, $(0.09813535)
GLOBL dct32<>(SB), RODATA|NOPTR, $64

DATA dct16<>+0(SB)/4, $(1.9903694)
DATA dct16<>+4(SB)/4, $(1.9138807)
DATA dct16<>+8(SB)/4, $(1.7638426)
DATA dct16<>+12(SB)/4, $(1.5460209)
DATA dct16<>+16(SB)/4, $(1.2687865)
DATA dct16<>+20(SB)/4, $(0.9427935)
DATA dct16<>+24(SB)/4, $(0.5805693)
DATA dct16<>+28(SB)/4, $(0.19603428)
GLOBL dct16<>(SB), RODATA|NOPTR, $32

DATA dct8<>+0(SB)/4, $(1.9615705)
DATA dct8<>+4(SB)/4, $(1.6629392)
DATA dct8<>+8(SB)/4, $(1.1111405)
DATA dct8<>+12(SB)/4, $(0.39018065)
GLOBL dct8<>(SB), RODATA|NOPTR, $16

DATA dct4<>+0(SB)/4, $(1.847759)
DATA dct4<>+4(SB)/4, $(1.847759)
DATA dct4<>+8(SB)/4, $(0.76536685)
DATA dct4<>+12(SB)/4, $(0.76536685)
GLOBL dct4<>(SB), RODATA|NOPTR, $16

DATA dct2<>+0(SB)/4, $(1.4142135)
DATA dct2<>+4(SB)/4, $(1.4142135)
DATA dct2<>+8(SB)/4, $(1.4142135)
DATA dct2<>+12(SB)/4, $(1.4142135)
GLOBL dct2<>(SB), RODATA|NOPTR, $16

// func asmForwardDCT64(input []float32)
// Requires: AVX, SSE, SSE2
TEXT ·asmForwardDCT64(SB), NOSPLIT, $0-24
	MOVQ   input_base+0(FP), AX
	VZEROUPPER
	MOVAPS (AX), X0
	MOVAPS 16(AX), X1
	MOVAPS 32(AX), X2
	MOVAPS 48(AX), X3
	PSHUFD $0x1b, 240(AX), X8
	PSHUFD $0x1b, 224(AX), X9
	PSHUFD $0x1b, 208(AX), X10
	PSHUFD $0x1b, 192(AX), X4
	VADDPS X0, X8, X6
	VADDPS X1, X9, X11
	VADDPS X2, X10, X5
	VADDPS X3, X4, X7
	MOVAPS X6, (AX)
	MOVAPS X11, 16(AX)
	MOVAPS X5, 32(AX)
	MOVAPS X7, 48(AX)
	VSUBPS X8, X0, X8
	VSUBPS X9, X1, X9
	VSUBPS X10, X2, X10
	VSUBPS X4, X3, X13
	DIVPS  dct64<>+0(SB), X8
	DIVPS  dct64<>+16(SB), X9
	DIVPS  dct64<>+32(SB), X10
	DIVPS  dct64<>+48(SB), X13
	MOVAPS 64(AX), X4
	MOVAPS 80(AX), X5
	MOVAPS 96(AX), X6
	MOVAPS 112(AX), X7
	PSHUFD $0x1b, 176(AX), X0
	PSHUFD $0x1b, 160(AX), X1
	PSHUFD $0x1b, 144(AX), X14
	PSHUFD $0x1b, 128(AX), X15
	VADDPS X4, X0, X2
	VADDPS X5, X1, X3
	VADDPS X6, X14, X11
	VADDPS X7, X15, X12
	MOVAPS X2, 64(AX)
	MOVAPS X3, 80(AX)
	MOVAPS X11, 96(AX)
	MOVAPS X12, 112(AX)
	MOVAPS X8, 128(AX)
	MOVAPS X9, 144(AX)
	MOVAPS X10, 160(AX)
	MOVAPS X13, 176(AX)
	VSUBPS X0, X4, X9
	VSUBPS X1, X5, X4
	VSUBPS X14, X6, X5
	VSUBPS X15, X7, X2
	DIVPS  dct64<>+64(SB), X9
	DIVPS  dct64<>+80(SB), X4
	DIVPS  dct64<>+96(SB), X5
	DIVPS  dct64<>+112(SB), X2
	MOVAPS X9, 192(AX)
	MOVAPS X4, 208(AX)
	MOVAPS X5, 224(AX)
	MOVAPS X2, 240(AX)

	// DCT32
	MOVAPS (AX), X0
	MOVAPS 16(AX), X1
	MOVAPS 32(AX), X2
	MOVAPS 48(AX), X3
	PSHUFD $0x1b, 64(AX), X8
	PSHUFD $0x1b, 80(AX), X9
	PSHUFD $0x1b, 96(AX), X10
	PSHUFD $0x1b, 112(AX), X4
	VADDPS X4, X0, X5
	VADDPS X10, X1, X12
	VADDPS X9, X2, X13
	VADDPS X8, X3, X7
	VSUBPS X4, X0, X4
	VSUBPS X10, X1, X10
	VSUBPS X9, X2, X14
	VSUBPS X8, X3, X15
	DIVPS  dct32<>+0(SB), X4
	DIVPS  dct32<>+16(SB), X10
	DIVPS  dct32<>+32(SB), X14
	DIVPS  dct32<>+48(SB), X15

	// DCT16
	PSHUFD $0x1b, X13, X9
	PSHUFD $0x1b, X7, X8
	VADDPS X8, X5, X6
	VADDPS X9, X12, X11
	VSUBPS X8, X5, X8
	VSUBPS X9, X12, X9
	DIVPS  dct16<>+0(SB), X8
	DIVPS  dct16<>+16(SB), X9

	// DCT8
	PSHUFD $0x1b, X11, X1
	VADDPS X1, X6, X2
	VSUBPS X1, X6, X3
	DIVPS  dct8<>+0(SB), X3

	// DCT4
	PSHUFD     $0xb4, X2, X2
	PSHUFD     $0xb4, X3, X3
	VPUNPCKLDQ X3, X2, X0
	VPUNPCKHDQ X3, X2, X1
	VADDPS     X1, X0, X2
	VSUBPS     X1, X0, X3
	DIVPS      dct4<>+0(SB), X3
	VPUNPCKLDQ X3, X2, X0
	VPUNPCKHDQ X3, X2, X1

	// DCT2
	PSHUFD    $0xd8, X0, X2
	PSHUFD    $0xd8, X1, X3
	VADDPS    X3, X2, X0
	VSUBPS    X3, X2, X3
	DIVPS     dct2<>+0(SB), X3
	VADDPS    X0, X3, X2
	VBLENDPS  $0x03, X0, X2, X2
	VSHUFPS   $0x88, X3, X2, X0
	VSHUFPS   $0xdd, X3, X2, X1
	VPSRLDQ   $0x04, X1, X3
	ADDPS     X3, X1
	VUNPCKLPS X1, X0, X6
	VUNPCKHPS X1, X0, X11

	// end DCT8
	// DCT8
	PSHUFD $0x1b, X9, X1
	VADDPS X1, X8, X2
	VSUBPS X1, X8, X3
	DIVPS  dct8<>+0(SB), X3

	// DCT4
	PSHUFD     $0xb4, X2, X2
	PSHUFD     $0xb4, X3, X3
	VPUNPCKLDQ X3, X2, X0
	VPUNPCKHDQ X3, X2, X1
	VADDPS     X1, X0, X2
	VSUBPS     X1, X0, X3
	DIVPS      dct4<>+0(SB), X3
	VPUNPCKLDQ X3, X2, X0
	VPUNPCKHDQ X3, X2, X1

	// DCT2
	PSHUFD    $0xd8, X0, X2
	PSHUFD    $0xd8, X1, X3
	VADDPS    X3, X2, X0
	VSUBPS    X3, X2, X3
	DIVPS     dct2<>+0(SB), X3
	VADDPS    X0, X3, X2
	VBLENDPS  $0x03, X0, X2, X2
	VSHUFPS   $0x88, X3, X2, X0
	VSHUFPS   $0xdd, X3, X2, X1
	VPSRLDQ   $0x04, X1, X3
	ADDPS     X3, X1
	VUNPCKLPS X1, X0, X8
	VUNPCKHPS X1, X0, X9

	// end DCT8
	VPSRLDQ   $0x04, X8, X5
	VPSLLDQ   $0x0c, X9, X12
	VADDPS    X5, X12, X5
	VADDPS    X5, X8, X8
	VPSRLDQ   $0x04, X9, X12
	VADDPS    X12, X9, X9
	VUNPCKLPS X8, X6, X5
	VUNPCKHPS X8, X6, X12
	VUNPCKLPS X9, X11, X13
	VUNPCKHPS X9, X11, X7

	// end DCT16
	// DCT16
	PSHUFD $0x1b, X14, X9
	PSHUFD $0x1b, X15, X8
	VADDPS X8, X4, X6
	VADDPS X9, X10, X11
	VSUBPS X8, X4, X8
	VSUBPS X9, X10, X9
	DIVPS  dct16<>+0(SB), X8
	DIVPS  dct16<>+16(SB), X9

	// DCT8
	PSHUFD $0x1b, X11, X1
	VADDPS X1, X6, X2
	VSUBPS X1, X6, X3
	DIVPS  dct8<>+0(SB), X3

	// DCT4
	PSHUFD     $0xb4, X2, X2
	PSHUFD     $0xb4, X3, X3
	VPUNPCKLDQ X3, X2, X0
	VPUNPCKHDQ X3, X2, X1
	VADDPS     X1, X0, X2
	VSUBPS     X1, X0, X3
	DIVPS      dct4<>+0(SB), X3
	VPUNPCKLDQ X3, X2, X0
	VPUNPCKHDQ X3, X2, X1

	// DCT2
	PSHUFD    $0xd8, X0, X2
	PSHUFD    $0xd8, X1, X3
	VADDPS    X3, X2, X0
	VSUBPS    X3, X2, X3
	DIVPS     dct2<>+0(SB), X3
	VADDPS    X0, X3, X2
	VBLENDPS  $0x03, X0, X2, X2
	VSHUFPS   $0x88, X3, X2, X0
	VSHUFPS   $0xdd, X3, X2, X1
	VPSRLDQ   $0x04, X1, X3
	ADDPS     X3, X1
	VUNPCKLPS X1, X0, X6
	VUNPCKHPS X1, X0, X11

	// end DCT8
	// DCT8
	PSHUFD $0x1b, X9, X1
	VADDPS X1, X8, X2
	VSUBPS X1, X8, X3
	DIVPS  dct8<>+0(SB), X3

	// DCT4
	PSHUFD     $0xb4, X2, X2
	PSHUFD     $0xb4, X3, X3
	VPUNPCKLDQ X3, X2, X0
	VPUNPCKHDQ X3, X2, X1
	VADDPS     X1, X0, X2
	VSUBPS     X1, X0, X3
	DIVPS      dct4<>+0(SB), X3
	VPUNPCKLDQ X3, X2, X0
	VPUNPCKHDQ X3, X2, X1

	// DCT2
	PSHUFD    $0xd8, X0, X2
	PSHUFD    $0xd8, X1, X3
	VADDPS    X3, X2, X0
	VSUBPS    X3, X2, X3
	DIVPS     dct2<>+0(SB), X3
	VADDPS    X0, X3, X2
	VBLENDPS  $0x03, X0, X2, X2
	VSHUFPS   $0x88, X3, X2, X0
	VSHUFPS   $0xdd, X3, X2, X1
	VPSRLDQ   $0x04, X1, X3
	ADDPS     X3, X1
	VUNPCKLPS X1, X0, X8
	VUNPCKHPS X1, X0, X9

	// end DCT8
	VPSRLDQ   $0x04, X8, X4
	VPSLLDQ   $0x0c, X9, X10
	VADDPS    X4, X10, X4
	VADDPS    X4, X8, X8
	VPSRLDQ   $0x04, X9, X10
	VADDPS    X10, X9, X9
	VUNPCKLPS X8, X6, X4
	VUNPCKHPS X8, X6, X10
	VUNPCKLPS X9, X11, X14
	VUNPCKHPS X9, X11, X15

	// end DCT16
	VPSRLDQ   $0x04, X4, X0
	VPSLLDQ   $0x0c, X10, X1
	VADDPS    X0, X1, X0
	VADDPS    X0, X4, X4
	VPSRLDQ   $0x04, X10, X0
	VPSLLDQ   $0x0c, X14, X1
	VADDPS    X0, X1, X0
	VADDPS    X0, X10, X10
	VPSRLDQ   $0x04, X14, X0
	VPSLLDQ   $0x0c, X15, X1
	VADDPS    X0, X1, X0
	VADDPS    X0, X14, X14
	VPSRLDQ   $0x04, X15, X1
	VADDPS    X1, X15, X15
	VUNPCKLPS X4, X5, X0
	VUNPCKHPS X4, X5, X1
	VUNPCKLPS X10, X12, X2
	VUNPCKHPS X10, X12, X3
	VUNPCKLPS X14, X13, X4
	VUNPCKHPS X14, X13, X5
	VUNPCKLPS X15, X7, X6
	VUNPCKHPS X15, X7, X7
	MOVAPS    X0, (AX)
	MOVAPS    X1, 16(AX)
	MOVAPS    X2, 32(AX)
	MOVAPS    X3, 48(AX)
	MOVAPS    X4, 64(AX)
	MOVAPS    X5, 80(AX)
	MOVAPS    X6, 96(AX)
	MOVAPS    X7, 112(AX)

	// end DCT32
	// DCT32
	MOVAPS 128(AX), X0
	MOVAPS 144(AX), X1
	MOVAPS 160(AX), X2
	MOVAPS 176(AX), X3
	PSHUFD $0x1b, 192(AX), X8
	PSHUFD $0x1b, 208(AX), X9
	PSHUFD $0x1b, 224(AX), X10
	PSHUFD $0x1b, 240(AX), X4
	VADDPS X4, X0, X5
	VADDPS X10, X1, X12
	VADDPS X9, X2, X13
	VADDPS X8, X3, X7
	VSUBPS X4, X0, X4
	VSUBPS X10, X1, X10
	VSUBPS X9, X2, X14
	VSUBPS X8, X3, X15
	DIVPS  dct32<>+0(SB), X4
	DIVPS  dct32<>+16(SB), X10
	DIVPS  dct32<>+32(SB), X14
	DIVPS  dct32<>+48(SB), X15

	// DCT16
	PSHUFD $0x1b, X13, X9
	PSHUFD $0x1b, X7, X8
	VADDPS X8, X5, X6
	VADDPS X9, X12, X11
	VSUBPS X8, X5, X8
	VSUBPS X9, X12, X9
	DIVPS  dct16<>+0(SB), X8
	DIVPS  dct16<>+16(SB), X9

	// DCT8
	PSHUFD $0x1b, X11, X1
	VADDPS X1, X6, X2
	VSUBPS X1, X6, X3
	DIVPS  dct8<>+0(SB), X3

	// DCT4
	PSHUFD     $0xb4, X2, X2
	PSHUFD     $0xb4, X3, X3
	VPUNPCKLDQ X3, X2, X0
	VPUNPCKHDQ X3, X2, X1
	VADDPS     X1, X0, X2
	VSUBPS     X1, X0, X3
	DIVPS      dct4<>+0(SB), X3
	VPUNPCKLDQ X3, X2, X0
	VPUNPCKHDQ X3, X2, X1

	// DCT2
	PSHUFD    $0xd8, X0, X2
	PSHUFD    $0xd8, X1, X3
	VADDPS    X3, X2, X0
	VSUBPS    X3, X2, X3
	DIVPS     dct2<>+0(SB), X3
	VADDPS    X0, X3, X2
	VBLENDPS  $0x03, X0, X2, X2
	VSHUFPS   $0x88, X3, X2, X0
	VSHUFPS   $0xdd, X3, X2, X1
	VPSRLDQ   $0x04, X1, X3
	ADDPS     X3, X1
	VUNPCKLPS X1, X0, X6
	VUNPCKHPS X1, X0, X11

	// end DCT8
	// DCT8
	PSHUFD $0x1b, X9, X1
	VADDPS X1, X8, X2
	VSUBPS X1, X8, X3
	DIVPS  dct8<>+0(SB), X3

	// DCT4
	PSHUFD     $0xb4, X2, X2
	PSHUFD     $0xb4, X3, X3
	VPUNPCKLDQ X3, X2, X0
	VPUNPCKHDQ X3, X2, X1
	VADDPS     X1, X0, X2
	VSUBPS     X1, X0, X3
	DIVPS      dct4<>+0(SB), X3
	VPUNPCKLDQ X3, X2, X0
	VPUNPCKHDQ X3, X2, X1

	// DCT2
	PSHUFD    $0xd8, X0, X2
	PSHUFD    $0xd8, X1, X3
	VADDPS    X3, X2, X0
	VSUBPS    X3, X2, X3
	DIVPS     dct2<>+0(SB), X3
	VADDPS    X0, X3, X2
	VBLENDPS  $0x03, X0, X2, X2
	VSHUFPS   $0x88, X3, X2, X0
	VSHUFPS   $0xdd, X3, X2, X1
	VPSRLDQ   $0x04, X1, X3
	ADDPS     X3, X1
	VUNPCKLPS X1, X0, X8
	VUNPCKHPS X1, X0, X9

	// end DCT8
	VPSRLDQ   $0x04, X8, X5
	VPSLLDQ   $0x0c, X9, X12
	VADDPS    X5, X12, X5
	VADDPS    X5, X8, X8
	VPSRLDQ   $0x04, X9, X12
	VADDPS    X12, X9, X9
	VUNPCKLPS X8, X6, X5
	VUNPCKHPS X8, X6, X12
	VUNPCKLPS X9, X11, X13
	VUNPCKHPS X9, X11, X7

	// end DCT16
	// DCT16
	PSHUFD $0x1b, X14, X9
	PSHUFD $0x1b, X15, X8
	VADDPS X8, X4, X6
	VADDPS X9, X10, X11
	VSUBPS X8, X4, X8
	VSUBPS X9, X10, X9
	DIVPS  dct16<>+0(SB), X8
	DIVPS  dct16<>+16(SB), X9

	// DCT8
	PSHUFD $0x1b, X11, X1
	VADDPS X1, X6, X2
	VSUBPS X1, X6, X3
	DIVPS  dct8<>+0(SB), X3

	// DCT4
	PSHUFD     $0xb4, X2, X2
	PSHUFD     $0xb4, X3, X3
	VPUNPCKLDQ X3, X2, X0
	VPUNPCKHDQ X3, X2, X1
	VADDPS     X1, X0, X2
	VSUBPS     X1, X0, X3
	DIVPS      dct4<>+0(SB), X3
	VPUNPCKLDQ X3, X2, X0
	VPUNPCKHDQ X3, X2, X1

	// DCT2
	PSHUFD    $0xd8, X0, X2
	PSHUFD    $0xd8, X1, X3
	VADDPS    X3, X2, X0
	VSUBPS    X3, X2, X3
	DIVPS     dct2<>+0(SB), X3
	VADDPS    X0, X3, X2
	VBLENDPS  $0x03, X0, X2, X2
	VSHUFPS   $0x88, X3, X2, X0
	VSHUFPS   $0xdd, X3, X2, X1
	VPSRLDQ   $0x04, X1, X3
	ADDPS     X3, X1
	VUNPCKLPS X1, X0, X6
	VUNPCKHPS X1, X0, X11

	// end DCT8
	// DCT8
	PSHUFD $0x1b, X9, X1
	VADDPS X1, X8, X2
	VSUBPS X1, X8, X3
	DIVPS  dct8<>+0(SB), X3

	// DCT4
	PSHUFD     $0xb4, X2, X2
	PSHUFD     $0xb4, X3, X3
	VPUNPCKLDQ X3, X2, X0
	VPUNPCKHDQ X3, X2, X1
	VADDPS     X1, X0, X2
	VSUBPS     X1, X0, X3
	DIVPS      dct4<>+0(SB), X3
	VPUNPCKLDQ X3, X2, X0
	VPUNPCKHDQ X3, X2, X1

	// DCT2
	PSHUFD    $0xd8, X0, X2
	PSHUFD    $0xd8, X1, X3
	VADDPS    X3, X2, X0
	VSUBPS    X3, X2, X3
	DIVPS     dct2<>+0(SB), X3
	VADDPS    X0, X3, X2
	VBLENDPS  $0x03, X0, X2, X2
	VSHUFPS   $0x88, X3, X2, X0
	VSHUFPS   $0xdd, X3, X2, X1
	VPSRLDQ   $0x04, X1, X3
	ADDPS     X3, X1
	VUNPCKLPS X1, X0, X8
	VUNPCKHPS X1, X0, X9

	// end DCT8
	VPSRLDQ   $0x04, X8, X4
	VPSLLDQ   $0x0c, X9, X10
	VADDPS    X4, X10, X4
	VADDPS    X4, X8, X8
	VPSRLDQ   $0x04, X9, X10
	VADDPS    X10, X9, X9
	VUNPCKLPS X8, X6, X4
	VUNPCKHPS X8, X6, X10
	VUNPCKLPS X9, X11, X14
	VUNPCKHPS X9, X11, X15

	// end DCT16
	VPSRLDQ   $0x04, X4, X0
	VPSLLDQ   $0x0c, X10, X1
	VADDPS    X0, X1, X0
	VADDPS    X0, X4, X4
	VPSRLDQ   $0x04, X10, X0
	VPSLLDQ   $0x0c, X14, X1
	VADDPS    X0, X1, X0
	VADDPS    X0, X10, X10
	VPSRLDQ   $0x04, X14, X0
	VPSLLDQ   $0x0c, X15, X1
	VADDPS    X0, X1, X0
	VADDPS    X0, X14, X14
	VPSRLDQ   $0x04, X15, X1
	VADDPS    X1, X15, X15
	VUNPCKLPS X4, X5, X0
	VUNPCKHPS X4, X5, X1
	VUNPCKLPS X10, X12, X2
	VUNPCKHPS X10, X12, X3
	VUNPCKLPS X14, X13, X4
	VUNPCKHPS X14, X13, X5
	VUNPCKLPS X15, X7, X6
	VUNPCKHPS X15, X7, X7
	MOVAPS    X0, 128(AX)
	MOVAPS    X1, 144(AX)
	MOVAPS    X2, 160(AX)
	MOVAPS    X3, 176(AX)
	MOVAPS    X4, 192(AX)
	MOVAPS    X5, 208(AX)
	MOVAPS    X6, 224(AX)
	MOVAPS    X7, 240(AX)

	// end DCT32
	MOVAPS    (AX), X6
	MOVAPS    16(AX), X11
	MOVAPS    32(AX), X5
	MOVAPS    48(AX), X7
	MOVAPS    128(AX), X8
	MOVAPS    144(AX), X9
	MOVAPS    160(AX), X10
	VPSRLDQ   $0x04, X8, X0
	VPSLLDQ   $0x0c, X9, X1
	VADDPS    X0, X1, X0
	VADDPS    X0, X8, X8
	VPSRLDQ   $0x04, X9, X0
	VPSLLDQ   $0x0c, X10, X1
	VADDPS    X0, X1, X0
	VADDPS    X0, X9, X9
	VUNPCKLPS X8, X6, X0
	VUNPCKHPS X8, X6, X1
	VUNPCKLPS X9, X11, X2
	VUNPCKHPS X9, X11, X3
	MOVAPS    X0, (AX)
	MOVAPS    X1, 16(AX)
	MOVAPS    X2, 32(AX)
	MOVAPS    X3, 48(AX)
	MOVAPS    176(AX), X13
	MOVAPS    192(AX), X9
	VPSRLDQ   $0x04, X10, X0
	VPSLLDQ   $0x0c, X13, X1
	VADDPS    X0, X1, X0
	VADDPS    X0, X10, X10
	VPSRLDQ   $0x04, X13, X0
	VPSLLDQ   $0x0c, X9, X1
	VADDPS    X0, X1, X0
	VADDPS    X0, X13, X13
	VUNPCKLPS X10, X5, X4
	VUNPCKHPS X10, X5, X5
	VUNPCKLPS X13, X7, X6
	VUNPCKHPS X13, X7, X7
	MOVAPS    64(AX), X2
	MOVAPS    80(AX), X3
	MOVAPS    96(AX), X11
	MOVAPS    112(AX), X12
	MOVAPS    X4, 64(AX)
	MOVAPS    X5, 80(AX)
	MOVAPS    X6, 96(AX)
	MOVAPS    X7, 112(AX)
	MOVAPS    208(AX), X4
	MOVAPS    224(AX), X5
	VPSRLDQ   $0x04, X9, X0
	VPSLLDQ   $0x0c, X4, X1
	VADDPS    X0, X1, X0
	VADDPS    X0, X9, X9
	VPSRLDQ   $0x04, X4, X0
	VPSLLDQ   $0x0c, X5, X1
	VADDPS    X0, X1, X0
	VADDPS    X0, X4, X4
	VUNPCKLPS X9, X2, X8
	VUNPCKHPS X9, X2, X9
	VUNPCKLPS X4, X3, X10
	VUNPCKHPS X4, X3, X4
	MOVAPS    X8, 128(AX)
	MOVAPS    X9, 144(AX)
	MOVAPS    X10, 160(AX)
	MOVAPS    X4, 176(AX)
	MOVAPS    240(AX), X2
	VPSRLDQ   $0x04, X5, X0
	VPSLLDQ   $0x0c, X2, X1
	VADDPS    X0, X1, X0
	VADDPS    X0, X5, X5
	VPSRLDQ   $0x04, X2, X1
	VADDPS    X1, X2, X2
	VUNPCKLPS X5, X11, X0
	VUNPCKHPS X5, X11, X1
	VUNPCKLPS X2, X12, X14
	VUNPCKHPS X2, X12, X15
	MOVAPS    X0, 192(AX)
	MOVAPS    X1, 208(AX)
	MOVAPS    X14, 224(AX)
	MOVAPS    X15, 240(AX)
	RET

// func asmForwardDCT32(input []float32)
// Requires: AVX, SSE, SSE2
TEXT ·asmForwardDCT32(SB), NOSPLIT, $0-24
	MOVQ input_base+0(FP), AX

	// DCT32
	MOVAPS (AX), X0
	MOVAPS 16(AX), X1
	MOVAPS 32(AX), X2
	MOVAPS 48(AX), X3
	PSHUFD $0x1b, 64(AX), X4
	PSHUFD $0x1b, 80(AX), X9
	PSHUFD $0x1b, 96(AX), X6
	PSHUFD $0x1b, 112(AX), X8
	VADDPS X8, X0, X10
	VADDPS X6, X1, X11
	VADDPS X9, X2, X5
	VADDPS X4, X3, X7
	VSUBPS X8, X0, X12
	VSUBPS X6, X1, X13
	VSUBPS X9, X2, X6
	VSUBPS X4, X3, X8
	DIVPS  dct32<>+0(SB), X12
	DIVPS  dct32<>+16(SB), X13
	DIVPS  dct32<>+32(SB), X6
	DIVPS  dct32<>+48(SB), X8

	// DCT16
	PSHUFD $0x1b, X5, X9
	PSHUFD $0x1b, X7, X4
	VADDPS X4, X10, X0
	VADDPS X9, X11, X1
	VSUBPS X4, X10, X2
	VSUBPS X9, X11, X3
	DIVPS  dct16<>+0(SB), X2
	DIVPS  dct16<>+16(SB), X3

	// DCT8
	PSHUFD $0x1b, X1, X4
	VADDPS X4, X0, X5
	VSUBPS X4, X0, X0
	DIVPS  dct8<>+0(SB), X0

	// DCT4
	PSHUFD     $0xb4, X5, X5
	PSHUFD     $0xb4, X0, X0
	VPUNPCKLDQ X0, X5, X1
	VPUNPCKHDQ X0, X5, X4
	VADDPS     X4, X1, X5
	VSUBPS     X4, X1, X0
	DIVPS      dct4<>+0(SB), X0
	VPUNPCKLDQ X0, X5, X1
	VPUNPCKHDQ X0, X5, X4

	// DCT2
	PSHUFD    $0xd8, X1, X5
	PSHUFD    $0xd8, X4, X0
	VADDPS    X0, X5, X1
	VSUBPS    X0, X5, X0
	DIVPS     dct2<>+0(SB), X0
	VADDPS    X1, X0, X5
	VBLENDPS  $0x03, X1, X5, X5
	VSHUFPS   $0x88, X0, X5, X1
	VSHUFPS   $0xdd, X0, X5, X4
	VPSRLDQ   $0x04, X4, X0
	ADDPS     X0, X4
	VUNPCKLPS X4, X1, X0
	VUNPCKHPS X4, X1, X1

	// end DCT8
	// DCT8
	PSHUFD $0x1b, X3, X4
	VADDPS X4, X2, X5
	VSUBPS X4, X2, X2
	DIVPS  dct8<>+0(SB), X2

	// DCT4
	PSHUFD     $0xb4, X5, X5
	PSHUFD     $0xb4, X2, X2
	VPUNPCKLDQ X2, X5, X3
	VPUNPCKHDQ X2, X5, X4
	VADDPS     X4, X3, X5
	VSUBPS     X4, X3, X2
	DIVPS      dct4<>+0(SB), X2
	VPUNPCKLDQ X2, X5, X3
	VPUNPCKHDQ X2, X5, X4

	// DCT2
	PSHUFD    $0xd8, X3, X5
	PSHUFD    $0xd8, X4, X2
	VADDPS    X2, X5, X3
	VSUBPS    X2, X5, X2
	DIVPS     dct2<>+0(SB), X2
	VADDPS    X3, X2, X5
	VBLENDPS  $0x03, X3, X5, X5
	VSHUFPS   $0x88, X2, X5, X3
	VSHUFPS   $0xdd, X2, X5, X4
	VPSRLDQ   $0x04, X4, X2
	ADDPS     X2, X4
	VUNPCKLPS X4, X3, X2
	VUNPCKHPS X4, X3, X3

	// end DCT8
	VPSRLDQ   $0x04, X2, X10
	VPSLLDQ   $0x0c, X3, X11
	VADDPS    X10, X11, X10
	VADDPS    X10, X2, X2
	VPSRLDQ   $0x04, X3, X11
	VADDPS    X11, X3, X3
	VUNPCKLPS X2, X0, X10
	VUNPCKHPS X2, X0, X11
	VUNPCKLPS X3, X1, X5
	VUNPCKHPS X3, X1, X7

	// end DCT16
	// DCT16
	PSHUFD $0x1b, X6, X9
	PSHUFD $0x1b, X8, X4
	VADDPS X4, X12, X0
	VADDPS X9, X13, X1
	VSUBPS X4, X12, X2
	VSUBPS X9, X13, X3
	DIVPS  dct16<>+0(SB), X2
	DIVPS  dct16<>+16(SB), X3

	// DCT8
	PSHUFD $0x1b, X1, X4
	VADDPS X4, X0, X6
	VSUBPS X4, X0, X0
	DIVPS  dct8<>+0(SB), X0

	// DCT4
	PSHUFD     $0xb4, X6, X6
	PSHUFD     $0xb4, X0, X0
	VPUNPCKLDQ X0, X6, X1
	VPUNPCKHDQ X0, X6, X4
	VADDPS     X4, X1, X6
	VSUBPS     X4, X1, X0
	DIVPS      dct4<>+0(SB), X0
	VPUNPCKLDQ X0, X6, X1
	VPUNPCKHDQ X0, X6, X4

	// DCT2
	PSHUFD    $0xd8, X1, X6
	PSHUFD    $0xd8, X4, X0
	VADDPS    X0, X6, X1
	VSUBPS    X0, X6, X0
	DIVPS     dct2<>+0(SB), X0
	VADDPS    X1, X0, X6
	VBLENDPS  $0x03, X1, X6, X6
	VSHUFPS   $0x88, X0, X6, X1
	VSHUFPS   $0xdd, X0, X6, X4
	VPSRLDQ   $0x04, X4, X0
	ADDPS     X0, X4
	VUNPCKLPS X4, X1, X0
	VUNPCKHPS X4, X1, X1

	// end DCT8
	// DCT8
	PSHUFD $0x1b, X3, X4
	VADDPS X4, X2, X6
	VSUBPS X4, X2, X2
	DIVPS  dct8<>+0(SB), X2

	// DCT4
	PSHUFD     $0xb4, X6, X6
	PSHUFD     $0xb4, X2, X2
	VPUNPCKLDQ X2, X6, X3
	VPUNPCKHDQ X2, X6, X4
	VADDPS     X4, X3, X6
	VSUBPS     X4, X3, X2
	DIVPS      dct4<>+0(SB), X2
	VPUNPCKLDQ X2, X6, X3
	VPUNPCKHDQ X2, X6, X4

	// DCT2
	PSHUFD    $0xd8, X3, X6
	PSHUFD    $0xd8, X4, X2
	VADDPS    X2, X6, X3
	VSUBPS    X2, X6, X2
	DIVPS     dct2<>+0(SB), X2
	VADDPS    X3, X2, X6
	VBLENDPS  $0x03, X3, X6, X6
	VSHUFPS   $0x88, X2, X6, X3
	VSHUFPS   $0xdd, X2, X6, X4
	VPSRLDQ   $0x04, X4, X2
	ADDPS     X2, X4
	VUNPCKLPS X4, X3, X2
	VUNPCKHPS X4, X3, X3

	// end DCT8
	VPSRLDQ   $0x04, X2, X12
	VPSLLDQ   $0x0c, X3, X13
	VADDPS    X12, X13, X12
	VADDPS    X12, X2, X2
	VPSRLDQ   $0x04, X3, X13
	VADDPS    X13, X3, X3
	VUNPCKLPS X2, X0, X12
	VUNPCKHPS X2, X0, X13
	VUNPCKLPS X3, X1, X6
	VUNPCKHPS X3, X1, X8

	// end DCT16
	VPSRLDQ   $0x04, X12, X0
	VPSLLDQ   $0x0c, X13, X1
	VADDPS    X0, X1, X0
	VADDPS    X0, X12, X12
	VPSRLDQ   $0x04, X13, X0
	VPSLLDQ   $0x0c, X6, X1
	VADDPS    X0, X1, X0
	VADDPS    X0, X13, X13
	VPSRLDQ   $0x04, X6, X0
	VPSLLDQ   $0x0c, X8, X1
	VADDPS    X0, X1, X0
	VADDPS    X0, X6, X6
	VPSRLDQ   $0x04, X8, X1
	VADDPS    X1, X8, X8
	VUNPCKLPS X12, X10, X0
	VUNPCKHPS X12, X10, X1
	VUNPCKLPS X13, X11, X2
	VUNPCKHPS X13, X11, X3
	VUNPCKLPS X6, X5, X4
	VUNPCKHPS X6, X5, X5
	VUNPCKLPS X8, X7, X6
	VUNPCKHPS X8, X7, X7
	MOVAPS    X0, (AX)
	MOVAPS    X1, 16(AX)
	MOVAPS    X2, 32(AX)
	MOVAPS    X3, 48(AX)
	MOVAPS    X4, 64(AX)
	MOVAPS    X5, 80(AX)
	MOVAPS    X6, 96(AX)
	MOVAPS    X7, 112(AX)

	// end DCT32
	RET

// func asmForwardDCT16(input []float32)
// Requires: AVX, SSE, SSE2
TEXT ·asmForwardDCT16(SB), NOSPLIT, $0-24
	MOVQ   input_base+0(FP), AX
	MOVAPS (AX), X0
	MOVAPS 16(AX), X1
	MOVAPS 32(AX), X2
	MOVAPS 48(AX), X3

	// DCT16
	PSHUFD $0x1b, X2, X2
	PSHUFD $0x1b, X3, X3
	VADDPS X3, X0, X4
	VADDPS X2, X1, X5
	VSUBPS X3, X0, X3
	VSUBPS X2, X1, X6
	DIVPS  dct16<>+0(SB), X3
	DIVPS  dct16<>+16(SB), X6

	// DCT8
	PSHUFD $0x1b, X5, X1
	VADDPS X1, X4, X2
	VSUBPS X1, X4, X4
	DIVPS  dct8<>+0(SB), X4

	// DCT4
	PSHUFD     $0xb4, X2, X2
	PSHUFD     $0xb4, X4, X4
	VPUNPCKLDQ X4, X2, X0
	VPUNPCKHDQ X4, X2, X1
	VADDPS     X1, X0, X2
	VSUBPS     X1, X0, X4
	DIVPS      dct4<>+0(SB), X4
	VPUNPCKLDQ X4, X2, X0
	VPUNPCKHDQ X4, X2, X1

	// DCT2
	PSHUFD    $0xd8, X0, X2
	PSHUFD    $0xd8, X1, X4
	VADDPS    X4, X2, X0
	VSUBPS    X4, X2, X4
	DIVPS     dct2<>+0(SB), X4
	VADDPS    X0, X4, X2
	VBLENDPS  $0x03, X0, X2, X2
	VSHUFPS   $0x88, X4, X2, X0
	VSHUFPS   $0xdd, X4, X2, X1
	VPSRLDQ   $0x04, X1, X4
	ADDPS     X4, X1
	VUNPCKLPS X1, X0, X4
	VUNPCKHPS X1, X0, X5

	// end DCT8
	// DCT8
	PSHUFD $0x1b, X6, X1
	VADDPS X1, X3, X2
	VSUBPS X1, X3, X3
	DIVPS  dct8<>+0(SB), X3

	// DCT4
	PSHUFD     $0xb4, X2, X2
	PSHUFD     $0xb4, X3, X3
	VPUNPCKLDQ X3, X2, X0
	VPUNPCKHDQ X3, X2, X1
	VADDPS     X1, X0, X2
	VSUBPS     X1, X0, X3
	DIVPS      dct4<>+0(SB), X3
	VPUNPCKLDQ X3, X2, X0
	VPUNPCKHDQ X3, X2, X1

	// DCT2
	PSHUFD    $0xd8, X0, X2
	PSHUFD    $0xd8, X1, X3
	VADDPS    X3, X2, X0
	VSUBPS    X3, X2, X3
	DIVPS     dct2<>+0(SB), X3
	VADDPS    X0, X3, X2
	VBLENDPS  $0x03, X0, X2, X2
	VSHUFPS   $0x88, X3, X2, X0
	VSHUFPS   $0xdd, X3, X2, X1
	VPSRLDQ   $0x04, X1, X3
	ADDPS     X3, X1
	VUNPCKLPS X1, X0, X3
	VUNPCKHPS X1, X0, X6

	// end DCT8
	VPSRLDQ   $0x04, X3, X0
	VPSLLDQ   $0x0c, X6, X1
	VADDPS    X0, X1, X0
	VADDPS    X0, X3, X3
	VPSRLDQ   $0x04, X6, X1
	VADDPS    X1, X6, X6
	VUNPCKLPS X3, X4, X0
	VUNPCKHPS X3, X4, X1
	VUNPCKLPS X6, X5, X2
	VUNPCKHPS X6, X5, X3

	// end DCT16
	MOVAPS X0, (AX)
	MOVAPS X1, 16(AX)
	MOVAPS X2, 32(AX)
	MOVAPS X3, 48(AX)
	RET

DATA xmm128<>+0(SB)/4, $+128
DATA xmm128<>+4(SB)/4, $+65793
DATA xmm128<>+8(SB)/4, $+91881
DATA xmm128<>+12(SB)/4, $+46802
DATA xmm128<>+16(SB)/4, $+22554
DATA xmm128<>+20(SB)/4, $+116130
DATA xmm128<>+24(SB)/4, $(257)
DATA xmm128<>+28(SB)/4, $(0.29783657)
DATA xmm128<>+32(SB)/4, $(0.58471596)
DATA xmm128<>+36(SB)/4, $(0.114)
GLOBL xmm128<>(SB), RODATA|NOPTR, $40

// func AsmYCbCrToGray(pixels []float32, minX int, minY int, maxX int, maxY int, sY []uint8, sCb []uint8, sCr []uint8, yStride int, cStride int) uint64
// Requires: AVX, AVX2
TEXT ·AsmYCbCrToGray(SB), NOSPLIT, $0-152
	VPBROADCASTD xmm128<>+0(SB), X0
	VPBROADCASTD xmm128<>+4(SB), X1
	VPBROADCASTD xmm128<>+8(SB), X2
	VPBROADCASTD xmm128<>+12(SB), X3
	VPBROADCASTD xmm128<>+16(SB), X4
	VPBROADCASTD xmm128<>+20(SB), X5
	VPBROADCASTD xmm128<>+24(SB), X6
	VPBROADCASTD xmm128<>+28(SB), X6
	VPBROADCASTD xmm128<>+32(SB), X7
	VPBROADCASTD xmm128<>+36(SB), X8
	MOVQ         yStride+128(FP), AX
	MOVQ         cStride+136(FP), BX
	MOVQ         maxY+48(FP), R8
	MOVQ         maxX+40(FP), R9
	MOVQ         sY_base+56(FP), R10
	MOVQ         sCb_base+80(FP), R11
	MOVQ         sCr_base+104(FP), R12
	MOVQ         pixels_base+0(FP), R13
	XORQ         CX, CX
	XORQ         R14, R14
	XORQ         R15, R15

y:
	CMPQ  R14, R8
	JE    done
	MOVQ  AX, CX
	IMULQ R14, CX
	MOVQ  BX, SI
	IMULQ R14, SI

x:
	CMPQ R15, R9
	JE   xDone

	// Start innerloop instructions
	MOVQ      CX, DX
	ADDQ      R15, DX
	MOVQ      SI, DI
	ADDQ      R15, DI
	VPMOVZXBD (R10)(DX*1), X9
	VPMOVZXBD (R11)(DI*1), X10
	VPMOVZXBD (R12)(DI*1), X11
	VPMULLD   X1, X9, X9
	VPSUBD    X0, X10, X10
	VPSUBD    X0, X11, X11
	VPMULLD   X2, X11, X12
	VPADDD    X9, X12, X12
	VPSRAD    $0x08, X12, X12
	VCVTDQ2PS X12, X12
	VMULPS    X6, X12, X12
	VPMULLD   X3, X11, X11
	VPMULLD   X4, X10, X13
	VPSUBQ    X13, X9, X13
	VPSUBQ    X11, X13, X13
	VPSRAD    $0x08, X13, X13
	VCVTDQ2PS X13, X13
	VMULPS    X7, X13, X13
	VPMULLD   X5, X10, X10
	VPADDD    X9, X10, X10
	VPSRAD    $0x08, X10, X10
	VCVTDQ2PS X10, X10
	VMULPS    X8, X10, X10
	VADDPS    X12, X10, X10
	VADDPS    X13, X10, X10
	VMOVAPS   X10, (R13)(DX*4)

	// End innerloop instructions
	ADDQ $0x04, R15
	JMP  x

xDone:
	XORQ R15, R15
	INCQ R14
	JMP  y

done:
	RET

DATA ymm128<>+0(SB)/4, $+128
DATA ymm128<>+4(SB)/4, $+65793
DATA ymm128<>+8(SB)/4, $+91881
DATA ymm128<>+12(SB)/4, $+46802
DATA ymm128<>+16(SB)/4, $+22554
DATA ymm128<>+20(SB)/4, $+116130
DATA ymm128<>+24(SB)/4, $(257)
DATA ymm128<>+28(SB)/4, $(0.29783657)
DATA ymm128<>+32(SB)/4, $(0.58471596)
DATA ymm128<>+36(SB)/4, $(0.114)
GLOBL ymm128<>(SB), RODATA|NOPTR, $40

// func AsmYCbCrToGray8(pixels []float32, minX int, minY int, maxX int, maxY int, sY []uint8, sCb []uint8, sCr []uint8, yStride int, cStride int) uint64
// Requires: AVX, AVX2
TEXT ·AsmYCbCrToGray8(SB), NOSPLIT, $0-152
	VPBROADCASTD ymm128<>+0(SB), Y0
	VPBROADCASTD ymm128<>+4(SB), Y1
	VPBROADCASTD ymm128<>+8(SB), Y2
	VPBROADCASTD ymm128<>+12(SB), Y3
	VPBROADCASTD ymm128<>+16(SB), Y4
	VPBROADCASTD ymm128<>+20(SB), Y5
	VPBROADCASTD ymm128<>+24(SB), Y6
	VPBROADCASTD ymm128<>+28(SB), Y6
	VPBROADCASTD ymm128<>+32(SB), Y7
	VPBROADCASTD ymm128<>+36(SB), Y8
	MOVQ         yStride+128(FP), AX
	MOVQ         cStride+136(FP), BX
	MOVQ         maxY+48(FP), R8
	MOVQ         maxX+40(FP), R9
	MOVQ         sY_base+56(FP), R10
	MOVQ         sCb_base+80(FP), R11
	MOVQ         sCr_base+104(FP), R12
	MOVQ         pixels_base+0(FP), R13
	XORQ         CX, CX
	XORQ         R14, R14
	XORQ         R15, R15

y:
	CMPQ  R14, R8
	JE    done
	MOVQ  AX, CX
	IMULQ R14, CX
	MOVQ  BX, SI
	IMULQ R14, SI

x:
	CMPQ R15, R9
	JE   xDone

	// Start innerloop instructions
	MOVQ      CX, DX
	ADDQ      R15, DX
	MOVQ      SI, DI
	ADDQ      R15, DI
	VPMOVZXBD (R10)(DX*1), Y9
	VPMOVZXBD (R11)(DI*1), Y10
	VPMOVZXBD (R12)(DI*1), Y11
	VPMULLD   Y1, Y9, Y9
	VPSUBD    Y0, Y10, Y10
	VPSUBD    Y0, Y11, Y11
	VPMULLD   Y2, Y11, Y12
	VPADDD    Y9, Y12, Y12
	VPSRAD    $0x08, Y12, Y12
	VCVTDQ2PS Y12, Y12
	VMULPS    Y6, Y12, Y12
	VPMULLD   Y3, Y11, Y11
	VPMULLD   Y4, Y10, Y13
	VPSUBQ    Y13, Y9, Y13
	VPSUBQ    Y11, Y13, Y13
	VPSRAD    $0x08, Y13, Y13
	VCVTDQ2PS Y13, Y13
	VMULPS    Y7, Y13, Y13
	VPMULLD   Y5, Y10, Y10
	VPADDD    Y9, Y10, Y10
	VPSRAD    $0x08, Y10, Y10
	VCVTDQ2PS Y10, Y10
	VMULPS    Y8, Y10, Y10
	VADDPS    Y12, Y10, Y10
	VADDPS    Y13, Y10, Y10
	VMOVAPS   Y10, (R13)(DX*4)

	// End innerloop instructions
	ADDQ $0x08, R15
	JMP  x

xDone:
	XORQ R15, R15
	INCQ R14
	JMP  y

done:
	RET
